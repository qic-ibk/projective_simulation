[
  {
    "objectID": "tutorials/index_tutorials.html",
    "href": "tutorials/index_tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "We have gathered a series of tutorials that will allow you to explore from the very basics of PS to applications presented in different research papers."
  },
  {
    "objectID": "tutorials/index_tutorials.html#learning-ps",
    "href": "tutorials/index_tutorials.html#learning-ps",
    "title": "Tutorials",
    "section": "Learning PS",
    "text": "Learning PS\n\nIntro to PS: this notebook will guide you through the basics of a PS agent equipped with a two layer ECM. You will learn how an agent updates its internal structure based on reinforcement learning."
  },
  {
    "objectID": "tutorials/index_tutorials.html#applications",
    "href": "tutorials/index_tutorials.html#applications",
    "title": "Tutorials",
    "section": "Applications",
    "text": "Applications\n\n??"
  },
  {
    "objectID": "tutorials/index_tutorials.html#contributing",
    "href": "tutorials/index_tutorials.html#contributing",
    "title": "Tutorials",
    "section": "Contributing",
    "text": "Contributing\nIf you want to contribute, please check this tutorial on contributing guidelines"
  },
  {
    "objectID": "tutorials/contributing/contributing.html",
    "href": "tutorials/contributing/contributing.html",
    "title": "Contribution guide",
    "section": "",
    "text": "1. The nbdev package\nThis library is based on nbdev. This is a nice tutorial on everything you need to learnd about it.\nIn the rest of this notebook I will highlight the minimum needed from the library to contribute to the projective_simulation. In particular, I will show you the few magic commands that will allow us to create a python package from notebooks.\n\n\n2. Installation\nBefore starting to write your code, use pipto install the library. To do so, clone the repository and do a local installation of the package. For that, you just need to go to the repo’s folder and run:\npip install -e .\n\n\n3. Write new code\nNow that you have the library installed, let’s write some new code using Jupyter Notebooks. For the moment, let’s do something simple and talk later about how to properly structure the library. Let’s consider that I want to create, inside the library projective_simulation, a new module utils in which I will gather few useful functions.\nTo tell nbdev that I want this notebook to do the previous, we use the default_exp magic command in the following way:\n\nImportant: any nbdev magic command must be preceded by #|\n\nThis will ensure the objects we create within this notebook will be exported to a .py file called contributing_guide.py within the package folder. You can find the latter in the parent repo folder with the name projective_simulation (the chosen name for the library).\n\nImportant: never write in those .py, as their content is automatically generated from the notebooks and any change there will be overwritten :) .\n\nNow let’s write some proper code! We will create a function that we want to be contained in the module utils. To do so, we use the command #| export to tell nbdev that we want the content of this to go to the current module:\n\nsource\n\nrandom_func\n\n random_func (k)\n\nWe need to actively export the current state of the notebook. To do so, you can either run nbdev_export in your terminal (inside the repo folder) or run the following cell:\n\nImportant 1: nbdev_export exports the current state of the notebooks in you library. This means the last saved version! While notebooks usually autosave, it is good practice to save the notebook before running the command.\n\n\nImportant 2: because I don’t want this command to go to the module, I don’t put the #| export command in the cell.\n\n\nimport nbdev; nbdev.nbdev_export()\n\nAnd that’s it! If you are curious, you can go to projective_simulation/utils.py and see that the content of the file is exactly the one in the cell above.\n\n\n\n3. Importing generated functions\nBecause the package has been installed through pip, we can now import the function we just created. Let’s see how. First, restart the notebook’s kernel. Now, you can import the random_func function as:\n\nfrom projective_simulation.contributing_guide import random_func\n\n\nrandom_func(2)\n\n0.31480691257004256\n\n\n\n\n4. Contributing to the repo\nNotebooks have a lot of useless metadata in them (i.e. the count of the cell execution, the environment you are using,…). This would make impossible contribute to a shared repo, as for instance different contributors may be using different environment names, which would end up in a conflict. To avoid this, nbdev has created hooks that deal with this problem (check their documentation if you want to know details). For now, you just need to do the following:\n\nAfter installing nbdev (should have been done automatically when installing projective_simulation as I put it as a requirement), run the following in your terminal, inside the repo’s folder:\n\nnbdev_install_hooks\n\nMOST IMPORTANTLY, before pushing any changes to the origin, be sure to have done two things:\n\nExport the changes you did in the notebook (see above)\nClean the metadata of your notebooks using the command nbdev_clean\n\n\n\n\n5. Git considerations\nFor a brief tutorial on using git and project protocols, please see 01_using_git in the same folder as this notebook."
  },
  {
    "objectID": "tutorials/contributing/using_git.html",
    "href": "tutorials/contributing/using_git.html",
    "title": "Intro to Git for PS Users - Intro",
    "section": "",
    "text": "git clone https://@github.com/qic-ibk/projective_simulation.git\n\nwhere <PAT> is your personal access token.\n\nCreating A Branch\nThe best practice for collaborative projects it to work in your own branch of the repository. Creating a branch essentially means that git keeps track of two versions of the repository that you can easily switch between. When you have finished a task or a project, you can “merge” branches back together. Generally, if merging two branches would be destructive, meaning any data from either branch would be overwritten, you will recieve a warning and an opportunity to review any conflicts. Creating your own branch means that you can push any changes you make to the remote repository without overwriting anything and give other team members an opportunity to review any conflicts before the merge is made. To create a new branch, use the command\n\ngit branch \n\n\n**Important**: Creating a new branch does automatically change the version you are working on to that branch! To do this, you need to checkout the branch:\n\n\ngit checkout \n\n\nYou can view a list of all available branches using\n\n\ngit branch\n\n\n\nCommitting Your Changes\nOnce you have created your own branch of the repository, you can work in the repository however you choose. You can create a snapshot of the repository at any time, allowing you to revert to that repository state later. We won’t discuss reverting in this document, but it is recommended that you commit your changes often. This is done in a two step process. First, you need to “stage” the files you want git to update with its new snapshot, then you commit all of the staged files:\n\ngit add file1.py git add file2.py git commit -m “Commit Demonstration”\n\nThe -m (“message”) is optional, but it is highly (highly) recommended that you include a short message with every commit that makes it clear what changes you have made. As a general rule-of-thumb, if you are staging multiple files or need long commit messages to describe your changes, you aren’t commiting often enough! The simple habit of making a commit any time you complete a task in your workflow will go a long way toward helping other people understand your work, and can be a real life saver if you ever want to go back and remember exactly how you did something, or try a new approach starting from an old version of code, etc. . .\n\n\nPushing Your Changes to the Remote Repository\nWhen you are ready to add your work to the remote PS repository, you can do this either by “pushing” your local repository or by making a “pull request” of the remote repository administrators. Pushing is much simpler, but if your work creates conflicts you will overwrite data in the remote repository. Note that conflicts are not caused by changes in files per se but by differences in commits. For example, if you push your branch to the remote repository, then delete a file locally, commit and push again, there won’t be a conflict! The conflict occurs if somebody else has already pushed a commit that isn’t the branch you are trying to push from your local repository. You can easily recover the deleted file by reverting to an old commit. If you are working using your own branch, you can thus always push it to the remote repository safely:\n\ngit push origin \n\nThe argument “origin” simply tells git that you are pushing your local repository to the repository from which it was cloned.\nIf you are working on a branch of the repository collaboratively, or you want to push your changes directly to the “master” brach of the remote repository you should always pull from the remote repository before you push. When you pull from the remote repository, git will try to add any changes made in the remote repository since the last time you pulled from (or cloned) it. If any of these changes would be destructive, you will get a warning and the pull will fail. There are ways to override this, but we won’t discuss them here: if you get a warning about conflicts when both attempting to push and to pull a branch, your collaborative workflow is not working and you should discuss how to proceed with your teammates. The effective approach to making changes to a shared branch such as “master” goes as follows:\n\ngit pull origin master git merge  -m “local changes merge” git push origin master\n\nNote that merging automatically makes a commit to the master branch (or whatever branch  orginates from). Thus, even if your merge requires you to manage some conflicts (for example, your branch deleted a few line of code), you should be able to push to the remote repository without trouble. You aren’t deleting those lines of code, you are just adding a commit that doesn’t include them! Of course, such changes should still be discussed with project administrators - even though the deleted code is recoverable, it doesn’t mean other teammates want to revert to an old commit to find the file version that has it!\nWith that, you should be equipped to work collaboratively with the PS team! Happy Coding (:"
  },
  {
    "objectID": "tutorials/learning_ps/introduction_to_ps.html",
    "href": "tutorials/learning_ps/introduction_to_ps.html",
    "title": "Introduction to PS",
    "section": "",
    "text": "In this tutorial, we present how to set up a simple projective simulation (PS) agent to learn to play the invader game. To do so, we will first set up the environment and create a basis instantiation of PS agent with a two-layer ECM. We will then train the agent during episodes of deliberation and updates and plot the final learning curve."
  },
  {
    "objectID": "tutorials/learning_ps/introduction_to_ps.html#install-the-ps-package",
    "href": "tutorials/learning_ps/introduction_to_ps.html#install-the-ps-package",
    "title": "Introduction to PS",
    "section": "Install the PS package",
    "text": "Install the PS package\nStart by making sure that you have access to the package: clone the repository using git clone https://github.com/qic-ibk/projective_simulation.git from your terminal, in your project’s folder. Install the package with the command pip install -e .. To remove dependencies on user-specific metadata, run the command nbdev_install_hooks."
  },
  {
    "objectID": "tutorials/learning_ps/introduction_to_ps.html#set-up-the-environment",
    "href": "tutorials/learning_ps/introduction_to_ps.html#set-up-the-environment",
    "title": "Introduction to PS",
    "section": "Set up the environment",
    "text": "Set up the environment\nThe invader opposes two agents. The invader attempts to enter a city and shows a sign to the defender to indicate his intention to move right or left. In order to defend his territory, the defender learns to move in the matching direction in order to block the defender. The invader can either declare his true intention or he can lie and show the opposite direction instead. The defender must adapt his strategy accordingly.\nLet us now set the environment up by creating the invader game environment:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\n\nimport projective_simulation as ps\nimport projective_simulation.envs.core as environments\n\n\nenv = environments.Invader_Game_Env()"
  },
  {
    "objectID": "tutorials/learning_ps/introduction_to_ps.html#create-the-agent",
    "href": "tutorials/learning_ps/introduction_to_ps.html#create-the-agent",
    "title": "Introduction to PS",
    "section": "Create the agent",
    "text": "Create the agent\nFor this task, we PS-agent that has two actions (“go right” and “go left”) and an ECM organized in two layers suffices. Deliberation will operate by transitioning directly from percepts to actions in a random walk.\nAt the same time, we set the hyperparameters of the agent. The damping parameter \\(\\gamma\\) represents forgetting: when large, the agent forgets what it has reinforced in the previous episodes. The edge weight, the h-values, are updated as follows for an edge linking clip \\(i\\) to clip \\(j\\) after receiving a reward \\(R\\) from the environment: \\[\\begin{equation}\nh_{ij}^{(t)} = (1-\\gamma) \\cdot h_{ij}^{(t-1)} + \\gamma \\cdot h_{ij}^{(0)} + R\\cdot g_{ij}^{(t)}\n\\end{equation}\\]\nThe glow parameter \\(\\eta\\) decides how quickly the agent forgets it sampled a specific edge to deliberate: if one, the agent can remember and reward exactly one step, whereas if it takes a smaller value, edges that were sampled in the past will be reinforced proportionally to how far in the past they were used: \\[\\begin{equation}\ng_{ij}^{(t)} = \\begin{cases}\n1 &\\text{ if edge $i-j$ was sampled last} \\\\\n(1-\\eta) g_{ij}^{(t-1)} &\\text{otherwise}\n\\end{cases}\n\\end{equation}\\]\nSince the invader game environment does not require any memory beyond one step, we set both parameters to 0 for this task.\n\nimport projective_simulation.agents.core as agents\n\n# Create a basic two-layer agent, with two actions\nagent = agents.Basic_2Layer_Agent(num_actions=2,\n                                 glow=1,\n                                 damp=0,\n                                 policy = 'softmax',\n                                 policy_parameters=1)"
  },
  {
    "objectID": "tutorials/learning_ps/introduction_to_ps.html#train-the-agent",
    "href": "tutorials/learning_ps/introduction_to_ps.html#train-the-agent",
    "title": "Introduction to PS",
    "section": "Train the agent",
    "text": "Train the agent\nIntegrate agent and environment to create a percept-action loop during which the agent deliberates and updates its memory.\n\n# Store the reward history of the agent\n\nreward_history = []\nN_episodes = 100\n# Initialize the sign of the invader\nfor episode in range(N_episodes):\n    percept = env.get_observation()\n    action = agent.deliberate(percept)\n    reward = env.get_reward(action, liar=False)\n    agent.update(reward)\n    reward_history.append(reward)\n    env.transition(action)\n\n\n# Plot the learning curve\nplt.plot(list(range(len(reward_history))), \n              [np.mean([reward_history[step] for step in range(i-10,i+1) if step >= 0]) for i in range(N_episodes)])\nplt.xlabel(\"episodes\")\nplt.ylabel(\"rewards\")\nplt.show()\n\n\n\n\nWe can check the attributes of the agent:\n\n# Check the percepts the agent has encountered\nprint(\"percepts: \", agent.ECM.percepts)\n\n# Check the h-values of the agent\nprint(\"ECM:\", agent.ECM.hmatrix)\n\npercepts:  {'left': 0, 'right': 1}\nECM: [[  1. 527.]\n [474.   1.]]\n\n\nWe can now train 10 agents to have an average learning curve.\n\nN_agents = 10\nreward_history = []\n\nfor i in range(N_agents):\n    reward_history.append([])\n    env = environments.Invader_Game_Env()\n    agent = agents.Basic_2Layer_Agent(num_actions=2,\n                                     glow=1,\n                                     damp=0,\n                                     policy = 'softmax',\n                                     policy_parameters=1)\n    for episode in range(N_episodes):\n        percept = env.get_observation()\n        action = agent.deliberate(percept)\n        reward = env.get_reward(action, liar=False)\n        agent.update(reward)\n        reward_history[i].append(reward)\n        env.transition(action)\n\n# Plot the average learning curve\nmean_rewards = np.mean(np.array(reward_history), axis=0)\nplt.plot(list(range(len(reward_history[0]))), \n              [np.mean([mean_rewards[step] for step in range(i-10,i+1) if step >= 0]) for i in range(N_episodes)])"
  },
  {
    "objectID": "tutorials/learning_ps/introduction_to_ps.html#effect-of-the-forgetting-parameter-gamma",
    "href": "tutorials/learning_ps/introduction_to_ps.html#effect-of-the-forgetting-parameter-gamma",
    "title": "Introduction to PS",
    "section": "Effect of the forgetting parameter \\(\\gamma\\)",
    "text": "Effect of the forgetting parameter \\(\\gamma\\)\nIn order to understand the importance of the different hyperparameters, we train the agents in different scenarios. We start by exploring the role of the forgetting parameter. For this, we switch the strategy of the invader from non-liar to liar in the middle of the training, and explore the impact this switch on the learning curve for agents with different learning rates.\n\nN_agents = 100\nreward_history = []\ndamp_params = [1/50, 1/10, 1/5]\n\nfor d in range(len(damp_params)):\n    reward_history.append([])\n    for i in range(N_agents):\n        reward_history[d].append([])\n        env = environments.Invader_Game_Env()\n        agent = agents.Basic_2Layer_Agent(num_actions=2,\n                                        glow=1,\n                                        damp=damp_params[d],\n                                        policy = 'softmax',\n                                        policy_parameters=1)\n        for episode in range(N_episodes):\n            percept = env.get_observation()\n            action = agent.deliberate(percept)\n            reward = env.get_reward(action, liar=episode > N_episodes // 2)\n            agent.update(reward)\n            reward_history[d][i].append(reward)\n            env.transition(action)\n\n# Plot the average learning curve\nmean_rewards = [np.mean(np.array(reward_history[d]), axis=0) for d in range(len(damp_params))]\nfig, ax = plt.subplots(figsize=(10, 6))\nfor d in range(len(damp_params)):\n    ax.plot(list(range(N_episodes)), \n              [np.mean([mean_rewards[d][step] for step in range(i-10,i+1) if step >= 0]) for i in range(N_episodes)],\n              label=f'damp={damp_params[d]:.2f}')\nax.legend()\nax.set_xlabel('Episode')\nax.set_ylabel('Average Reward')\nax.set_title('Effect of the forgetting parameter on the learning curve')\nplt.show()"
  },
  {
    "objectID": "tutorials/learning_ps/introduction_to_ps.html#test-the-effect-of-the-glow-parameter",
    "href": "tutorials/learning_ps/introduction_to_ps.html#test-the-effect-of-the-glow-parameter",
    "title": "Introduction to PS",
    "section": "Test the effect of the glow parameter",
    "text": "Test the effect of the glow parameter\nIn order to understand the role of the glow damping parameter, we need to set up an environment that requires the agent to remember more than a single percept to properly contextualize its policy. Therefore, we use the Grid World environment that consists of a 2D grid with a number of obstacles. The goal of the agent is to reach a target at a precise location.\nSince rewards in this environment are sparse (only a single cell leads to a reward), reinforcement must be applied to the whole trajectory for the agent to learn to navigate the whole grid towards the target. Therefore, we now make use of glow and tune its damping parameter to values smaller than 1.\n\n# Plot the environment\ndef plot_Grid_World(obs_loc, reward_loc, initial_loc):\n    # Plot the grid with obstacles, reward, initial location, and learnt policy arrows\n    # Get grid dimensions\n    grid_shape = env.dimensions\n    fig, ax = plt.subplots(figsize=(9,6))\n    ax.set_xlim(0, grid_shape[1])\n    ax.set_ylim(0, grid_shape[0])\n    ax.set_xticks(np.arange(grid_shape[1]))\n    ax.set_yticks(np.arange(grid_shape[0]))\n    \n    # Draw obstacles\n    for loc in obs_loc:\n        ax.add_patch(plt.Rectangle((loc[1], loc[0]), 1, 1, color='black', alpha=0.5))\n    \n    # Draw reward location\n    ax.add_patch(plt.Rectangle((reward_loc[1], reward_loc[0]), 1, 1, color='gold', alpha=0.5))\n    ax.text(reward_loc[1]+0.25, reward_loc[0]+0.6, 'Target')\n    \n    # Draw initial location\n    ax.add_patch(plt.Rectangle((initial_loc[1], initial_loc[0]), 1, 1, color='cyan', alpha=0.5))\n    ax.text(initial_loc[1]+0.5, initial_loc[0]+0.6, 'Start', ha='center', va='bottom', color='black')\n    \n    ax.set_title(\"Grid World Environment\")\n    ax.set_xticklabels(np.arange(grid_shape[1]))\n    ax.set_yticklabels(np.arange(grid_shape[0]))\n    ax.invert_yaxis()\n    plt.grid(True)\n    plt.show()\n\n\nobs_loc = [(1,2), (2,2), (3,2),\n           (4,5),\n           (0,7), (1,7), (2,7)]\nreward_loc = (0,8)\ninitial_loc = (2,0)\nenv = environments.Grid_World_Env(dimensions=(6, 9),\n                                  N_obstacles=len(obs_loc), obstacle_locations=obs_loc,\n                                  reward_location=reward_loc, state=None)\n\n# Plot the environment\nplot_Grid_World(obs_loc, reward_loc, initial_loc)\n\n\n\n\nThe glow mechanism marks the transitions a PS agent has deliberated over in the recent past. When a reward is received from the environment, the reinforcement of the marked edges is proportional to how strongly they are still “glowing’’ in the memory of the agent.\nThe damping parameter (glow in the code below, or \\(\\eta\\) above) determines how fast the glow of edges decays. Consequently, it allows agents to reinforce a controlled number of edges even though only one transition actually received a reward.\nLet us explore the influence of glow in the Grid World environment above by training agents with two-layer ECMs but with different glow parameters.\n\nN_agents = 100\nN_episodes = 50\nN_steps_history = []\ng_damp_params = np.array([0, 0.04, 0.08, 0.12, 0.15, 0.20, 1.])\nN_rewards = []\nagents_opt = []\n\nN_steps_history = np.zeros((len(g_damp_params), N_agents, N_episodes))\nfor d in range(len(g_damp_params)):\n    print(f\"Glow parameter: {g_damp_params[d]}\")\n    agents_opt.append([])\n    N_rewards.append(0)\n    for i in range(N_agents):\n        agent = agents.Basic_2Layer_Agent(\n            num_actions=4,\n            glow=g_damp_params[d],\n            damp=0,\n            policy='softmax',\n            policy_parameters=1,  # Higher beta for sharper policy\n            glow_method='init')   # Use 'init' for trajectory credit assignment\n        \n        for episode in range(N_episodes):\n            n_step = 0\n            env.reset_position(initial_state=initial_loc)\n            percept = env.get_observation()\n            reward = 0\n            while reward == 0:\n                action = agent.deliberate(percept)\n                env.transition(action, periodic=False)\n                percept = env.get_observation()\n                reward = 1 * env.get_reward()\n                agent.update(reward)\n                if n_step >= 200:  # Prevent infinite loops\n                    break\n                n_step += 1\n\n            N_rewards[d] += reward\n            N_steps_history[d,i,episode] = n_step\n            if episode < N_episodes - 1:\n                agent.ECM.gmatrix = np.zeros_like(agent.ECM.gmatrix)\n        \n        agents_opt[d].append(agent)\n\nGlow parameter: 0.0\nGlow parameter: 0.04\nGlow parameter: 0.08\nGlow parameter: 0.12\nGlow parameter: 0.15\nGlow parameter: 0.2\nGlow parameter: 1.0\n\n\nLet us plot the learning curve and trained model to understand how glow affects learning in the grid world environment.\n\ndef plot_learning_curves(N_steps_history, N_episodes, g_damp_params):\n    # Plot the average number of steps to reach the goal\n    fig, ax = plt.subplots(figsize=(10, 6))\n    colors= ['blue', 'orange', 'green', 'red', 'purple', 'cyan' , 'brown']\n    mean_steps = [np.mean(N_steps_history[d,:,:], axis=0) for d in range(len(g_damp_params))]\n    std_steps = [np.std(np.array(N_steps_history[d]), axis=0) for d in range(len(g_damp_params))]\n    for d in range(len(g_damp_params)):\n        ax.plot(list(range(N_episodes)), mean_steps[d],\n                label=f'glow={g_damp_params[d]:.2f}', color=colors[d])\n    ax.legend()\n    ax.grid(True)\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Average Number of Steps to Reach the Goal')\n    ax.set_title('Effect of the glow parameter on the number of steps to reach the goal')\n    plt.show()\n\n\nplot_learning_curves(N_steps_history, N_episodes, g_damp_params)\n\n\n\n\nGlow in this scenario is essential for agents to learn (see the brown curve). For the agents to learn as efficiently as possible, the value for the damping parameter should be tuned carefully, according to the size of the environment, and how many steps are relevant to the reward that is received.\nIn particular in this environment, a damping parameter between 0.04 and 0.08 seems to work best. In what follows, we plot glow matrix of an agent on the grid to give intuition about its contribution to learning. Glowing edges between locations on the grid (percepts in the ECM) and directions (actions) are represented by arrows on the grid. At each location that was visited, the size of the arrow is proportional to its glow value.\n\nfrom matplotlib.patches import FancyArrow\nfrom projective_simulation.methods.transforms import _softmax\n\n# Plot the grid with obstacles, reward, initial location, and learnt policy arrows\ndef plot_glow_GW(agents_opt, agent_idx, glow_damping, glow_idx, env, obs_loc, reward_loc, initial_loc):\n    # Get grid dimensions\n    grid_shape = env.dimensions\n    action_colors = ['blue', 'orange', 'green', 'red']  # Colors for the arrows0\n    \n    fig, ax = plt.subplots(figsize=(9,6))\n    ax.set_xlim(0, grid_shape[1])\n    ax.set_ylim(0, grid_shape[0])\n    ax.set_xticks(np.arange(grid_shape[1]))\n    ax.set_yticks(np.arange(grid_shape[0]))\n    \n    # Draw obstacles\n    for loc in obs_loc:\n        ax.add_patch(plt.Rectangle((loc[1], loc[0]), 1, 1, color='black', alpha=0.5))\n    \n    # Draw reward location\n    ax.add_patch(plt.Rectangle((reward_loc[1], reward_loc[0]), 1, 1, color='gold', alpha=0.5))\n    ax.text(reward_loc[1]+0.25, reward_loc[0]+0.6, 'Target')\n    \n    # Draw initial location\n    ax.add_patch(plt.Rectangle((initial_loc[1], initial_loc[0]), 1, 1, color='cyan', alpha=0.5))\n    ax.text(initial_loc[1]+0.5, initial_loc[0]+0.85, 'Start', ha='center', va='bottom', color='black')\n    \n    # Arrow directions: ↑, →, ↓, ←\n    arrow_deltas = [(0.35, 0), (-0.35, 0), (0, -0.35), (0, 0.35)]\n    \n    for i in range(grid_shape[0]):\n        for j in range(grid_shape[1]):\n            cell = (i,j)\n            # Skip obstacles and reward\n            if cell in obs_loc or cell == reward_loc:\n                continue\n            percept = str(cell)\n            if percept in agents_opt[glow_idx][agent_idx].ECM.percepts:\n                glow_values = agents_opt[glow_idx][agent_idx].ECM.gmatrix[agents_opt[glow_idx][agent_idx].ECM.percepts[percept],:]\n                for a, glow in enumerate(glow_values):\n                    if glow > 0.05:  # Only draw arrows for significant probabilities\n                        dx, dy = arrow_deltas[a]\n                        ax.add_patch(FancyArrow(j+0.5, i+0.5, dx*glow, dy*glow, width=0.05*glow, length_includes_head=True,\n                                               color = action_colors[a], alpha = 0.85))\n            else:\n                ax.text(j+0.5, i+0.5, '.', ha='center', va='center', color='grey', fontsize=12)\n    \n    ax.set_title(\"Grid World Environment\")\n    ax.set_xticklabels(np.arange(grid_shape[1]))\n    ax.set_yticklabels(np.arange(grid_shape[0]))\n    ax.invert_yaxis()\n    plt.grid(True)\n    plt.title(f\"Example of the distribution of glow values (damping = {glow_damping})\")\n    #plt.gca().invert_yaxis()\n    plt.show()\n\n\nplot_glow_GW(agents_opt, 0, g_damp_params[1], 1, env, obs_loc, reward_loc, initial_loc)\n\n\n\n\nWith the right damping parameter (here 0.04 works), glow allows the agent to reinforce sequences of actions that are just long enough to reach the target. If the damping is too strong (for example 0.20), as shown below, the agent struggles to learn the right actions at the beginning of the sequence because the corresponding edges are associated with negligible values of glow by the end of the sequence. Conversly, if damping is not strong enough (0 being an extreme case), the action sequence that is rewarded incorporates actions that did not contribute to reaching the target.\n\nplot_glow_GW(agents_opt, 0, g_damp_params[-2], -2, env, obs_loc, reward_loc, initial_loc)\n\n\n\n\n\nplot_glow_GW(agents_opt, 20, g_damp_params[0], 0, env, obs_loc, reward_loc, initial_loc)\n\n\n\n\nFinally, we show the final policy of a trained agent on the grid, where the length of each arrows is proportional to the probability of sampling the corresponding action in the final policy of the agent.\n\ndef plot_policy_GW(agents_opt, agent_idx, glow_damping, glow_idx, env, obs_loc, reward_loc, initial_loc):\n    # Get grid dimensions\n    grid_shape = env.dimensions\n    action_colors = ['blue', 'orange', 'green', 'red']  # Colors for the arrows\n    \n    fig, ax = plt.subplots(figsize=(9,6))\n    ax.set_xlim(0, grid_shape[1])\n    ax.set_ylim(0, grid_shape[0])\n    ax.set_xticks(np.arange(grid_shape[1]))\n    ax.set_yticks(np.arange(grid_shape[0]))\n    \n    # Draw obstacles\n    for loc in obs_loc:\n        ax.add_patch(plt.Rectangle((loc[1], loc[0]), 1, 1, color='black', alpha=0.5))\n    \n    # Draw reward location\n    ax.add_patch(plt.Rectangle((reward_loc[1], reward_loc[0]), 1, 1, color='gold', alpha=0.5))\n    ax.text(reward_loc[1]+0.25, reward_loc[0]+0.6, 'Target')\n    \n    # Draw initial location\n    ax.add_patch(plt.Rectangle((initial_loc[1], initial_loc[0]), 1, 1, color='cyan', alpha=0.5))\n    ax.text(initial_loc[1]+0.5, initial_loc[0]+0.85, 'Start', ha='center', va='bottom', color='black')\n    \n    # Arrow directions: ↑, →, ↓, ←\n    arrow_deltas = [(0.35, 0), (-0.35, 0), (0, -0.35), (0, 0.35)]\n    \n    for i in range(grid_shape[0]):\n        for j in range(grid_shape[1]):\n            cell = (i,j)\n            # Skip obstacles and reward\n            if cell in obs_loc or cell == reward_loc:\n                continue\n            percept = str(cell)\n            if percept in agents_opt[glow_idx][agent_idx].ECM.percepts:\n                h = agents_opt[glow_idx][agent_idx].ECM.hmatrix[agents_opt[glow_idx][agent_idx].ECM.percepts[percept],:]\n                probs = _softmax(beta=1, x=h)\n                for a, prob in enumerate(probs):\n                    if prob > 0.05:  # Only draw arrows for significant probabilities\n                        dx, dy = arrow_deltas[a]\n                        ax.add_patch(FancyArrow(j+0.5, i+0.5, dx*prob, dy*prob, width=0.05*prob, length_includes_head=True,\n                                               color = action_colors[a], alpha = 0.85))\n            else:\n                ax.text(j+0.5, i+0.5, '.', ha='center', va='center', color='grey', fontsize=12)\n    \n    ax.set_title(\"Grid World Environment\")\n    ax.set_xticklabels(np.arange(grid_shape[1]))\n    ax.set_yticklabels(np.arange(grid_shape[0]))\n    ax.invert_yaxis()\n    plt.grid(True)\n    plt.title(\"Trained Policy\")\n    plt.show()\n\n\nplot_policy_GW(agents_opt, 0, g_damp_params[1], 1, env, obs_loc, reward_loc, initial_loc)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projective Simulation",
    "section": "",
    "text": "Get started | Documentation | Tutorials\nProjective Simulation (PS) is a framework for studying agency from a physical perspective. PS agents learn from interaction with their environment and base their decisions on episodic memory. Internally, their deliberation corresponds to random walks on a clip network (graph), which provides an interpretable route to physical realizations and analysis.\nThis library all the necessary tools to develop, deploy and train PS agents in a variety of environments. Its content spans from entry-level tutorials that introduce the core principles of PS to cutting-edge research on the latest developments and applications. It is actively developed by the QIC group at UIBK."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Projective Simulation",
    "section": "Installation",
    "text": "Installation\nProjective simulation is available for python>=3.10 via\npip install projective_simulation\nThis will also install the necessary library requirements."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Projective Simulation",
    "section": "Getting Started",
    "text": "Getting Started\nTo begin exploring Projective Simulation, we recommend to start with the tutorials, which contain from the basics to more advance applications.\nIf you want to develop or deploy PS agents, please check the documetation."
  },
  {
    "objectID": "index.html#citing",
    "href": "index.html#citing",
    "title": "Projective Simulation",
    "section": "Citing",
    "text": "Citing\nIf you found this package useful and used it in your projects, you can either the particular PS paper that you found most useful (see a list here) or use the following to cite the whole library:\n\nhere will be some zenodo link"
  },
  {
    "objectID": "webpage/research.html",
    "href": "webpage/research.html",
    "title": "Research",
    "section": "",
    "text": "Here you can find a complete list of papers using PS:"
  },
  {
    "objectID": "webpage/research.html#section",
    "href": "webpage/research.html#section",
    "title": "Research",
    "section": "2025",
    "text": "2025\n\n“Run-and-Tumble Particles Learning Chemotaxis” by N. Tovazzi, G. Muñoz-Gil and M. Caraglio* (2025). In this work we explore the ability of active particles to adapt their run-and-tumble strategy to reach targets, based on their chemotactic response, just as bacteria do in the real world!\n“Learning to reset in target search problems” by G. Muñoz-Gil, H. J. Briegel and M. Caraglio (2025). Here we extended the agents to be able to reset to the origin, a feature that has revolutionize target search problems in the last years."
  },
  {
    "objectID": "webpage/research.html#section-1",
    "href": "webpage/research.html#section-1",
    "title": "Research",
    "section": "2024",
    "text": "2024\n\n“Learning how to find targets in the micro-world: the case of intermittent active Brownian particles” by M. Caraglio, H. Kaur, L. Fiderer, A. López-Incera, H. J. Briegel, T. Franosch, and G. Muñoz-Gil (2024). In this case, we study the ability of agents to learn how to switch from passive to active diffusion to enhance their target search efficiency.\n“Optimal foraging strategies can be learned” by G. Muñoz-Gil, A. López-Incera, L. J. Fiderer and H. J. Briegel (2024). Here we developed agents able to learn how to forage efficiently in environments with multiple targets."
  },
  {
    "objectID": "lib_nbs/ECMs/priming.html",
    "href": "lib_nbs/ECMs/priming.html",
    "title": "Priming",
    "section": "",
    "text": "source\n\nPriming_ECM\n\n Priming_ECM (num_actions:int, glow:float=0.1, damp:float=0.01,\n              softmax:float=0.5, action_primes:list=None)\n\nThis sub-class of the Two-Layer ECM adds a variable for action priming. This variable should be a list of floats, each element of which corresponds to an action in the ECM. These “priming values” are summed with h-values of any edge connected to the associated action node prior to calculating walk probabilites with the softmax function\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_actions\nint\n\nThe number of available actions.\n\n\nglow\nfloat\n0.1\nThe glow (or eta) parameter.\n\n\ndamp\nfloat\n0.01\nThe damping (or gamma) parameter.\n\n\nsoftmax\nfloat\n0.5\nThe softmax (or beta) parameter.\n\n\naction_primes\nlist\nNone\nweights on the probability that deliberation steps into each action. Defaults to 0 for each action\n\n\n\n\nExample\n\ntest_ECM = Priming_ECM(num_actions = 2, action_primes = [0., 1.5])\n#Number of steps to run simulation\nT = 100\ndata_log = [None] * T\nenv = RLGL() #create a default red-light-green-light environment\n\nfor t in range(T):\n    observation = env.get_observation()\n    action = test_ECM.sample(observation)\n    reward = env.get_reward(action)\n    test_ECM.learn(reward)\n    data_log[t] = {\"env_state\": env.state, \"action\": action, \"reward\": reward}\n    env.transition(action)\n\nplt.plot(range(T), [np.mean([data_log[step][\"reward\"] for step in range(i-10,i+1) if step >= 0]) for i in range(T)]) #plot a 10 step moving average of the reward"
  },
  {
    "objectID": "lib_nbs/ECMs/core.html",
    "href": "lib_nbs/ECMs/core.html",
    "title": "Core",
    "section": "",
    "text": "Here we collect different standalone functions that will help us construct different types of ECM\n\nsource\n\n\n\n standard_ps_upd (reward, hmatrix, gmatrix, h_damp, g_damp)\n\n*Given a reward, updates h-matrix and g-matrix following the standard PS update rule:\n\\(h \\leftarrow h - h_{damp}*(h-1)+ reward*g\\)\n\\(g \\leftarrow (1-g_{damp})*g\\)*"
  },
  {
    "objectID": "lib_nbs/ECMs/core.html#pre-built-ecms",
    "href": "lib_nbs/ECMs/core.html#pre-built-ecms",
    "title": "Core",
    "section": "Pre-built ECMs",
    "text": "Pre-built ECMs\nHere we collect the abstract parent class that any ECM should be built upon as well as some pre-built ECM ready to use.\n\nsource\n\nAbstract_ECM\n\n Abstract_ECM (*args, **kwargs)\n\nAbstract agent class any episodic and compositional memory (ECM) should be derived from. Asserts that the necessary methods are implemented. No compulsory input objects are needed.\nPS agents must have as parent class Abstract_ECM, and hence must contain one compulsory method:\n\nsource\n\nAbstract_ECM.sample\n\n Abstract_ECM.sample ()\n\nPerforms a random walk through the ECM. Typically, this implies receiving an input percept and returning an action.\n\nsource\n\n\n\nTwo_Layer\n\n Two_Layer (num_actions:int, g_damp:float, h_damp:float,\n            policy:str='greedy', policy_parameters:dict=None,\n            glow_method:str='sum')\n\n*Two layer ECM. First layer, encoding the percepts observed in an environment, is initially empty (e.g. self.num_percepts = 0). As percepts are observed, they are added to the ECM and to the percept dictionary self.percepts. The second layer, encoding the actions, has size self.num_actions. In practice, the ECM graph is never created. Instead, it is defined indirectly by the h-matrix and g-matrix. Both have size (self.num_percepts, self.num_actions). The input policy (greedy, softmax or other) is used to sample actions based on the h-matrix.\nFor an end-to-end example of how to use this class, see the introductory tutorial notebook on PS agents.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_actions\nint\n\nThe number of available actions.\n\n\ng_damp\nfloat\n\nThe glow damping(or eta) parameter.\n\n\nh_damp\nfloat\n\nThe damping (or gamma) parameter.\n\n\npolicy\nstr\ngreedy\nIf ‘greedy’, uses a greedy policy that samples the most action based on the h-matrix. If ‘softmax’, uses a softmax policy that samples an action based on the h-matrix and a temperature parameter (encoded in policy_parameters).If object, uses this object to sample action. Input must be h_values corresponding to current percept + arbitrary policy_parameters.\n\n\npolicy_parameters\ndict\nNone\nThe parameters of the policy.\n\n\nglow_method\nstr\nsum\nMethod to update the g-matrix. If ‘sum’, adds the new value to the current value.If ‘init’, sets the new value to 1."
  },
  {
    "objectID": "lib_nbs/envs/core.html",
    "href": "lib_nbs/envs/core.html",
    "title": "Environments",
    "section": "",
    "text": "source\n\n\n\n Abstract_Env (state:object)\n\nAbstract environment from which other environments can de derived."
  },
  {
    "objectID": "lib_nbs/envs/core.html#invader-game",
    "href": "lib_nbs/envs/core.html#invader-game",
    "title": "Environments",
    "section": "Invader Game",
    "text": "Invader Game\nAdd a description\n\nsource\n\nInvader_Game_Env\n\n Invader_Game_Env (state=None, transition_matrix=None)\n\nAbstract environment from which other environments can de derived."
  },
  {
    "objectID": "lib_nbs/envs/core.html#grid-world-environment",
    "href": "lib_nbs/envs/core.html#grid-world-environment",
    "title": "Environments",
    "section": "Grid World Environment",
    "text": "Grid World Environment\nGrid world is a navigation task in a 2D environment configurated as a grid, with obstacles. The goal is for the agent to reach a target at a certain position, which entails the agent exploring enough and remembering some of its past actions. Observations consist of the agent’s coordinates and actions of the set of allowed displacements in the grid.\n\nsource\n\nGrid_World_Env\n\n Grid_World_Env (dimensions=(10, 10), N_obstacles=10,\n                 obstacle_locations=None, reward_location=None,\n                 state=None)\n\nAbstract environment from which other environments can de derived."
  },
  {
    "objectID": "lib_nbs/index_docs.html",
    "href": "lib_nbs/index_docs.html",
    "title": "Documentation",
    "section": "",
    "text": "The library is organized with three main modules\n\nAgents: collects different types of agents that observe their environment, update their internal structure and deliberate on new actions\nECMs: gathers different episodic and compositional memories, the internal structure from which agents will sample their next action.\nEnvironments: a collection of environments develop to either showcase some properties of PS or rather as an application.\n\nTo know which agent fits better your particular application, you can see a map of the various PS agents existing, and contained in this library in this link."
  },
  {
    "objectID": "lib_nbs/methods/lib_helpers.html",
    "href": "lib_nbs/methods/lib_helpers.html",
    "title": "Library helpers",
    "section": "",
    "text": "source\n\nCustomABCMeta\n\n CustomABCMeta (name, bases, namespace, **kwargs)\n\n*Metaclass for defining Abstract Base Classes (ABCs).\nUse this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as ‘virtual subclasses’ – these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won’t show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).*"
  },
  {
    "objectID": "lib_nbs/methods/preprocessors.html",
    "href": "lib_nbs/methods/preprocessors.html",
    "title": "Get percept",
    "section": "",
    "text": "source\n\nget_percept\n\n get_percept (observation)\n\n\n\nFactorizor methods\n\nDescription\n\n\nsource\n\nfactorizor\n\n factorizor (percept_dict={})\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\naction_factorizor\n\n action_factorizor (percept_dict={}, num_actions=None)\n\nThis preprocessor acts like factorizor, except it assumes an agent treats a predefined number of actions as a dimension of its percept It thus intialize its percept dictionary with a dictionary of these n actions that label the first n sensory representations of the agent It also handles the addition of the observed action to the percept when get_percept is called"
  },
  {
    "objectID": "lib_nbs/agents/core.html",
    "href": "lib_nbs/agents/core.html",
    "title": "Agents",
    "section": "",
    "text": "This module contains the basic types of Projective Simulation agents.\n\nsource\n\nAbstract_Agent\n\n Abstract_Agent (ECM=None, percept_processor=None, action_processor=None)\n\nAbstract agent class any PS agent should be derived from. Asserts that the necessary methods are implemented.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nECM\nNoneType\nNone\nThe compulsory ECM Object to use. If kept Nonem raises error.\n\n\npercept_processor\nNoneType\nNone\nAn optional object for transforming observations prior to passing to ECM as a percept. Must have method “preprocess”\n\n\naction_processor\nNoneType\nNone\nAn optional object for transforming actions prior to passing to Environment as an actuator state. Must have method “postprocess”\n\n\n\nPS agents must have as parent class Abstract_Agent, and hence must contain two compulsory methods:\n\nsource\n\nAbstract_Agent.update\n\n Abstract_Agent.update ()\n\nUpdates the internal structure of the agent, typically by updating its ECM.\n\nsource\n\n\nAbstract_Agent.deliberate\n\n Abstract_Agent.deliberate ()\n\nReturns the action to be taken by the agent. Typically, this would involve getting a state from the environment, passing it through the percept processor, passing it to the ECM, and then passing the output through the action processor if needed.\n\nsource\n\n\n\nBasic_Agent\n\n Basic_Agent (ECM:object=None, percept_processor:object=None)\n\nSimple, projective simulation (PS) with arbitrary ECM. Uses the typical ECM structure (see module ECMs). Percepts are added to the ECM as new obsevations are encountered\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nECM\nobject\nNone\nECM object\n\n\npercept_processor\nobject\nNone\nPreprocessor object for transforming observations prior to passing to ECM as a percept. Must have method “preprocess”\n\n\n\n\nsource\n\n\nBasic_2Layer_Agent\n\n Basic_2Layer_Agent (num_actions:int, glow:float=0.1, damp:float=0.0,\n                     glow_method:str='sum', policy:str='greedy',\n                     policy_parameters:dict=None, percept_processor=None)\n\nBasic PS agent with a two layer ECM. Percepts are added to the ECM as new obsevations are encountered.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_actions\nint\n\nThe number of available actions. If ECM = is not given, must be int\n\n\nglow\nfloat\n0.1\nThe glow (or eta) parameter. Won’t be used if ECM is given\n\n\ndamp\nfloat\n0.0\nThe damping (or gamma) parameter. Won’t be used if ECM is given\n\n\nglow_method\nstr\nsum\nThe method for updating the glow. See ECMs.Two_Layer for details\n\n\npolicy\nstr\ngreedy\nThe policy to use. See ECMs.Two_Layer for details\n\n\npolicy_parameters\ndict\nNone\nThe parameters for the policy. See ECMs.Two_Layer for details\n\n\npercept_processor\nNoneType\nNone"
  }
]