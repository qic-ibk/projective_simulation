[
  {
    "objectID": "tutorials/applications/foraging/learning_to_forage.html",
    "href": "tutorials/applications/foraging/learning_to_forage.html",
    "title": "Reinforcement Learning",
    "section": "",
    "text": "In order to train an RL agent, we need to have (i) an environment and (ii) a learning method. In this work, we define a foraging environment where the goal of the agent is to find as many targets as possible in a given time. We consider environments with non-destructive -or replenishable- targets, which we implement by displacing the agent a distance \\(l_\\textrm{c}\\) from the center of the found target.\nAs for the agent, we use Projective Simulation (PS) to model its decision making process and learning method. However, other algorithms that work with stochastic policies can also be used.\nFirst, we import the classes that define the environment (TargetEnv), the forager dynamics (Forager), and its learning method.\n\nimport numpy as np\n\nfrom projective_simulation.agents.foraging import Forager\nfrom projective_simulation.envs.foraging import TargetEnv\nfrom tqdm.notebook import tqdm\n\nNote: the class Forager as it currently is inherits the methods of a PS agent for decision making and learning. However, other learning algorithms can be directly implemented by changing this inheritance. The learning algorithm should contain a method for decision making, called deliberate, which inputs a state; and another one for updating the policy, called learn, which inputs a reward.\nWe set up the parameters defining the length of the episodes (number of RL steps) and the number of episodes.\n\nTIME_EP = 200 #time steps per episode\nEPISODES = 1200 #number of episodes\n\nWe initialize the environment.\n\n#Environment parameters\nNt = 100 #number of targets\nL = 100 #world size\nr = 0.5 #target detection radius\nlc = np.array([[1.0],[1]]) #cutoff length\n\n#Initialize environment\nenv = TargetEnv(Nt, L, r, lc)\n\nWe initialize the agent. As states, the agent perceives the value of an internal counter that keeps track of the number of small steps that it has performed without turning. The possible actions are continue walking in the same direction or turning. The agent performs a small step of length \\(d=1\\) in any case after making a decision. Let’s define the parameters of the PS forager agent and initialize it:\n\nNUM_ACTIONS = 2 # continue in the same direction, turn\nSIZE_STATE_SPACE = np.array([TIME_EP]) # one state per value that the counter may possibly have within an episode.\n#--the last two entries are just placeholders here, but the code is general enough to implement ensembles of interacting agents that forage together.--\nGAMMA = 0.00001 #forgetting parameter in PS\nETA_GLOW = 0.1 #glow damping parameter in PS\n\n#set a different initialization policy\nINITIAL_DISTR = np.ones((2, TIME_EP))\nINITIAL_DISTR[0, :] = 0.99\nINITIAL_DISTR[1, :] = 0.01\n    \n\n#Initialize agent\nagent = Forager(num_actions=NUM_ACTIONS,\n                size_state_space=SIZE_STATE_SPACE,\n                gamma_damping=GAMMA,\n                eta_glow_damping=ETA_GLOW,\n                initial_prob_distr=INITIAL_DISTR)\n\nWe run the learning process.\n\nfor e in tqdm(range(EPISODES)):\n        \n    #restart environment and agent's counter and g matrix\n    env.init_env()\n    agent.agent_state = 0\n    agent.reset_g()\n\n    for t in range(TIME_EP):\n        \n        #step to set counter to its min. value n=1\n        if t == 0 or env.kicked[0]:\n            #do one step with random direction (no learning in this step)\n            env.update_pos(1)\n            #check boundary conditions\n            env.check_bc()\n            #reset counter\n            agent.agent_state = 0\n            #set kicked value to false again\n            env.kicked[0] = 0\n            \n        else:\n            #get perception\n            state = agent.get_state()\n            #decide\n            action = agent.deliberate(state)\n            #act (update counter)\n            agent.act(action)\n            \n            #update positions\n            env.update_pos(action)\n            #check if target was found + kick if it is\n            reward = env.check_encounter()\n                \n            #check boundary conditions\n            env.check_bc()\n            #learn\n            agent.update(reward)\n\n\n\n\n\nFor more details, please look at the rl_opts repository (link), which was developed for this project, and from which we inherited all functions. Shortly that library will be deprecated and everything will be run from projective_simulation."
  },
  {
    "objectID": "tutorials/applications/foraging/learning_to_reset.html",
    "href": "tutorials/applications/foraging/learning_to_reset.html",
    "title": "Learning to reset in target search problems",
    "section": "",
    "text": "In this notebook, we exemplify how to train agents in a target search problems, similar to the foraging problems we deal with in the rest of this library. In this case we consider a single target.\nThe agents we consider here have an important addition: they are able to reset to the origin. As we show in our paper, this helps agents reach much better efficiencies compared to a non-resetting strategy.\nLet’s start this tutorial by importing some necesarry libraries:"
  },
  {
    "objectID": "tutorials/applications/foraging/learning_to_reset.html#sharp-and-exponential",
    "href": "tutorials/applications/foraging/learning_to_reset.html#sharp-and-exponential",
    "title": "Learning to reset in target search problems",
    "section": "Sharp and exponential",
    "text": "Sharp and exponential\nWe start by looking at the sharp and exponential resetting strategies. We start by defining the parameters for each strategy:\n\n# Reset rate for exponential\nrates = np.logspace(-2.5,-1.25,100 )\n\n# Reset time for sharp\nresets = np.linspace(5, 250, 100).astype(np.int64)\n\nNow we perform a simulation of the target search problem for each strategy and parameter. The library contains functions that launch a parallel simulation over the number of resets / rates, based on the number of cores you have:\n\nfrom projective_simulation.envs.foraging import parallel_Reset1D_exp, parallel_Reset1D_sharp\n\nFeel free to increase reps to get better results (for the paper we used reps = 1e5).\n\nreps = int(1e3)\ntime_ep = int(1e4)\n\nreps = int(1e2)\ntime_ep = int(1e2)\n\nrews_exp = np.zeros((reps, len(Ls), len(rates)))\nrews_sharp = np.zeros((reps, len(Ls), len(resets)))\n\nfor idxL, L in enumerate(tqdm(Ls)):\n\n    for idxr in range(reps):\n    \n        rews_exp[idxr, idxL] = parallel_Reset1D_exp(time_ep, rates, L, D)\n        \n        rews_sharp[idxr, idxL] = parallel_Reset1D_sharp(time_ep, resets, L, D)\n\n\n\n\nWe can now plot the efficiencies of each resetting time for the sharp distribution:\n\ncmap = plt.get_cmap('Blues')\ncolors = cmap(np.linspace(0.3, 1, len(Ls)))\n\nfig, ax = plt.subplots()\n\nfor i in range(len(Ls)):\n    ax.plot(resets, rews_sharp.mean(0)[i], c = colors[i])\n\n# Add a colorbar\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=Ls.min(), vmax=Ls.max()))\nsm._A = []\ncbar = plt.colorbar(sm, ax=ax)\ncbar.set_label('Target distance')\n\nax.set_xlabel('Reset time')\nax.set_ylabel('Average reward')\nax.set_title('Sharp reset')\n\nText(0.5, 1.0, 'Sharp reset')\n\n\n\n\n\nand for each rate of the exponential distribution:\n\ncmap = plt.get_cmap('Reds')\ncolors = cmap(np.linspace(0.3, 1, len(Ls)))\n\nfig, ax = plt.subplots()\n\nfor i in range(len(Ls)):\n    ax.plot(np.log10(rates), rews_exp.mean(0)[i], c = colors[i])\n\n# Add a colorbar\nsm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=Ls.min(), vmax=Ls.max()))\nsm._A = []\ncbar = plt.colorbar(sm, ax=ax)\ncbar.set_label('Target distance')\n\nax.set_xlabel('Reset time')\nax.set_ylabel('Average reward')\nax.set_title('Exponential reset')\n\nText(0.5, 1.0, 'Exponential reset')"
  },
  {
    "objectID": "tutorials/applications/foraging/learning_to_reset.html#learning",
    "href": "tutorials/applications/foraging/learning_to_reset.html#learning",
    "title": "Learning to reset in target search problems",
    "section": "Learning",
    "text": "Learning\n\nDidactic implementation: training a single agent\nHere we will focus on implementing all steps of a training loop in a didactic way.\nWe start by defining the RL agent. We use the Forager provided by the library:\n\nfrom projective_simulation.agents.foraging import Forager\n\n\n# Number of actions of the agent. here we consider 2 actions: diffuse or reset.\nnum_actions = 2\n\n# Size of the state space: in the current context, means the maximum counter the agent can reach before compulsory turning\nsize_state_space = np.array([int(2e3)])\n\n# For the PS parameter, we choose the best parameters we chose from a grid search (see below how to do it)\ngamma_damping = 3e-6\neta_glow_damping = 0.12\n\n\n# We now define the agent (may take time in the first run due to numba compilation\nReset_agent = Forager(num_actions, \n                      size_state_space,\n                      gamma_damping, eta_glow_damping)\n\nNext we define the Reset environment:\n\nfrom projective_simulation.envs.foraging import ResetEnv_1D\n\n\nReset_env = ResetEnv_1D(L = Ls[0], # Distance of the target to the origin. We consider for this example the first distance from above.\n                  D = D # Diffusion coefficient of the agent\n                 )\n\nFinally, we perform the RL training loop:\n\n# Number of episodes\nepisodes = 300 \n# Number of steps per episode (we use same as above for the sharp and exponential)\ntime_ep = int(1e4)\n\n# To keep track of the obtained rewards\nsave_rewards = np.zeros(episodes)\n\nfor ep in tqdm(range(episodes)):\n        \n        # Initialize the environment and agent's counter and g-matrix\n        Reset_env.init_env()\n        Reset_agent.agent_state = 0\n        Reset_agent.reset_g()\n\n        for t in range(time_ep):\n\n            # The efficient agent we created needs to keep track when the last\n            # update to the h_matrix happened. \n            Reset_agent.N_upd_H += 1\n            Reset_agent.N_upd_G += 1\n\n            # Get the agent's state (i.e. its current counter)\n            # This differs a bit from typical RL scenarios in which the state comes from the environment,\n            # but changes nothing in terms of the whole RL loop.\n            state = Reset_agent.get_state()\n            \n            # If we reached the maximum state space, we perform turn action\n            if state == Reset_agent.h_matrix.shape[-1]:\n                action = 1\n            # Else we sample an action from the agent's policy\n            else: \n                action = Reset_agent.deliberate(state)\n                \n            # Now we implement this action into the state, which here means updating the agent's counter.\n            # Again, this is slightly different from typical RL where the action is implemented in the environment,\n            # but actually changes nothing to a normal RL implementation (see paper for details)\n            Reset_agent.act(action)\n\n            # We now update the position of the agent in the environment. This also checks if the agnet reached a target\n            # and return a reward = 1 if so.\n            reward = Reset_env.update_pos(action)            \n\n            # If we got a reward or reached the maximum no update value for the H_matrix (for efficient agents), we update \n            # the h_matrix\n            if reward == 1 or Reset_agent.N_upd_H == Reset_agent.max_no_H_update-1:\n                Reset_agent._learn_post_reward(reward)\n\n            # Now we make sure that the state of the agent (counter = 0) is updated if we got the reward\n            if reward != 0:\n                Reset_agent.agent_state = 0\n\n            # We keep track of the number of rewards obtained:\n            save_rewards[ep] += reward\n\nWe can now see how the agent learned by plotting the rewards (i.e. targets acquired) per episode. We will also compare to the best sharp and exponential strategies (horizontal line):\n\nThis figure corresponds to Fig. 2a in our paper.\n\n\nplt.plot(save_rewards, label = 'Efficient agent')\n\n\nplt.axhline(rews_sharp.mean(0)[0].max(), color = 'red', linestyle = '--', label = 'Sharp reset')\nplt.axhline(rews_exp.mean(0)[0].max(), color = 'green', linestyle = '--', label = 'Exponential reset')\n\n\nplt.legend()\n\nplt.xlabel('Episodes')\nplt.ylabel('Rewards')\n\nText(0, 0.5, 'Rewards')\n\n\n\n\n\nWe can see that with only few episodes, the agent is already able to go from a very bad initial strategy to something that outperforms the exponential strategy and closely matches the efficiency of the sharp resetting!\n\nAgent’s strategy\nIn our paper we showed how the strategy learned by the agent converges to the sharp reset strategy. To do so, we have to look at the agent’s policy, defined in this case by its h-matrix.\n\nThis figures corresponds to Figure 3a from our paper.\n\n\n# The policy is calculated by normalizing the h_matrix\npolicy_reset = Reset_agent.h_matrix[1] / Reset_agent.h_matrix.sum(0)\n\n# Let's plot the first points of the policy. Beyond this, the policy converges to the initial policy\n# at 0.5, because the agent will always reset after reaching those counter values.\nplt.plot(policy_reset[:50], label = 'Trained policy')\n\nplt.axhline(0.5, color = 'k', linestyle = '--', label = 'Initial policy', zorder = -1)\n\nplt.legend()\nplt.xlabel('Counter')\nplt.ylabel('Probability of the reset action')\n\nText(0, 0.5, 'Probability of the reset action')\n\n\n\n\n\n\n\n\nGrid Search and Multi-Agent Training\nThe rl_opts library provides functionality for parallel training of multiple agents, enabling comprehensive benchmarking of their efficiencies. This feature allows us to perform grid searches and compare the performance of different agents under various conditions.\n\nfrom projective_simulation.agents.foraging import run_agents_reset_1D\n\nWe start by defining the training specifications. For the paper, in the case of Reset 1D, we used the following:\n\n\n\n\n\n\n\nParameter\nValue\n\n\n\n\nEpisodes\n1000\n\n\nSteps per episdoe\n5000\n\n\nNumber of agents\n190\n\n\nmultiplier_agents\n5\n\n\nD\n0.5\n\n\nDistances (as defined above)\nnp.arange(5, 15)\n\n\nMaximum counter value\n2000\n\n\n\\(\\gamma\\)\nnp.logspace(-9, -5.5, 10)\n\n\n\\(\\eta\\)\nnp.linspace(0.05, 0.3, 10)\n\n\n\n\nImportant: the next cell can take quite long to run, depending on your computational capabilities. We have set at a minimum such that you can still perform some analysis from the outputs. If you just want to explore how the function works, decrease the number of episodes, the number of steps per episode, as well as the number of gammas and etas explored.\n\n\n# Training specs. Commented are the original values used in the paper, but we will use a smaller numbers for this example\nepisodes = int(1e2) # paper: int(1e3)\ntime_ep = int(5e2) # paper: int(5e3)\n\n# Number of agents, defined here by the numbe of cores available multiplied by the number\n# of runs we want to perform (multiplier_agents)\nnum_cores = int(numba.get_num_threads()*0.8)\nmultiplier_agents = 5\n\n\n# Size of the state space: in the current context, means the maximum counter the agent can reach before compulsory turning\nsize_state_space = np.array([int(2e3)])\n\n# Projective simulation parameters\ngammas = np.logspace(-9, -5.5, 10)\netas = np.linspace(0.05, 0.3, 10)\n\n# Now we loop over the different parameters and distances\nrews = np.zeros((len(Ls), len(gammas), len(etas), num_cores*multiplier_agents, episodes))\nfor idxL, L in enumerate(tqdm(Ls)):\n    for idxg, gamma in enumerate(gammas):\n        for idxe, eta in enumerate(etas):\n\n            # Now we run the parallel training. This function spits out the rewards and the h_matrices. We will only keep the rewards\n            # here. The h_matrices were used during our analysis to create the plots of the policies (as the example above) as well\n            # as the analysis of the resetting times.\n            rews[idxL, idxg, idxe], h_matrices = run_agents_reset_1D(episodes = int(episodes), time_ep = int(time_ep), \n                                                                    N_agents = num_cores,\n                                                                    num_runs = multiplier_agents,\n                                                                    D = D, L = L,\n                                                                    size_state_space = size_state_space,\n                                                                    gamma_damping = gamma,\n                                                                    eta_glow_damping = eta, \n                                                                    )\n\nWe can now take a look at how the average accuracy was for the grid of parameters at each distance:\n\nfig, axs = plt.subplots(1, len(Ls), figsize=(3*len(Ls), 3))\n\nfor idxL, L in enumerate(Ls):\n    avg_efficiency = rews[idxL, :, :, :, -1].mean(axis=-1)\n    cax = axs[idxL].matshow(avg_efficiency, aspect='auto')\n    axs[idxL].set_title(f'L = {L}')\n\n    axs[idxL].set_xticks(np.arange(0, len(etas), 2))\n    axs[idxL].set_xticklabels([f'{etas[i]:.2f}' for i in range(0, len(etas), 2)], rotation=90)\n    axs[idxL].set_yticks(np.arange(0, len(gammas), 2))\n    axs[idxL].set_yticklabels([f'{gammas[i]:.1e}' for i in range(0, len(gammas), 2)])\n\n    axs[idxL].xaxis.set_ticks_position('bottom')\n\n    if idxL == 0:\n        axs[idxL].set_ylabel(r'$\\gamma$')\n    axs[idxL].set_xlabel(r'$\\eta$')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nHere, yellow means a bigger efficiency. This plots have been created with very few epsiodes, hence their noise. Running them for 100x episodes would lead to much clearer plots. The best parameters for each \\(L\\) were the following:\n\n\n\n\\(L\\)\n\\(\\gamma\\)\n\\(\\eta\\)\n\n\n5\n3.16e-06\n0.122\n\n\n6\n1.29e-06\n0.122\n\n\n7\n5.27e-07\n0.151\n\n\n8\n5.27e-07\n0.151\n\n\n9\n3.59e-08\n0.206\n\n\n10\n1.00e-09\n0.220"
  },
  {
    "objectID": "tutorials/applications/foraging/learning_to_reset.html#sharp-resetting",
    "href": "tutorials/applications/foraging/learning_to_reset.html#sharp-resetting",
    "title": "Learning to reset in target search problems",
    "section": "Sharp resetting",
    "text": "Sharp resetting\nOur proposed baseline for this problem is again a sharp resetting strategy. For the turning time, we will also consider a sharp strategy. Indeed, in our work we found that the best turning point is at \\(\\sim L + 1\\) (see paper for details).\nIf you want to play with the sharp baseline, inputting arbitray reset and turning times, you can do with the following:\n\nfrom projective_simulation.envs.foraging import search_loop_turn_reset_sharp\n\n\nturn = env_TurnRes.dist_target + 1 \nreset = env_TurnRes.dist_target + 2\n\nlength_sharp_run = int(1e7) # Feel free to increase to get a better estimate\n\nefficiency_sharp = search_loop_turn_reset_sharp(T = length_sharp_run, # Number of steps of the search\n                                    reset = reset, # After how many steps to reset\n                                    turn = turn, # After how many steps to turn\n                                    env = env_TurnRes # The environment\n                                    )"
  },
  {
    "objectID": "tutorials/applications/foraging/learning_to_reset.html#didactic-example-training-a-single-agent",
    "href": "tutorials/applications/foraging/learning_to_reset.html#didactic-example-training-a-single-agent",
    "title": "Learning to reset in target search problems",
    "section": "Didactic example: training a single agent",
    "text": "Didactic example: training a single agent\nAs before, let’s start with the basics! We will first define the new environment and agent. For the later, it is actually the same agent Forager, but with now 3 actions:\n\n# Number of actions of the agent, 3: continue, turn and reset.\nnum_actions = 3\n\n# Size of the state space. Because now we have two counters (turn and reset), the state space is 2D.\nsize_state_space = np.array([100, 100])\n\n# For the PS parameter, we performed a grid search and found the best parameters as shown above\n# IMPORTANT: these parameters were indeed used for all distances in the paper!\ngamma_damping = 1.93e-6\neta_glow_damping = 0.086\n\n\n# We now define the agent (may take time in the first run due to numba compilation\nTurnReset_agent = Forager(num_actions, \n                        size_state_space,\n                        gamma_damping, eta_glow_damping)\n\n\n# Number of episodes\nepisodes = 300 \n# Number of steps per episode (we use same as above for the sharp and exponential)\ntime_ep = int(1e4)\n\nsave_rewards = np.zeros(episodes)\n\nfor ep in tqdm(range(episodes)):\n    #initialize environment and agent's counter and g matrix\n    # Improve documentation\n\n    # Initialize the environment and the agent's counters and g-matrix\n    env_TurnRes.init_env()\n\n    turn_counter = 0\n    reset_counter = 0\n\n    TurnReset_agent.reset_g()\n\n    for t in range(time_ep):\n\n        # Update counters for matrices updates\n\n        # The efficient agent we created needs to keep track when the last\n        # update to the h_matrix happened. \n        TurnReset_agent.N_upd_H += 1\n        TurnReset_agent.N_upd_G += 1    \n\n\n        # Get the agent's state (i.e. its current counter)\n        state = np.array([turn_counter, reset_counter])\n        \n        # Get the action from the agent's policy \n        action = TurnReset_agent.deliberate(state)\n            \n        # Update the counters based on the action. This would\n        # typically be done inside of the environment, but because the state\n        # is in the case inherent to the agent, we have to do it here.\n        if action == 0: # Continue\n            turn_counter += 1  \n            reset_counter += 1 \n\n        elif action == 1: # Turn\n            turn_counter = 0  \n            reset_counter += 1 \n\n        elif action == 2: # Reset            \n            # Note that resetting also resets the turn counter, as we sample a new direction\n            turn_counter = 0     \n            reset_counter = 0 \n            \n        \n        # We now send the action to the environment to update the position\n        reward = env_TurnRes.update_pos(True if action == 1 else False, \n                                        True if action == 2 else False)\n\n        # If we got a reward or reached the maximum no update value for the H_matrix (for efficient agents), we update\n        # the h_matrix\n        if reward == 1:\n            TurnReset_agent._learn_post_reward(reward)            \n\n            # After receiving a reward, we also reset the counters\n            turn_counter = 0\n            reset_counter = 0                \n\n        if TurnReset_agent.N_upd_H == TurnReset_agent.max_no_H_update-1:\n            TurnReset_agent._learn_post_reward(reward)\n                \n        # Saving the reward\n        save_rewards[ep] += reward\n\n\nplt.plot(save_rewards/time_ep, label = 'Efficient agent')\n\n\n# We compare with the sharp reset efficiency\nplt.axhline(efficiency_sharp/length_sharp_run, color = 'red', linestyle = '--', label = 'Sharp reset')\n\n\nplt.legend()\n\nplt.xlabel('Episodes')\nplt.ylabel('Rewards')\n\nText(0, 0.5, 'Rewards')\n\n\n\n\n\nAs we see, the agent learns, and steadily reaches the sharp baseline! While the proposed set of learning parameters have shown to be quite robust, different runs will yield different training efficiencies. Nonetheless, in average all agents reach the baseline with sufficient episodes at the current target distance.\nNow, as we did above, we can take a look at the learned strategy, namely the policy of the agent. In this case, because we have two actions, we will have something more complex:\n\nThis figure corresponds to Fig. 5a in our paper.\n\n\nfig, axs = plt.subplots(1, 3, figsize=(12, 4))\n\n\ncurrent_dist = env_TurnRes.dist_target\n\nfor i, (action, cmap) in enumerate(zip(['continue', 'turn', 'reset'],\n                                       ['Blues', 'Reds', 'Greens'])\n                                    ):\n\n    m = (TurnReset_agent.h_matrix[i] / TurnReset_agent.h_matrix.sum(0)).reshape(size_state_space)\n    \n    \n    axs[i].matshow(m, cmap = cmap)\n\n    axs[i].xaxis.set_ticks_position(\"bottom\")\n    axs[i].invert_yaxis()\n\n    axs[i].axvline(current_dist, ls = '--', c = 'k', alpha = 0.8)\n\nplt.setp(axs, aspect = 'auto', xlim = (-0.5, 10), ylim = (-0.5, 15))\nplt.setp(axs[0], ylabel = 'Reset counter')\nplt.setp(axs[:], xlabel = 'Turn counter');\n\n\n\n\nThese plots, analogous to those of Fig. 5 in our paper, show the behaviour of the agent. As we explain in the paper, there is a very consistent behaviour (you can run the training again to see a very similar result!): the agent has high probability of continuing until the vertical line at \\(L\\), entailing a step of length \\(L+1\\) (python numbering :P). It then turns. Because of the short training, the rest of the behaviour shown in the paper is still to be learned! You can run the trainings longer to see the same clean patter we show in the paper!"
  },
  {
    "objectID": "tutorials/index_tutorials.html",
    "href": "tutorials/index_tutorials.html",
    "title": "Tutorials",
    "section": "",
    "text": "We have compiled a series of tutorials that take you from the very basics of Projective Simulation (PS) to applications presented in research papers."
  },
  {
    "objectID": "tutorials/index_tutorials.html#learning-ps",
    "href": "tutorials/index_tutorials.html#learning-ps",
    "title": "Tutorials",
    "section": "Learning PS",
    "text": "Learning PS\nThese tutorials are designed to introduce the fundamentals of PS.\n\nIntro to PS: This notebook guides you through the basics of a PS agent equipped with a two-layer ECM. You will learn how the agent updates its internal structure based on reinforcement learning."
  },
  {
    "objectID": "tutorials/index_tutorials.html#applications",
    "href": "tutorials/index_tutorials.html#applications",
    "title": "Tutorials",
    "section": "Applications",
    "text": "Applications\nHere you will find tutorials illustrating the use of PS in various applications, often developed within scientific papers.\n\nLearning to reset in target search problems: This notebook explores how a PS agent can learn an efficient resetting strategy for searching randomly distributed targets."
  },
  {
    "objectID": "tutorials/index_tutorials.html#contributing",
    "href": "tutorials/index_tutorials.html#contributing",
    "title": "Tutorials",
    "section": "Contributing",
    "text": "Contributing\nIf you would like to contribute, please see this tutorial on contributing guidelines."
  },
  {
    "objectID": "tutorials/contributing/contributing.html",
    "href": "tutorials/contributing/contributing.html",
    "title": "Contribution guide",
    "section": "",
    "text": "To make it easier to develop PS agents and structures, we provide Abstract classes for Agents and Episodic and Compositional Memories (ECM) designed to guide you through the process. These classes include some compulsory modules that must be implemented for your code to qualify as a “true” PS module. Let’s review them so you can start coding your own PS agents!\nIf you are not yet familiar with PS or how they learn in a Reinforcement Learning environment, we recommend starting with this tutorial.\n\n\nIn the PS framework, agents must implement two compulsory modules: - deliberate: typically returns an action given an input state\n- update: updates the internal structure of your agent\nThat’s it, the rest is up to you. Let’s look at an example. First, import the Abstract_Agent class from the library:\n\nfrom projective_simulation.agents.core import Abstract_Agent\n\nAnd now let’s create our new shinny agent. To illustrate what happens when something goes wrong, let’s “forget” to define the module update:\n\nclass my_new_agent(Abstract_Agent):\n    \n    def __init__(self, num_actions):\n        # Takes a random action\n        self.num_actions = num_actions\n\n    def deliberate(self):\n        return np.random.randint(self.num_actions)\n\nNow let’s call our new class and see what happens:\n\nagent = my_new_agent(3)\n\nTypeError: Cannot instantiate class my_new_agent because it is missing method(s): update.\nPlease implement all abstract methods (see documentation of class for details).\n\n\nUps! As you see, and as expected, the agent is missing the update module. Let’s correct that and double check:\n\nclass my_new_BETTER_agent(Abstract_Agent):\n    \n    def __init__(self, num_actions):\n        self.num_actions = num_actions\n\n    def deliberate(self):\n        # Takes a random action\n        return np.random.randint(self.num_actions)\n\n    def update(self):\n            pass\n\n\nagent = my_new_BETTER_agent(3)\n\n\nPS: As you can see, the framework does not require the modules to actually perform any operations—only that they are defined. For example, if your research focuses on agents that do not update themselves, you can simply define a dummy update module as shown above.\n\n\n\n\nThe second main building block of the library is the Episodic and Compositional Memory (ECM). An ECM is typically represented as a graph that stores percepts, encoding observations, internal knowledge, or actions. However, this library does not enforce any specific ECM structure.\nThe only compulsory module is:\n\nsample: typically used to generate an action by performing a random walk through the ECM.\n\nLet’s now briefly review what a typical ECM looks like, based on the abstract ECM class (a simplification of projective_simulation.ECMs.core.Two_Layer).\n\nfrom projective_simulation.ECMs.core import Abstract_ECM\n\n\nclass ECM_2layers(Abstract_ECM):\n\n    def __init__(self,\n                 num_actions, # number of actions the agent performs from this ECM\n                 num_states # number of different states of the environment\n                ):\n        # Let's define a \"two-layer\" h-matrix (see learning PS tutorial for more).\n        self.hmatrix = np.random.rand(num_states, num_actions)\n\n    def sample(self, state):\n        # Sample greedly the action with the highest h-value\n        h_values = self.hmatrix[state]\n        action = h_values.argmax()  \n        return action\n\n\necm = ECM_2layers(5,5)\necm.sample(2)\n\nnp.int64(4)\n\n\nAnd that is all that is fixed by the framwork, from here you are free to create agents and ECMs at will!"
  },
  {
    "objectID": "tutorials/contributing/contributing.html#the-nbdev-package",
    "href": "tutorials/contributing/contributing.html#the-nbdev-package",
    "title": "Contribution guide",
    "section": "The nbdev package",
    "text": "The nbdev package\nThis library is based on nbdev. This is a nice tutorial on everything you need to learnd about it.\nIn the rest of this notebook we will highlight the minimum needed from the library to contribute to the projective_simulation. In particular, we will show you the few magic commands that will allow us to create a python package from notebooks."
  },
  {
    "objectID": "tutorials/contributing/contributing.html#installation",
    "href": "tutorials/contributing/contributing.html#installation",
    "title": "Contribution guide",
    "section": "Installation",
    "text": "Installation\nBefore starting to write your code, use pipto install the library. To do so, clone the repository and do a local installation of the package. For that, you just need to go to the repo’s folder and run:\npip install -e ."
  },
  {
    "objectID": "tutorials/contributing/contributing.html#write-new-code",
    "href": "tutorials/contributing/contributing.html#write-new-code",
    "title": "Contribution guide",
    "section": "Write new code",
    "text": "Write new code\nNow that you have the library installed, let’s write some new code using Jupyter Notebooks. For the moment, let’s do something simple and talk later about how to properly structure the library. Let’s consider that we want to create, inside the library projective_simulation, a new module utils in which we will gather few useful functions.\nTo tell nbdev that we want this notebook to do the previous, we use the default_exp magic command in the following way:\n\nImportant: any nbdev magic command must be preceded by #|\n\nThis will ensure the objects we create within this notebook will be exported to a .py file called contributing_guide.py within the package folder. You can find the latter in the parent repo folder with the name projective_simulation (the chosen name for the library).\n\nImportant: never write in those .py, as their content is automatically generated from the notebooks and any change there will be overwritten :) .\n\nNow let’s write some proper code! We will create a function that we want to be contained in the module utils. To do so, we use the command #| export to tell nbdev that we want the content of this to go to the current module:\n\nsource\n\nrandom_func\n\n random_func (k)\n\nWe need to actively export the current state of the notebook. To do so, you can either run nbdev_export in your terminal (inside the repo folder) or run the following cell:\n\nimport nbdev; nbdev.nbdev_export()\n\n\n> **Important 1:** `nbdev_export` exports the current state of the notebooks in you library. This means the last saved version! While notebooks usually autosave, it is good practice to save the notebook before running the command.\n\n> **Important 2:** because I don't want this command to go to the module, I don't put the `#| export` command in the cell.\n\nAnd that’s it! If you are curious, you can go to the repo folder, then projective_simulation/contributing_guide.py and see that the content of the file is exactly the one in the cell above."
  },
  {
    "objectID": "tutorials/contributing/contributing.html#importing-generated-functions",
    "href": "tutorials/contributing/contributing.html#importing-generated-functions",
    "title": "Contribution guide",
    "section": "Importing generated functions",
    "text": "Importing generated functions\nBecause the package has been installed through pip, we can now import the function we just created. Let’s see how. First, restart the notebook’s kernel. Now, you can import the random_func function as:\n\nfrom projective_simulation.contributing_guide import random_func\n\n\nrandom_func(2)\n\n0.31480691257004256"
  },
  {
    "objectID": "tutorials/contributing/contributing.html#contributing-to-the-repo",
    "href": "tutorials/contributing/contributing.html#contributing-to-the-repo",
    "title": "Contribution guide",
    "section": "Contributing to the repo",
    "text": "Contributing to the repo\nNotebooks have a lot of useless metadata in them (i.e. the count of the cell execution, the environment you are using,…). This would make impossible contribute to a shared repo, as for instance different contributors may be using different environment names, which would end up in a conflict. To avoid this, nbdev has created hooks that deal with this problem (check their documentation if you want to know details). For now, you just need to do the following:\n\nAfter installing nbdev (should have been done automatically when installing projective_simulation as I put it as a requirement), run the following in your terminal, inside the repo’s folder:\n\nnbdev_install_hooks\n\nMOST IMPORTANTLY, before pushing any changes to the origin, be sure to have done two things:\n\nExport the changes you did in the notebook (see above)\nClean the metadata of your notebooks using the command nbdev_clean"
  },
  {
    "objectID": "tutorials/contributing/using_git.html",
    "href": "tutorials/contributing/using_git.html",
    "title": "Intro to Git for PS Users - Intro",
    "section": "",
    "text": "git clone https://@github.com/qic-ibk/projective_simulation.git\n\nwhere <PAT> is your personal access token.\n\nCreating A Branch\nThe best practice for collaborative projects it to work in your own branch of the repository. Creating a branch essentially means that git keeps track of two versions of the repository that you can easily switch between. When you have finished a task or a project, you can “merge” branches back together. Generally, if merging two branches would be destructive, meaning any data from either branch would be overwritten, you will recieve a warning and an opportunity to review any conflicts. Creating your own branch means that you can push any changes you make to the remote repository without overwriting anything and give other team members an opportunity to review any conflicts before the merge is made. To create a new branch, use the command\n\ngit branch \n\n\n**Important**: Creating a new branch does automatically change the version you are working on to that branch! To do this, you need to checkout the branch:\n\n\ngit checkout \n\n\nYou can view a list of all available branches using\n\n\ngit branch\n\n\n\nCommitting Your Changes\nOnce you have created your own branch of the repository, you can work in the repository however you choose. You can create a snapshot of the repository at any time, allowing you to revert to that repository state later. We won’t discuss reverting in this document, but it is recommended that you commit your changes often. This is done in a two step process. First, you need to “stage” the files you want git to update with its new snapshot, then you commit all of the staged files:\n\ngit add file1.py git add file2.py git commit -m “Commit Demonstration”\n\nThe -m (“message”) is optional, but it is highly (highly) recommended that you include a short message with every commit that makes it clear what changes you have made. As a general rule-of-thumb, if you are staging multiple files or need long commit messages to describe your changes, you aren’t commiting often enough! The simple habit of making a commit any time you complete a task in your workflow will go a long way toward helping other people understand your work, and can be a real life saver if you ever want to go back and remember exactly how you did something, or try a new approach starting from an old version of code, etc. . .\n\n\nPushing Your Changes to the Remote Repository\nWhen you are ready to add your work to the remote PS repository, you can do this either by “pushing” your local repository or by making a “pull request” of the remote repository administrators. Pushing is much simpler, but if your work creates conflicts you will overwrite data in the remote repository. Note that conflicts are not caused by changes in files per se but by differences in commits. For example, if you push your branch to the remote repository, then delete a file locally, commit and push again, there won’t be a conflict! The conflict occurs if somebody else has already pushed a commit that isn’t the branch you are trying to push from your local repository. You can easily recover the deleted file by reverting to an old commit. If you are working using your own branch, you can thus always push it to the remote repository safely:\n\ngit push origin \n\nThe argument “origin” simply tells git that you are pushing your local repository to the repository from which it was cloned.\nIf you are working on a branch of the repository collaboratively, or you want to push your changes directly to the “master” brach of the remote repository you should always pull from the remote repository before you push. When you pull from the remote repository, git will try to add any changes made in the remote repository since the last time you pulled from (or cloned) it. If any of these changes would be destructive, you will get a warning and the pull will fail. There are ways to override this, but we won’t discuss them here: if you get a warning about conflicts when both attempting to push and to pull a branch, your collaborative workflow is not working and you should discuss how to proceed with your teammates. The effective approach to making changes to a shared branch such as “master” goes as follows:\n\ngit pull origin master git merge  -m “local changes merge” git push origin master\n\nNote that merging automatically makes a commit to the master branch (or whatever branch  orginates from). Thus, even if your merge requires you to manage some conflicts (for example, your branch deleted a few line of code), you should be able to push to the remote repository without trouble. You aren’t deleting those lines of code, you are just adding a commit that doesn’t include them! Of course, such changes should still be discussed with project administrators - even though the deleted code is recoverable, it doesn’t mean other teammates want to revert to an old commit to find the file version that has it!\nWith that, you should be equipped to work collaboratively with the PS team! Happy Coding (:"
  },
  {
    "objectID": "tutorials/learning_ps/introduction_to_ps.html",
    "href": "tutorials/learning_ps/introduction_to_ps.html",
    "title": "Introduction to PS",
    "section": "",
    "text": "In this tutorial, we present how to set up a simple projective simulation (PS) agent to learn to play the invader game. To do so, we will first set up the environment and create a basis instantiation of PS agent with a two-layer ECM. We will then train the agent during episodes of deliberation and updates and plot the final learning curve."
  },
  {
    "objectID": "tutorials/learning_ps/introduction_to_ps.html#install-the-ps-package",
    "href": "tutorials/learning_ps/introduction_to_ps.html#install-the-ps-package",
    "title": "Introduction to PS",
    "section": "Install the PS package",
    "text": "Install the PS package\nStart by making sure that you have access to the package: clone the repository using git clone https://github.com/qic-ibk/projective_simulation.git from your terminal, in your project’s folder. Install the package with the command pip install -e .. To remove dependencies on user-specific metadata, run the command nbdev_install_hooks."
  },
  {
    "objectID": "tutorials/learning_ps/introduction_to_ps.html#set-up-the-environment",
    "href": "tutorials/learning_ps/introduction_to_ps.html#set-up-the-environment",
    "title": "Introduction to PS",
    "section": "Set up the environment",
    "text": "Set up the environment\nThe invader opposes two agents. The invader attempts to enter a city and shows a sign to the defender to indicate his intention to move right or left. In order to defend his territory, the defender learns to move in the matching direction in order to block the defender. The invader can either declare his true intention or he can lie and show the opposite direction instead. The defender must adapt his strategy accordingly.\nLet us now set the environment up by creating the invader game environment:\n\nimport matplotlib.pyplot as plt \nimport numpy as np\n\nimport projective_simulation.envs.core as environments\n\n\nenv = environments.Invader_Game_Env()"
  },
  {
    "objectID": "tutorials/learning_ps/introduction_to_ps.html#create-the-agent",
    "href": "tutorials/learning_ps/introduction_to_ps.html#create-the-agent",
    "title": "Introduction to PS",
    "section": "Create the agent",
    "text": "Create the agent\nFor this task, we PS-agent that has two actions (“go right” and “go left”) and an ECM organized in two layers suffices. Deliberation will operate by transitioning directly from percepts to actions in a random walk.\nAt the same time, we set the hyperparameters of the agent. The damping parameter \\(\\gamma\\) represents forgetting: when large, the agent forgets what it has reinforced in the previous episodes. The edge weight, the h-values, are updated as follows for an edge linking clip \\(i\\) to clip \\(j\\) after receiving a reward \\(R\\) from the environment: \\[\\begin{equation}\nh_{ij}^{(t)} = (1-\\gamma) \\cdot h_{ij}^{(t-1)} + \\gamma \\cdot h_{ij}^{(0)} + R\\cdot g_{ij}^{(t)}\n\\end{equation}\\]\nThe glow parameter \\(\\eta\\) decides how quickly the agent forgets it sampled a specific edge to deliberate: if one, the agent can remember and reward exactly one step, whereas if it takes a smaller value, edges that were sampled in the past will be reinforced proportionally to how far in the past they were used: \\[\\begin{equation}\ng_{ij}^{(t)} = \\begin{cases}\n1 &\\text{ if edge $i-j$ was sampled last} \\\\\n(1-\\eta) g_{ij}^{(t-1)} &\\text{otherwise}\n\\end{cases}\n\\end{equation}\\]\nSince the invader game environment does not require any memory beyond one step, we set both parameters to 0 for this task.\n\nimport projective_simulation.agents.core as agents\n\n# Create a basic two-layer agent, with two actions\nagent = agents.Basic_2Layer_Agent(num_actions=2,\n                                 glow=1,\n                                 damp=0,\n                                 policy = 'softmax',\n                                 policy_parameters=1)"
  },
  {
    "objectID": "tutorials/learning_ps/introduction_to_ps.html#train-the-agent",
    "href": "tutorials/learning_ps/introduction_to_ps.html#train-the-agent",
    "title": "Introduction to PS",
    "section": "Train the agent",
    "text": "Train the agent\nIntegrate agent and environment to create a percept-action loop during which the agent deliberates and updates its memory.\n\n# Store the reward history of the agent\n\nreward_history = []\nN_episodes = 100\n# Initialize the sign of the invader\nfor episode in range(N_episodes):\n    percept = env.get_observation()\n    action = agent.deliberate(percept)\n    reward = env.get_reward(action, liar=False)\n    agent.update(reward)\n    reward_history.append(reward)\n    env.transition(action)\n\n\n# Plot the learning curve\nplt.plot(list(range(len(reward_history))), \n              [np.mean([reward_history[step] for step in range(i-10,i+1) if step >= 0]) for i in range(N_episodes)])\nplt.xlabel(\"episodes\")\nplt.ylabel(\"rewards\")\nplt.show()\n\n\n\n\nWe can check the attributes of the agent:\n\n# Check the percepts the agent has encountered\nprint(\"percepts: \", agent.ECM.percepts)\n\n# Check the h-values of the agent\nprint(\"ECM:\", agent.ECM.hmatrix)\n\npercepts:  {'left': 0, 'right': 1}\nECM: [[  1. 527.]\n [474.   1.]]\n\n\nWe can now train 10 agents to have an average learning curve.\n\nN_agents = 10\nreward_history = []\n\nfor i in range(N_agents):\n    reward_history.append([])\n    env = environments.Invader_Game_Env()\n    agent = agents.Basic_2Layer_Agent(num_actions=2,\n                                     glow=1,\n                                     damp=0,\n                                     policy = 'softmax',\n                                     policy_parameters=1)\n    for episode in range(N_episodes):\n        percept = env.get_observation()\n        action = agent.deliberate(percept)\n        reward = env.get_reward(action, liar=False)\n        agent.update(reward)\n        reward_history[i].append(reward)\n        env.transition(action)\n\n# Plot the average learning curve\nmean_rewards = np.mean(np.array(reward_history), axis=0)\nplt.plot(list(range(len(reward_history[0]))), \n              [np.mean([mean_rewards[step] for step in range(i-10,i+1) if step >= 0]) for i in range(N_episodes)])"
  },
  {
    "objectID": "tutorials/learning_ps/introduction_to_ps.html#effect-of-the-forgetting-parameter-gamma",
    "href": "tutorials/learning_ps/introduction_to_ps.html#effect-of-the-forgetting-parameter-gamma",
    "title": "Introduction to PS",
    "section": "Effect of the forgetting parameter \\(\\gamma\\)",
    "text": "Effect of the forgetting parameter \\(\\gamma\\)\nIn order to understand the importance of the different hyperparameters, we train the agents in different scenarios. We start by exploring the role of the forgetting parameter. For this, we switch the strategy of the invader from non-liar to liar in the middle of the training, and explore the impact this switch on the learning curve for agents with different learning rates.\n\nN_agents = 100\nreward_history = []\ndamp_params = [1/50, 1/10, 1/5]\n\nfor d in range(len(damp_params)):\n    reward_history.append([])\n    for i in range(N_agents):\n        reward_history[d].append([])\n        env = environments.Invader_Game_Env()\n        agent = agents.Basic_2Layer_Agent(num_actions=2,\n                                        glow=1,\n                                        damp=damp_params[d],\n                                        policy = 'softmax',\n                                        policy_parameters=1)\n        for episode in range(N_episodes):\n            percept = env.get_observation()\n            action = agent.deliberate(percept)\n            reward = env.get_reward(action, liar=episode > N_episodes // 2)\n            agent.update(reward)\n            reward_history[d][i].append(reward)\n            env.transition(action)\n\n# Plot the average learning curve\nmean_rewards = [np.mean(np.array(reward_history[d]), axis=0) for d in range(len(damp_params))]\nfig, ax = plt.subplots(figsize=(10, 6))\nfor d in range(len(damp_params)):\n    ax.plot(list(range(N_episodes)), \n              [np.mean([mean_rewards[d][step] for step in range(i-10,i+1) if step >= 0]) for i in range(N_episodes)],\n              label=f'damp={damp_params[d]:.2f}')\nax.legend()\nax.set_xlabel('Episode')\nax.set_ylabel('Average Reward')\nax.set_title('Effect of the forgetting parameter on the learning curve')\nplt.show()"
  },
  {
    "objectID": "tutorials/learning_ps/introduction_to_ps.html#test-the-effect-of-the-glow-parameter",
    "href": "tutorials/learning_ps/introduction_to_ps.html#test-the-effect-of-the-glow-parameter",
    "title": "Introduction to PS",
    "section": "Test the effect of the glow parameter",
    "text": "Test the effect of the glow parameter\nIn order to understand the role of the glow damping parameter, we need to set up an environment that requires the agent to remember more than a single percept to properly contextualize its policy. Therefore, we use the Grid World environment that consists of a 2D grid with a number of obstacles. The goal of the agent is to reach a target at a precise location.\nSince rewards in this environment are sparse (only a single cell leads to a reward), reinforcement must be applied to the whole trajectory for the agent to learn to navigate the whole grid towards the target. Therefore, we now make use of glow and tune its damping parameter to values smaller than 1.\n\n# Plot the environment\ndef plot_Grid_World(obs_loc, reward_loc, initial_loc):\n    # Plot the grid with obstacles, reward, initial location, and learnt policy arrows\n    # Get grid dimensions\n    grid_shape = env.dimensions\n    fig, ax = plt.subplots(figsize=(9,6))\n    ax.set_xlim(0, grid_shape[1])\n    ax.set_ylim(0, grid_shape[0])\n    ax.set_xticks(np.arange(grid_shape[1]))\n    ax.set_yticks(np.arange(grid_shape[0]))\n    \n    # Draw obstacles\n    for loc in obs_loc:\n        ax.add_patch(plt.Rectangle((loc[1], loc[0]), 1, 1, color='black', alpha=0.5))\n    \n    # Draw reward location\n    ax.add_patch(plt.Rectangle((reward_loc[1], reward_loc[0]), 1, 1, color='gold', alpha=0.5))\n    ax.text(reward_loc[1]+0.25, reward_loc[0]+0.6, 'Target')\n    \n    # Draw initial location\n    ax.add_patch(plt.Rectangle((initial_loc[1], initial_loc[0]), 1, 1, color='cyan', alpha=0.5))\n    ax.text(initial_loc[1]+0.5, initial_loc[0]+0.6, 'Start', ha='center', va='bottom', color='black')\n    \n    ax.set_title(\"Grid World Environment\")\n    ax.set_xticklabels(np.arange(grid_shape[1]))\n    ax.set_yticklabels(np.arange(grid_shape[0]))\n    ax.invert_yaxis()\n    plt.grid(True)\n    plt.show()\n\n\nobs_loc = [(1,2), (2,2), (3,2),\n           (4,5),\n           (0,7), (1,7), (2,7)]\nreward_loc = (0,8)\ninitial_loc = (2,0)\nenv = environments.Grid_World_Env(dimensions=(6, 9),\n                                  N_obstacles=len(obs_loc), obstacle_locations=obs_loc,\n                                  reward_location=reward_loc, state=None)\n\n# Plot the environment\nplot_Grid_World(obs_loc, reward_loc, initial_loc)\n\n\n\n\nThe glow mechanism marks the transitions a PS agent has deliberated over in the recent past. When a reward is received from the environment, the reinforcement of the marked edges is proportional to how strongly they are still “glowing’’ in the memory of the agent.\nThe damping parameter (glow in the code below, or \\(\\eta\\) above) determines how fast the glow of edges decays. Consequently, it allows agents to reinforce a controlled number of edges even though only one transition actually received a reward.\nLet us explore the influence of glow in the Grid World environment above by training agents with two-layer ECMs but with different glow parameters.\n\nN_agents = 100\nN_episodes = 50\nN_steps_history = []\ng_damp_params = np.array([0, 0.04, 0.08, 0.12, 0.15, 0.20, 1.])\nN_rewards = []\nagents_opt = []\n\nN_steps_history = np.zeros((len(g_damp_params), N_agents, N_episodes))\nfor d in range(len(g_damp_params)):\n    print(f\"Glow parameter: {g_damp_params[d]}\")\n    agents_opt.append([])\n    N_rewards.append(0)\n    for i in range(N_agents):\n        agent = agents.Basic_2Layer_Agent(\n            num_actions=4,\n            glow=g_damp_params[d],\n            damp=0,\n            policy='softmax',\n            policy_parameters=1,  # Higher beta for sharper policy\n            glow_method='init')   # Use 'init' for trajectory credit assignment\n        \n        for episode in range(N_episodes):\n            n_step = 0\n            env.reset_position(initial_state=initial_loc)\n            percept = env.get_observation()\n            reward = 0\n            while reward == 0:\n                action = agent.deliberate(percept)\n                env.transition(action, periodic=False)\n                percept = env.get_observation()\n                reward = 1 * env.get_reward()\n                agent.update(reward)\n                if n_step >= 200:  # Prevent infinite loops\n                    break\n                n_step += 1\n\n            N_rewards[d] += reward\n            N_steps_history[d,i,episode] = n_step\n            if episode < N_episodes - 1:\n                agent.ECM.gmatrix = np.zeros_like(agent.ECM.gmatrix)\n        \n        agents_opt[d].append(agent)\n\nGlow parameter: 0.0\nGlow parameter: 0.04\nGlow parameter: 0.08\nGlow parameter: 0.12\nGlow parameter: 0.15\nGlow parameter: 0.2\nGlow parameter: 1.0\n\n\nLet us plot the learning curve and trained model to understand how glow affects learning in the grid world environment.\n\ndef plot_learning_curves(N_steps_history, N_episodes, g_damp_params):\n    # Plot the average number of steps to reach the goal\n    fig, ax = plt.subplots(figsize=(10, 6))\n    colors= ['blue', 'orange', 'green', 'red', 'purple', 'cyan' , 'brown']\n    mean_steps = [np.mean(N_steps_history[d,:,:], axis=0) for d in range(len(g_damp_params))]\n    std_steps = [np.std(np.array(N_steps_history[d]), axis=0) for d in range(len(g_damp_params))]\n    for d in range(len(g_damp_params)):\n        ax.plot(list(range(N_episodes)), mean_steps[d],\n                label=f'glow={g_damp_params[d]:.2f}', color=colors[d])\n    ax.legend()\n    ax.grid(True)\n    ax.set_xlabel('Episode')\n    ax.set_ylabel('Average Number of Steps to Reach the Goal')\n    ax.set_title('Effect of the glow parameter on the number of steps to reach the goal')\n    plt.show()\n\n\nplot_learning_curves(N_steps_history, N_episodes, g_damp_params)\n\n\n\n\nGlow in this scenario is essential for agents to learn (see the brown curve). For the agents to learn as efficiently as possible, the value for the damping parameter should be tuned carefully, according to the size of the environment, and how many steps are relevant to the reward that is received.\nIn particular in this environment, a damping parameter between 0.04 and 0.08 seems to work best. In what follows, we plot glow matrix of an agent on the grid to give intuition about its contribution to learning. Glowing edges between locations on the grid (percepts in the ECM) and directions (actions) are represented by arrows on the grid. At each location that was visited, the size of the arrow is proportional to its glow value.\n\nfrom matplotlib.patches import FancyArrow\nfrom projective_simulation.methods.transforms import _softmax\n\n# Plot the grid with obstacles, reward, initial location, and learnt policy arrows\ndef plot_glow_GW(agents_opt, agent_idx, glow_damping, glow_idx, env, obs_loc, reward_loc, initial_loc):\n    # Get grid dimensions\n    grid_shape = env.dimensions\n    action_colors = ['blue', 'orange', 'green', 'red']  # Colors for the arrows0\n    \n    fig, ax = plt.subplots(figsize=(9,6))\n    ax.set_xlim(0, grid_shape[1])\n    ax.set_ylim(0, grid_shape[0])\n    ax.set_xticks(np.arange(grid_shape[1]))\n    ax.set_yticks(np.arange(grid_shape[0]))\n    \n    # Draw obstacles\n    for loc in obs_loc:\n        ax.add_patch(plt.Rectangle((loc[1], loc[0]), 1, 1, color='black', alpha=0.5))\n    \n    # Draw reward location\n    ax.add_patch(plt.Rectangle((reward_loc[1], reward_loc[0]), 1, 1, color='gold', alpha=0.5))\n    ax.text(reward_loc[1]+0.25, reward_loc[0]+0.6, 'Target')\n    \n    # Draw initial location\n    ax.add_patch(plt.Rectangle((initial_loc[1], initial_loc[0]), 1, 1, color='cyan', alpha=0.5))\n    ax.text(initial_loc[1]+0.5, initial_loc[0]+0.85, 'Start', ha='center', va='bottom', color='black')\n    \n    # Arrow directions: ↑, →, ↓, ←\n    arrow_deltas = [(0.35, 0), (-0.35, 0), (0, -0.35), (0, 0.35)]\n    \n    for i in range(grid_shape[0]):\n        for j in range(grid_shape[1]):\n            cell = (i,j)\n            # Skip obstacles and reward\n            if cell in obs_loc or cell == reward_loc:\n                continue\n            percept = str(cell)\n            if percept in agents_opt[glow_idx][agent_idx].ECM.percepts:\n                glow_values = agents_opt[glow_idx][agent_idx].ECM.gmatrix[agents_opt[glow_idx][agent_idx].ECM.percepts[percept],:]\n                for a, glow in enumerate(glow_values):\n                    if glow > 0.05:  # Only draw arrows for significant probabilities\n                        dx, dy = arrow_deltas[a]\n                        ax.add_patch(FancyArrow(j+0.5, i+0.5, dx*glow, dy*glow, width=0.05*glow, length_includes_head=True,\n                                               color = action_colors[a], alpha = 0.85))\n            else:\n                ax.text(j+0.5, i+0.5, '.', ha='center', va='center', color='grey', fontsize=12)\n    \n    ax.set_title(\"Grid World Environment\")\n    ax.set_xticklabels(np.arange(grid_shape[1]))\n    ax.set_yticklabels(np.arange(grid_shape[0]))\n    ax.invert_yaxis()\n    plt.grid(True)\n    plt.title(f\"Example of the distribution of glow values (damping = {glow_damping})\")\n    #plt.gca().invert_yaxis()\n    plt.show()\n\n\nplot_glow_GW(agents_opt, 0, g_damp_params[1], 1, env, obs_loc, reward_loc, initial_loc)\n\n\n\n\nWith the right damping parameter (here 0.04 works), glow allows the agent to reinforce sequences of actions that are just long enough to reach the target. If the damping is too strong (for example 0.20), as shown below, the agent struggles to learn the right actions at the beginning of the sequence because the corresponding edges are associated with negligible values of glow by the end of the sequence. Conversly, if damping is not strong enough (0 being an extreme case), the action sequence that is rewarded incorporates actions that did not contribute to reaching the target.\n\nplot_glow_GW(agents_opt, 0, g_damp_params[-2], -2, env, obs_loc, reward_loc, initial_loc)\n\n\n\n\n\nplot_glow_GW(agents_opt, 20, g_damp_params[0], 0, env, obs_loc, reward_loc, initial_loc)\n\n\n\n\nFinally, we show the final policy of a trained agent on the grid, where the length of each arrows is proportional to the probability of sampling the corresponding action in the final policy of the agent.\n\ndef plot_policy_GW(agents_opt, agent_idx, glow_damping, glow_idx, env, obs_loc, reward_loc, initial_loc):\n    # Get grid dimensions\n    grid_shape = env.dimensions\n    action_colors = ['blue', 'orange', 'green', 'red']  # Colors for the arrows\n    \n    fig, ax = plt.subplots(figsize=(9,6))\n    ax.set_xlim(0, grid_shape[1])\n    ax.set_ylim(0, grid_shape[0])\n    ax.set_xticks(np.arange(grid_shape[1]))\n    ax.set_yticks(np.arange(grid_shape[0]))\n    \n    # Draw obstacles\n    for loc in obs_loc:\n        ax.add_patch(plt.Rectangle((loc[1], loc[0]), 1, 1, color='black', alpha=0.5))\n    \n    # Draw reward location\n    ax.add_patch(plt.Rectangle((reward_loc[1], reward_loc[0]), 1, 1, color='gold', alpha=0.5))\n    ax.text(reward_loc[1]+0.25, reward_loc[0]+0.6, 'Target')\n    \n    # Draw initial location\n    ax.add_patch(plt.Rectangle((initial_loc[1], initial_loc[0]), 1, 1, color='cyan', alpha=0.5))\n    ax.text(initial_loc[1]+0.5, initial_loc[0]+0.85, 'Start', ha='center', va='bottom', color='black')\n    \n    # Arrow directions: ↑, →, ↓, ←\n    arrow_deltas = [(0.35, 0), (-0.35, 0), (0, -0.35), (0, 0.35)]\n    \n    for i in range(grid_shape[0]):\n        for j in range(grid_shape[1]):\n            cell = (i,j)\n            # Skip obstacles and reward\n            if cell in obs_loc or cell == reward_loc:\n                continue\n            percept = str(cell)\n            if percept in agents_opt[glow_idx][agent_idx].ECM.percepts:\n                h = agents_opt[glow_idx][agent_idx].ECM.hmatrix[agents_opt[glow_idx][agent_idx].ECM.percepts[percept],:]\n                probs = _softmax(beta=1, x=h)\n                for a, prob in enumerate(probs):\n                    if prob > 0.05:  # Only draw arrows for significant probabilities\n                        dx, dy = arrow_deltas[a]\n                        ax.add_patch(FancyArrow(j+0.5, i+0.5, dx*prob, dy*prob, width=0.05*prob, length_includes_head=True,\n                                               color = action_colors[a], alpha = 0.85))\n            else:\n                ax.text(j+0.5, i+0.5, '.', ha='center', va='center', color='grey', fontsize=12)\n    \n    ax.set_title(\"Grid World Environment\")\n    ax.set_xticklabels(np.arange(grid_shape[1]))\n    ax.set_yticklabels(np.arange(grid_shape[0]))\n    ax.invert_yaxis()\n    plt.grid(True)\n    plt.title(\"Trained Policy\")\n    plt.show()\n\n\nplot_policy_GW(agents_opt, 0, g_damp_params[1], 1, env, obs_loc, reward_loc, initial_loc)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Projective Simulation",
    "section": "",
    "text": "Get started | Documentation | Tutorials\nProjective Simulation (PS) is a framework for studying agency from a physical perspective. PS agents learn from interaction with their environment and base their decisions on episodic memory. Internally, their deliberation corresponds to random walks on a clip network (graph), which provides an interpretable route to physical realizations and analysis.\nThis library provides all the necessary tools to develop, deploy, and train PS agents in a variety of environments. Its content ranges from entry-level tutorials introducing the core principles of PS to cutting-edge research on the latest developments and applications. It is actively developed by the QIC group at UIBK."
  },
  {
    "objectID": "index.html#installation",
    "href": "index.html#installation",
    "title": "Projective Simulation",
    "section": "Installation",
    "text": "Installation\nProjective simulation is available for python>=3.10 via\npip install projective_simulation\nThis will also install all required dependencies."
  },
  {
    "objectID": "index.html#getting-started",
    "href": "index.html#getting-started",
    "title": "Projective Simulation",
    "section": "Getting Started",
    "text": "Getting Started\nTo begin exploring Projective Simulation, we recommend to start with the tutorials, which cover everything from the basics to more advanced applications.\nIf you wish to develop or deploy PS agents, please refer to the documetation."
  },
  {
    "objectID": "index.html#contributing",
    "href": "index.html#contributing",
    "title": "Projective Simulation",
    "section": "Contributing",
    "text": "Contributing\nIf you want to contribute to this library, please take a look at the contributing tutorial."
  },
  {
    "objectID": "index.html#citing",
    "href": "index.html#citing",
    "title": "Projective Simulation",
    "section": "Citing",
    "text": "Citing\nIf you find this package useful in your projects, you can either cite the specific PS paper most relevant to your work (see a list here) or use the following to cite the library as a whole:\n!soon!"
  },
  {
    "objectID": "webpage/research.html",
    "href": "webpage/research.html",
    "title": "Research",
    "section": "",
    "text": "Here you can find a complete list of papers using PS:"
  },
  {
    "objectID": "webpage/research.html#section",
    "href": "webpage/research.html#section",
    "title": "Research",
    "section": "2025",
    "text": "2025\n\nFree Energy Projective Simulation (FEPS): Active inference with interpretability by Pazem, J., Krumm, M., Vining, A. Q., Fiderer, L. J., & Briegel, H. J. (2025).\nRun-and-Tumble Particles Learning Chemotaxis” by N. Tovazzi, G. Muñoz-Gil and M. Caraglio (2025).\nLearning to reset in target search problems” by G. Muñoz-Gil, H. J. Briegel and M. Caraglio (2025)."
  },
  {
    "objectID": "webpage/research.html#section-1",
    "href": "webpage/research.html#section-1",
    "title": "Research",
    "section": "2024",
    "text": "2024\n\nLearning how to find targets in the micro-world: the case of intermittent active Brownian particles by M. Caraglio, H. Kaur, L. Fiderer, A. López-Incera, H. J. Briegel, T. Franosch, and G. Muñoz-Gil (2024).\nOptimal foraging strategies can be learned” by G. Muñoz-Gil, A. López-Incera, L. J. Fiderer and H. J. Briegel (2024)."
  },
  {
    "objectID": "lib_nbs/ECMs/priming.html",
    "href": "lib_nbs/ECMs/priming.html",
    "title": "Priming",
    "section": "",
    "text": "source\n\nPriming_ECM\n\n Priming_ECM (num_actions:int, glow:float=0.1, damp:float=0.01,\n              softmax:float=0.5, action_primes:list=None)\n\nThis sub-class of the Two-Layer ECM adds a variable for action priming. This variable should be a list of floats, each element of which corresponds to an action in the ECM. These “priming values” are summed with h-values of any edge connected to the associated action node prior to calculating walk probabilites with the softmax function\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_actions\nint\n\nThe number of available actions.\n\n\nglow\nfloat\n0.1\nThe glow (or eta) parameter.\n\n\ndamp\nfloat\n0.01\nThe damping (or gamma) parameter.\n\n\nsoftmax\nfloat\n0.5\nThe softmax (or beta) parameter.\n\n\naction_primes\nlist\nNone\nweights on the probability that deliberation steps into each action. Defaults to 0 for each action\n\n\n\n\nExample\n\ntest_ECM = Priming_ECM(num_actions = 2, action_primes = [0., 1.5])\n#Number of steps to run simulation\nT = 100\ndata_log = [None] * T\nenv = RLGL() #create a default red-light-green-light environment\n\nfor t in range(T):\n    observation = env.get_observation()\n    action = test_ECM.sample(observation)\n    reward = env.get_reward(action)\n    test_ECM.learn(reward)\n    data_log[t] = {\"env_state\": env.state, \"action\": action, \"reward\": reward}\n    env.transition(action)\n\nplt.plot(range(T), [np.mean([data_log[step][\"reward\"] for step in range(i-10,i+1) if step >= 0]) for i in range(T)]) #plot a 10 step moving average of the reward"
  },
  {
    "objectID": "lib_nbs/ECMs/core.html",
    "href": "lib_nbs/ECMs/core.html",
    "title": "Core",
    "section": "",
    "text": "Here we collect different standalone functions that will help us construct different types of ECM\n\nsource\n\n\n\n standard_ps_upd (reward, hmatrix, gmatrix, h_damp, g_damp)\n\n*Given a reward, updates h-matrix and g-matrix following the standard PS update rule:\n\\(h \\leftarrow h - h_{damp}*(h-1)+ reward*g\\)\n\\(g \\leftarrow (1-g_{damp})*g\\)*"
  },
  {
    "objectID": "lib_nbs/ECMs/core.html#pre-built-ecms",
    "href": "lib_nbs/ECMs/core.html#pre-built-ecms",
    "title": "Core",
    "section": "Pre-built ECMs",
    "text": "Pre-built ECMs\nHere we collect the abstract parent class that any ECM should be built upon as well as some pre-built ECM ready to use.\n\nsource\n\nAbstract_ECM\n\n Abstract_ECM (*args, **kwargs)\n\nAbstract agent class any episodic and compositional memory (ECM) should be derived from. Asserts that the necessary methods are implemented. No compulsory input objects are needed.\nPS agents must have as parent class Abstract_ECM, and hence must contain one compulsory method:\n\nsource\n\nAbstract_ECM.sample\n\n Abstract_ECM.sample ()\n\nPerforms a random walk through the ECM. Typically, this implies receiving an input percept and returning an action.\n\nsource\n\n\n\nTwo_Layer\n\n Two_Layer (num_actions:int, g_damp:float, h_damp:float,\n            policy:str='greedy', policy_parameters:dict=None,\n            glow_method:str='sum')\n\n*Two layer ECM. First layer, encoding the percepts observed in an environment, is initially empty (e.g. self.num_percepts = 0). As percepts are observed, they are added to the ECM and to the percept dictionary self.percepts. The second layer, encoding the actions, has size self.num_actions. In practice, the ECM graph is never created. Instead, it is defined indirectly by the h-matrix and g-matrix. Both have size (self.num_percepts, self.num_actions). The input policy (greedy, softmax or other) is used to sample actions based on the h-matrix.\nFor an end-to-end example of how to use this class, see the introductory tutorial notebook on PS agents.*\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_actions\nint\n\nThe number of available actions.\n\n\ng_damp\nfloat\n\nThe glow damping(or eta) parameter.\n\n\nh_damp\nfloat\n\nThe damping (or gamma) parameter.\n\n\npolicy\nstr\ngreedy\nIf ‘greedy’, uses a greedy policy that samples the most action based on the h-matrix. If ‘softmax’, uses a softmax policy that samples an action based on the h-matrix and a temperature parameter (encoded in policy_parameters).If object, uses this object to sample action. Input must be h_values corresponding to current percept + arbitrary policy_parameters.\n\n\npolicy_parameters\ndict\nNone\nThe parameters of the policy.\n\n\nglow_method\nstr\nsum\nMethod to update the g-matrix. If ‘sum’, adds the new value to the current value.If ‘init’, sets the new value to 1."
  },
  {
    "objectID": "lib_nbs/envs/foraging_envs.html",
    "href": "lib_nbs/envs/foraging_envs.html",
    "title": "Foraging",
    "section": "",
    "text": "This notebook gathers different kinds of environments for foraging and target search in various scenarios, adapted for their use in the reinforcement learning paradigm."
  },
  {
    "objectID": "lib_nbs/envs/foraging_envs.html#d",
    "href": "lib_nbs/envs/foraging_envs.html#d",
    "title": "Foraging",
    "section": "1D",
    "text": "1D\n\nsource\n\nResetEnv_1D\n\n ResetEnv_1D (L=1.3, D=1.0)\n\nClass defining a 1D environment where the agent can either continue walking or reset to the origin. Reward = 1 if the agent crosses the distance L, else = 0.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nL\nfloat\n1.3\nDistance to cross to get reward\n\n\nD\nfloat\n1.0\nDiffusion coefficient\n\n\n\n\nsource\n\n\nreset_search_loop\n\n reset_search_loop (T, reset_policy, env)\n\nLoop that runs the reset environment with a given reset policy.\n\n\n\n\nDetails\n\n\n\n\nT\nNumber of steps\n\n\nreset_policy\nReset policy\n\n\nenv\nEnvironment\n\n\n\n\nsource\n\n\nparallel_Reset1D_exp\n\n parallel_Reset1D_exp (T, rates, L, D)\n\nRuns the Reset 1D loop in parallel for different exponential resetting rates.\n\nsource\n\n\nparallel_Reset1D_sharp\n\n parallel_Reset1D_sharp (T:int, resets:<built-infunctionarray>, L:float,\n                         D:float)\n\nRuns the Reset 1D loop in parallel for different sharp resetting times.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nT\nint\nNumber of steps\n\n\nresets\narray\nArray of resetting times\n\n\nL\nfloat\nDistance to cross to get reward\n\n\nD\nfloat\nDiffusion coefficient\n\n\nReturns\narray\nRewards obtained for each resetting time"
  },
  {
    "objectID": "lib_nbs/envs/foraging_envs.html#d-1",
    "href": "lib_nbs/envs/foraging_envs.html#d-1",
    "title": "Foraging",
    "section": "2D",
    "text": "2D\n\nsource\n\nResetEnv_2D\n\n ResetEnv_2D (dist_target=0.2, radius_target=0.5, D=1.0)\n\nClass defining a 2D environment where the agent can either continue walking or reset to the origin. Reward = 1 if the agent enters the area defined by the distance and radius of the target.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ndist_target\nfloat\n0.2\nDistance from init position and target\n\n\nradius_target\nfloat\n0.5\nRadius of the target\n\n\nD\nfloat\n1.0\nDiffusion coefficient of the walker\n\n\n\n\nsource\n\n\nparallel_Reset2D_policies\n\n parallel_Reset2D_policies (T, reset_policies, dist_target, radius_target,\n                            D)\n\nRuns the Reset 2D loop in parallel for different sharp resetting times.\n\n\n\n\nType\nDetails\n\n\n\n\nT\n\nNumber of steps\n\n\nreset_policies\n\nArray of reset policies\n\n\ndist_target\n\nDistance of the target\n\n\nradius_target\n\nRadius of the target\n\n\nD\n\nDiffusion coefficient of the walker\n\n\nReturns\narray\nRewards obtained for each reset policy\n\n\n\n\nsource\n\n\nparallel_Reset2D_exp\n\n parallel_Reset2D_exp (T, rates, dist_target, radius_target, D)\n\nRuns the Reset 2D loop in parallel for different sharp resetting times.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nT\n\nNumber of steps\n\n\nrates\n\nReset rates\n\n\ndist_target\n\nDistance of the target\n\n\nradius_target\n\nRadius of the target\n\n\nD\n\nDiffusion coefficient of the walker\n\n\nReturns\narray\nRewards obtained for each exponential rate\n\n\n\n\nsource\n\n\nparallel_Reset2D_sharp\n\n parallel_Reset2D_sharp (T, resets, dist_target, radius_target, D)\n\nRuns the Reset 2D loop in parallel for different sharp resetting times.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\nT\n\nNumber of steps\n\n\nresets\n\nArray of resetting times\n\n\ndist_target\n\nDistance to target\n\n\nradius_target\n\nRadius of the target\n\n\nD\n\nDiffusion coefficient of the walker\n\n\nReturns\narray\nRewards obtained for each sharp resetting time"
  },
  {
    "objectID": "lib_nbs/envs/core.html",
    "href": "lib_nbs/envs/core.html",
    "title": "Core",
    "section": "",
    "text": "source\n\n\n\n Abstract_Env (state:object)\n\nAbstract environment from which other environments can de derived."
  },
  {
    "objectID": "lib_nbs/envs/core.html#invader-game",
    "href": "lib_nbs/envs/core.html#invader-game",
    "title": "Core",
    "section": "Invader Game",
    "text": "Invader Game\nAdd a description\n\nsource\n\nInvader_Game_Env\n\n Invader_Game_Env (state=None, transition_matrix=None)\n\nAbstract environment from which other environments can de derived."
  },
  {
    "objectID": "lib_nbs/envs/core.html#grid-world-environment",
    "href": "lib_nbs/envs/core.html#grid-world-environment",
    "title": "Core",
    "section": "Grid World Environment",
    "text": "Grid World Environment\nGrid world is a navigation task in a 2D environment configurated as a grid, with obstacles. The goal is for the agent to reach a target at a certain position, which entails the agent exploring enough and remembering some of its past actions. Observations consist of the agent’s coordinates and actions of the set of allowed displacements in the grid.\n\nsource\n\nGrid_World_Env\n\n Grid_World_Env (dimensions=(10, 10), N_obstacles=10,\n                 obstacle_locations=None, reward_location=None,\n                 state=None)\n\nAbstract environment from which other environments can de derived."
  },
  {
    "objectID": "lib_nbs/index_docs.html",
    "href": "lib_nbs/index_docs.html",
    "title": "Documentation",
    "section": "",
    "text": "The library is organized with three main modules\n\nAgents: collects different types of agents that observe their environment, update their internal structure and deliberate on new actions\nECMs: gathers different episodic and compositional memories, the internal structure from which agents will sample their next action.\nEnvironments: a collection of environments develop to either showcase some properties of PS or rather as an application.\n\nTo know which agent fits better your particular application, you can see a map of the various PS agents existing, and contained in this library in this link."
  },
  {
    "objectID": "lib_nbs/methods/lib_helpers.html",
    "href": "lib_nbs/methods/lib_helpers.html",
    "title": "Library helpers",
    "section": "",
    "text": "source\n\nCustomABCMeta\n\n CustomABCMeta (name, bases, namespace, **kwargs)\n\n*Metaclass for defining Abstract Base Classes (ABCs).\nUse this metaclass to create an ABC. An ABC can be subclassed directly, and then acts as a mix-in class. You can also register unrelated concrete classes (even built-in classes) and unrelated ABCs as ‘virtual subclasses’ – these and their descendants will be considered subclasses of the registering ABC by the built-in issubclass() function, but the registering ABC won’t show up in their MRO (Method Resolution Order) nor will method implementations defined by the registering ABC be callable (not even via super()).*"
  },
  {
    "objectID": "lib_nbs/methods/preprocessors.html",
    "href": "lib_nbs/methods/preprocessors.html",
    "title": "Get percept",
    "section": "",
    "text": "source\n\nget_percept\n\n get_percept (observation)\n\n\n\nFactorizor methods\n\nDescription\n\n\nsource\n\nfactorizor\n\n factorizor (percept_dict={})\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\naction_factorizor\n\n action_factorizor (percept_dict={}, num_actions=None)\n\nThis preprocessor acts like factorizor, except it assumes an agent treats a predefined number of actions as a dimension of its percept It thus intialize its percept dictionary with a dictionary of these n actions that label the first n sensory representations of the agent It also handles the addition of the observed action to the percept when get_percept is called"
  },
  {
    "objectID": "lib_nbs/agents/foraging_agents.html",
    "href": "lib_nbs/agents/foraging_agents.html",
    "title": "Foraging",
    "section": "",
    "text": "This notebook gathers different kinds of agents for foraging and target search in various scenarios, adapted for their use in the reinforcement learning paradigm."
  },
  {
    "objectID": "lib_nbs/agents/foraging_agents.html#reset-environment",
    "href": "lib_nbs/agents/foraging_agents.html#reset-environment",
    "title": "Foraging",
    "section": "Reset Environment",
    "text": "Reset Environment\n\nsource\n\ntrain_loop_reset\n\n train_loop_reset (episodes:int, time_ep:int, agent:object, env:object,\n                   h_mat_allT:bool=False, when_save_h_mat=1,\n                   reset_after_reward=True)\n\nTraining loop for a forager agent in a given environment.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nepisodes\nint\n\nNumber of episodes\n\n\ntime_ep\nint\n\nLength of an episode\n\n\nagent\nobject\n\nAgent that will be trained\n\n\nenv\nobject\n\nEnvironment where the agent will be trained\n\n\nh_mat_allT\nbool\nFalse\nIf True saves the h_mat at desired time\n\n\nwhen_save_h_mat\nint\n1\nIf h_mat_allT = True, sets the time where h-matrix is saved\n\n\nreset_after_reward\nbool\nTrue\nIf True, the agents performs a reset after getting a target\n\n\n\n\n\nLaunchers\nWe now prepare some functions that allow us to launch parallel trainings with ease. We have to separate the launchers in 1D and 2D because of numba compilation, which would give errors due to the enviroments asking for different inputs.\n\nsource\n\n\nrun_agents_reset_1D\n\n run_agents_reset_1D (episodes, time_ep, N_agents, D=0.5, L=10.0,\n                      num_actions=2, size_state_space=array([100]),\n                      gamma_damping=1e-05, eta_glow_damping=0.1,\n                      g_update='s', initial_prob_distr=array([], shape=(2,\n                      0), dtype=float64), policy_type='standard',\n                      beta_softmax=3, fixed_policy=array([], shape=(2, 0),\n                      dtype=float64), max_no_H_update=1000,\n                      h_mat_allT=False, when_save_h_mat=1,\n                      reset_after_reward=True, num_runs=None)\n\nLaunches parallel trainings of forager agents in a 1D Reset environment.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nepisodes\n\n\nNumber of episodes\n\n\ntime_ep\n\n\nLength of an episode\n\n\nN_agents\n\n\nNumber of parallel agents\n\n\nD\nfloat\n0.5\nDiffusion coefficient\n\n\nL\nfloat\n10.0\nSize of the environment\n\n\nnum_actions\nint\n2\nNumber of actions\n\n\nsize_state_space\nndarray\n[100]\nSize of the state space\n\n\ngamma_damping\nfloat\n1e-05\nPS damping factor\n\n\neta_glow_damping\nfloat\n0.1\nPS glow damping factor\n\n\ng_update\nstr\ns\nType of G update. Can be ‘s’ (sum) or ‘r’ (reset)\n\n\ninitial_prob_distr\n\n[]\nInitial probability distribution for the H matrix\n\n\npolicy_type\nstr\nstandard\nPolicy type. Can be ‘standard’ or ‘softmax’\n\n\nbeta_softmax\nint\n3\nSoftmax temperature if sotfmax policy is used\n\n\nfixed_policy\n\n[]\nFixed policy for the agent to follow\n\n\nmax_no_H_update\nint\n1000\nMax number of steps without updating the H matrix. After this number, the full H matrix is updated\n\n\nh_mat_allT\nbool\nFalse\nIf True saves the h_mat at desired time\n\n\nwhen_save_h_mat\nint\n1\nIf h_mat_allT = True, sets the time where h-matrix is saved\n\n\nreset_after_reward\nbool\nTrue\nIf True, the agents performs a reset after getting a target\n\n\nnum_runs\nNoneType\nNone\nWhen we want N_agent != number of max cores, we use this to make few runs over the selected number of cores, given by N_agents.\n\n\n\n\nsource\n\n\nrun_agents_reset_2D\n\n run_agents_reset_2D (episodes, time_ep, N_agents, dist_target=10.0,\n                      radius_target=1.0, D=0.5, num_actions=2,\n                      size_state_space=array([100]), gamma_damping=1e-05,\n                      eta_glow_damping=0.1, initial_prob_distr=array([],\n                      shape=(2, 0), dtype=float64),\n                      policy_type='standard', beta_softmax=3,\n                      fixed_policy=array([], shape=(2, 0), dtype=float64),\n                      max_no_H_update=1000, h_mat_allT=False,\n                      when_save_h_mat=1, reset_after_reward=True,\n                      g_update='s', num_runs=None)\n\nLaunches parallel trainings of forager agents in a 2D Reset environment.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nepisodes\n\n\nNumber of episodes\n\n\ntime_ep\n\n\nLength of an episode\n\n\nN_agents\n\n\nNumber of parallel agents\n\n\ndist_target\nfloat\n10.0\nDistance from the origin where the target is located\n\n\nradius_target\nfloat\n1.0\nRadius of the target\n\n\nD\nfloat\n0.5\nDiffusion coefficient\n\n\nnum_actions\nint\n2\nNumber of actions\n\n\nsize_state_space\nndarray\n[100]\nSize of the state space\n\n\ngamma_damping\nfloat\n1e-05\nPS damping factor\n\n\neta_glow_damping\nfloat\n0.1\nPS glow damping factor\n\n\ninitial_prob_distr\n\n[]\nInitial probability distribution for the H matrix\n\n\npolicy_type\nstr\nstandard\nPolicy type. Can be ‘standard’ or ‘softmax’\n\n\nbeta_softmax\nint\n3\nSoftmax temperature if softmax policy is used\n\n\nfixed_policy\n\n[]\nFixed policy for the agent to follow\n\n\nmax_no_H_update\nint\n1000\nMax number of steps without updating the H matrix. After this number, the full H matrix is updated\n\n\nh_mat_allT\nbool\nFalse\nIf True saves the h_mat at desired time\n\n\nwhen_save_h_mat\nint\n1\nIf h_mat_allT = True, sets the time where h-matrix is saved\n\n\nreset_after_reward\nbool\nTrue\nIf True, the agents performs a reset after getting a target\n\n\ng_update\nstr\ns\nType of G update. Can be ‘s’ (sum) or ‘r’ (reset)\n\n\nnum_runs\nNoneType\nNone\nWhen we want N_agent != number of max cores, we use this to make few runs over the selected number of cores, given by N_agents."
  },
  {
    "objectID": "lib_nbs/agents/core.html",
    "href": "lib_nbs/agents/core.html",
    "title": "Core",
    "section": "",
    "text": "This module contains the basic types of Projective Simulation agents.\n\nsource\n\nAbstract_Agent\n\n Abstract_Agent (ECM=None, percept_processor=None, action_processor=None)\n\nAbstract agent class any PS agent should be derived from. Asserts that the necessary methods are implemented.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nECM\nNoneType\nNone\nThe compulsory ECM Object to use. If kept Nonem raises error.\n\n\npercept_processor\nNoneType\nNone\nAn optional object for transforming observations prior to passing to ECM as a percept. Must have method “preprocess”\n\n\naction_processor\nNoneType\nNone\nAn optional object for transforming actions prior to passing to Environment as an actuator state. Must have method “postprocess”\n\n\n\nPS agents must have as parent class Abstract_Agent, and hence must contain two compulsory methods:\n\nsource\n\nAbstract_Agent.update\n\n Abstract_Agent.update ()\n\nUpdates the internal structure of the agent, typically by updating its ECM.\n\nsource\n\n\nAbstract_Agent.deliberate\n\n Abstract_Agent.deliberate ()\n\nReturns the action to be taken by the agent. Typically, this would involve getting a state from the environment, passing it through the percept processor, passing it to the ECM, and then passing the output through the action processor if needed.\n\nsource\n\n\n\nBasic_Agent\n\n Basic_Agent (ECM:object=None, percept_processor:object=None)\n\nSimple, projective simulation (PS) with arbitrary ECM. Uses the typical ECM structure (see module ECMs). Percepts are added to the ECM as new obsevations are encountered\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nECM\nobject\nNone\nECM object\n\n\npercept_processor\nobject\nNone\nPreprocessor object for transforming observations prior to passing to ECM as a percept. Must have method “preprocess”\n\n\n\n\nsource\n\n\nBasic_2Layer_Agent\n\n Basic_2Layer_Agent (num_actions:int, glow:float=0.1, damp:float=0.0,\n                     glow_method:str='sum', policy:str='greedy',\n                     policy_parameters:dict=None, percept_processor=None)\n\nBasic PS agent with a two layer ECM. Percepts are added to the ECM as new obsevations are encountered.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nnum_actions\nint\n\nThe number of available actions. If ECM = is not given, must be int\n\n\nglow\nfloat\n0.1\nThe glow (or eta) parameter. Won’t be used if ECM is given\n\n\ndamp\nfloat\n0.0\nThe damping (or gamma) parameter. Won’t be used if ECM is given\n\n\nglow_method\nstr\nsum\nThe method for updating the glow. See ECMs.Two_Layer for details\n\n\npolicy\nstr\ngreedy\nThe policy to use. See ECMs.Two_Layer for details\n\n\npolicy_parameters\ndict\nNone\nThe parameters for the policy. See ECMs.Two_Layer for details\n\n\npercept_processor\nNoneType\nNone"
  }
]