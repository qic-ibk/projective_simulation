# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/lib_nbs/05_environments.ipynb.

# %% auto 0
__all__ = ['Abstract_Env', 'RLGL', 'Delayed_Response']

# %% ../nbs/lib_nbs/05_environments.ipynb 2
import numpy as np
from abc import ABC, abstractmethod

# %% ../nbs/lib_nbs/05_environments.ipynb 4
class Abstract_Env(ABC):
    """A minimal Environment, every environment should be Derived from this class.

    Examples:
    >>> pass
    """

    def __init__(self, 
                 state: object):
        """
        Args:
            state: an object that defines the state of the environment            
        """
        self.state = state

    @abstractmethod
    def transition(self, action):
        """
        Args:
            action: an action (or actions) to process
        """
        raise NotImplementedError

    @abstractmethod
    def get_observation(self):
        """
        should determine and return an observation for an agent or agents as a function of self.state
        """
        raise NotImplementedError

# %% ../nbs/lib_nbs/05_environments.ipynb 6
class RLGL(Abstract_Env):
    def __init__(self, state = 0, transition_matrix = None):
        self.state = state
        self.state_labels = {0: "red", 1: "green"}
        if transition_matrix is None:
            #create random uniform transition probabilities
            transition_matrix = np.array([[0.5,0.5],[0.5,0.5]])
        assert np.shape(transition_matrix) == (2,2)
        self.transition_matrix = transition_matrix            

    def transition(self, action):
        '''
        In this environment the agents action determines the reward but does not determine the state
        '''
        self.state = np.random.choice(range(2), p = self.transition_matrix[self.state,])

    def get_observation(self):
        return self.state_labels[self.state]

    def get_reward(self, action):
        if action == self.state:
            return 1
        else:
            return 0

# %% ../nbs/lib_nbs/05_environments.ipynb 9
class Delayed_Response(Abstract_Env):
    def __init__(self, 
                 W: int, #number of steps agent must wait before response is evaluated
                 N: int, #number of stimuli (and associated actions) that are presented by random selection to agent at start of trial
                 max_trial_length: int = 10,
                 current_stimulus: int = None, 
                 rewarded_action: int = None, 
                 reward_available: bool = False,
                 trial_time: int = None
                ):
        super().__init__(state = {"current_stimulus": current_stimulus, "rewarded_action": rewarded_action, "reward_available": reward_available, "trial_time": trial_time}) #assigns state to self.state
        self.W = W
        self.N = N
        self.max_trial_length = max_trial_length
        assert self.max_trial_length >= self.W
        if self.state["current_stimulus"] is None:
            self.state["current_stimulus"] = np.random.randint(self.N) + 1  #this function randomly selects an interger from 0 to N-1. Add one because 0 is used to denote no stimulus
        if self.state["rewarded_action"] is None:
            self.state["rewarded_action"] = self.state["current_stimulus"]  #stores which stimulus was presented, and thus the correct action for the agent. 
            #Generally, if the user imputs "current_stimulus = 0", "rewarded_action" should not be 0 or None. This won't break the code, but it will give reward for waiting after W steps, which the environment otherwise will not do 
        if self.state["trial_time"] is None:
            self.state["trial_time"] = 0  #at trial time 0 the agent is presented with a stimulus. 

    def transition(self, action):
        if self.state["reward_available"] or self.state["current_stimulus"] == -1: #reset after reward or fail
            self.reset()
            
        #if agent acted, check if wait time has passed and if so, if action was correct
        elif not action == 0: 
            if self.state["trial_time"] < self.W:
                self.step()
            elif action == self.state["rewarded_action"]: #correct action, give reward
                self.state["reward_available"] = True
                self.step()
            #incorrect action, set fail state
            else:
                self.state["current_stimulus"] = -1
                self.step()

        #if agent did not act, continue trial
        else:
            self.step()      
            
    def step(self):
        #moves the trial one step forward
        if not self.state["current_stimulus"] == -1: #unless in fail state, agent only receives stimulus on trial reset
            self.state["current_stimulus"] = 0
        self.state["trial_time"] += 1
        if self.state["trial_time"] > self.max_trial_length:
            self.reset()

    def reset(self):
        #stats new trial, presenting agent with a new random stimulus
        self.state["trial_time"] = 0
        self.state["current_stimulus"] = np.random.randint(self.N) + 1
        self.state["rewarded_action"] = self.state["current_stimulus"]
        self.state["reward_available"] = False

    def get_observation(self):
        return [self.state["current_stimulus"], int(self.state["reward_available"])] #agents gets current stimulus and reward availability as observation
            
            
