# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/lib_nbs/03_environments.ipynb.

# %% auto 0
__all__ = ['Abstract_Env', 'Invader_Game_Env', 'Grid_World_Env', 'POMDP', 'Cyclic_Env', 'Noisy_Cycle', 'Causal_DBN',
           'Light_And_Lever']

# %% ../nbs/lib_nbs/03_environments.ipynb 3
import numpy as np
from abc import ABC, abstractmethod

# %% ../nbs/lib_nbs/03_environments.ipynb 5
class Abstract_Env(ABC):
    """A minimal Environment, every environment should be Derived from this class.

    Examples:
    >>> pass
    """

    def __init__(self, 
                 state: object):
        """
        Args:
            state: an object that defines the state of the environment            
        """
        self.state = state

    @abstractmethod
    def transition(self, action):
        """
        Args:
            action: an action (or actions) to process
        """
        raise NotImplementedError

    @abstractmethod
    def get_observation(self):
        """
        should determine and return an observation for an agent or agents as a function of self.state
        """
        raise NotImplementedError

# %% ../nbs/lib_nbs/03_environments.ipynb 8
class Invader_Game_Env(Abstract_Env):
    def __init__(self, 
                 state = None, 
                 transition_matrix = None):
        """
        Args:
            state: defines the direction of the sign of the invader, either right (0) or left (1)
            transition_matrix: a 2x2 matrix with the probabilities of transitioning from one state to another.
        If None, a random uniform transition matrix is created.  
        """
        self.state = state
        self.state_labels = {0: "right", 1: "left"}
        #transition_matrix is a 2x2 matrix with the probabilities of transitioning from one state
        if transition_matrix is None:
            #create random uniform transition probabilities
            transition_matrix = np.array([[0.5,0.5],[0.5,0.5]])
        assert np.shape(transition_matrix) == (2,2)
        self.transition_matrix = transition_matrix

    def transition(self, action):
        '''
        In this environment the agents action determines the reward but does not determine the state.
        '''
        self.state = np.random.choice(range(2), p = self.transition_matrix[self.state,])

    def get_observation(self):
        if self.state is None:
            self.state = np.random.choice(range(2))
        return self.state_labels[self.state]
    
    def get_reward(self, action, liar = False):
        """ Args:
            action: the action taken by the agent
            liar: if True, the agent is lying and the reward is always 0
        """
        if not liar:
            if action == self.state:
                return 1
            else:
                return 0
        else:
            if action != self.state:
                return 1
            else:
                return 0

# %% ../nbs/lib_nbs/03_environments.ipynb 10
class Grid_World_Env(Abstract_Env):

    def __init__(self, 
                 dimensions = (10, 10),
                 N_obstacles = 10,
                 obstacle_locations = None,
                 reward_location = None,
                 state = None):
        """
        Args:
            dimensions: a tuple defining the dimensions of the grid world (rows, columns)
            N_obstacles: number of obstacles in the grid world
            state: defines the current position of the agent in the grid world as a tuple (row, column)
        """
        self.state = state
        self.dimensions = dimensions
        self.N_obstacles = N_obstacles

        self.action_labels = {0: "up",
                              1: "down", 
                              2: "left", 
                              3: "right"}
        # Place the reward at a random location in the grid world
        if reward_location is None:
            reward_location = (np.random.randint(0, dimensions[0]), np.random.randint(0, dimensions[1]))
        self.reward_location = reward_location

        # Generate random obstacle locations
        if obstacle_locations is None:
            obstacle_locations = set()
            while len(obstacle_locations) < N_obstacles:
                loc = (np.random.randint(0, dimensions[0]), np.random.randint(0, dimensions[1]))
                if loc != reward_location:  # Ensure the obstacle is not placed on the reward location
                    obstacle_locations.add(loc)
        self.obstacle_locations = list(obstacle_locations)
    
    def reset_position(self, initial_state=None):
        """
        Reset the agent's position to a random location in the grid world.
        """
        if initial_state is None:
            self.state = (np.random.randint(0, self.dimensions[0]), np.random.randint(0, self.dimensions[1]))
            while self.state in self.obstacle_locations:
                self.state = (np.random.randint(0, self.dimensions[0]), np.random.randint(0, self.dimensions[1]))
        else:
            self.state = initial_state

    def transition(self, action, periodic = False):
        '''
        Move the agent in the grid world based on the action taken.
        '''
        action = self.action_labels[action]
        if periodic:
            if action == "right":
                new_state = ((self.state[0])%self.dimensions[0] + 1, self.state[1])
            elif action == "left":
                new_state = ((self.state[0]-1)%self.dimensions[0], self.state[1])
            elif action == "down":
                new_state = (self.state[0], (self.state[1]-1)%self.dimensions[1])
            elif action == "up":
                new_state = (self.state[0], (self.state[1]+1)%self.dimensions[1])
        else:
            if action == "right":
                new_state = (min(self.state[0] + 1, self.dimensions[0] - 1), self.state[1])
            elif action == "left":
                new_state = (max(self.state[0] - 1, 0), self.state[1])
            elif action == "down":
                new_state = (self.state[0], max(self.state[1] - 1, 0))
            elif action == "up":
                new_state = (self.state[0], min(self.state[1] + 1, self.dimensions[1] - 1))
        # Change the state only if the new state is not an obstacle
        if new_state not in self.obstacle_locations:# and new_state != self.reward_location:
            self.state = new_state

    def get_observation(self):
        return str(self.state)
    
    def get_reward(self):
        if self.state == self.reward_location:
            return 1
        else:
            return 0

# %% ../nbs/lib_nbs/03_environments.ipynb 12
class POMDP(Abstract_Env):
    def __init__(self,
                 percepts: np.ndarray,             #An SxK array where S is the number of Percepts and K is the number of categories for each percept. If 1d, converted to Sx1
                 observation_function: np.ndarray, #An NxS array, where N is the number of states in the cycle and S is the number of possible percepts. Rows contain probability distributions
                 transition_function: np.ndarray,  #An NxNxA array, where A is the number of actions. Rows in each slice contain probability distributions.
                 initial_state: int = 0            #Start state of POMDP
                ):
        """
        assert that dimensions of input arguments align and assign input arguments to self
        """
        
        assert isinstance(percepts, np.ndarray)
        if percepts.ndim == 1:
            percepts = percepts[:,np.newaxis]
        assert np.shape(percepts)[0] == np.shape(observation_function)[1]
        assert np.shape(transition_function)[0] == np.shape(observation_function)[0]
        assert np.shape(transition_function)[0] == np.shape(transition_function)[1]
        assert transition_function.ndim == 3
        assert np.isclose(np.sum(transition_function,axis =1), 1, atol=1e-9).all() #all rows sum to 1
        assert np.isclose(np.sum(observation_function,axis =1), 1, atol=1e-9).all() #all rows sum to 1
        assert initial_state in range(np.shape(observation_function)[0])
         
        self.percepts = percepts
        self.observation_function = observation_function
        self.transition_function = transition_function
        state = initial_state
        super().__init__(state = state)

    def transition(self, action):
        """
        randomly select a new state using transition probabilites from current state and action
        """
        if not action in range(np.shape(self.transition_function)[2]):
            raise ValueError("The action input for POMDP transition must be integer valued and within the scope of the transition function")
        transition_probs = self.transition_function[self.state,:, action]
        self.state = np.random.choice(len(transition_probs), p = transition_probs)

    def get_observation(self):
        """
        randomly select a percept using observation probabilites from current state
        """
        percept_probs = self.observation_function[self.state,:]
        percept_index = np.random.choice(len(percept_probs), p = percept_probs)
        return self.percepts[percept_index,:]

# %% ../nbs/lib_nbs/03_environments.ipynb 16
class Cyclic_Env(POMDP):
    """
    An environment that cycles deterministically through a sequence of percepts that may be passed to an agent
    """
    def __init__(self,
                 percept_cycle: np.ndarray, #an N x K array, where N is the number of states in the cycles and K is the number of categories in a percept. If 1d, will be converted to Nx1
                 initial_state: int = 0,
                 supress_warning: bool = False
                ):
        '''
        Sets up a determinstic cycles using the general format of a POMDP
        '''
        if percept_cycle.ndim == 1:
            percept_cycle = percept_cycle[:, np.newaxis]
        
        # Get unique rows and, for each row of percept_cycle, the index of its unique representative
        percepts, inverse = np.unique(percept_cycle, axis=0, return_inverse=True)
        
        # One-hot encode those indices to form the observation function
        observation_function = np.eye(percepts.shape[0], dtype=int)[inverse]       

        #create a cyclic shifted diagonal matrix
        S = np.shape(percept_cycle)[0]
        transition_function = np.roll(np.eye(N = S), shift = 1, axis = 1) 
        transition_function = transition_function[:,:,np.newaxis] #adds a dimension to allow for a single action
        
        super().__init__(percepts = percepts, observation_function = observation_function, transition_function = transition_function, initial_state = initial_state)
        self.supress_warning = supress_warning
        
    def transition(self, action):
        if not action == 0 and not self.supress_warning:
            print('Warning: Cyclic_Env is not action mediated. Action input has been converted to 0')
        action = 0
        super().transition(action)

# %% ../nbs/lib_nbs/03_environments.ipynb 22
class Noisy_Cycle(POMDP):
    def __init__(self,
                 percepts: np.ndarray,      #an SxK array where S is the number of Percepts and K is the number of categories for each percept
                                            #if 1d, converted to Sx1
                 observation_function: np.ndarray, #An NxS array, where N is the number of states in the cycle and S is the number of possible percepts
                 initial_state: int = 0,
                 supress_warning = False
                ):
        if percepts.ndim == 1:
            percepts = percepts[:,np.newaxis]
            
        #create a cyclic shifted diagonal matrix for transition_function
        S = np.shape(observation_function)[0] #number of percepts
        transition_function = np.roll(np.eye(N = S), shift = 1, axis = 1)
        transition_function = transition_function[:,:,np.newaxis] #adds a dimension to allow for a single action

        super().__init__(percepts = percepts, observation_function = observation_function, transition_function = transition_function, initial_state = initial_state)
        self.supress_warning = supress_warning
        
    def transition(self, action):
        if not action == 0 and not self.supress_warning:
            print('Warning: Cyclic_Env is not action mediated. Action input has been converted to 0')
        action = 0
        super().transition(action)

# %% ../nbs/lib_nbs/03_environments.ipynb 26
import inspect
class Causal_DBN(Abstract_Env):
    def __init__(self,
                 state: np.ndarray, #a one dimensional array. Each element gives the state of a variable.
                 update_functions: dict, #a dictionary of the functions used to update each variable
                 variable_names: np.ndarray = None, #an optional list of variables names. Default is integers. Must match keys of update functions
                 action_variables: np.ndarray = None, #indicates which system variables are under the control of an agent. Used to ensure inputs to transition function are correct
                 causal_network: np.ndarray = None #a square boolean array that indicates whether a variable at t is a parent of another variable at t+1. Optional: merely used to check transition function inputs
                ):
        if variable_names is None:
            variable_names = np.array(range(self.num_variables))

        #check variables
        if not state.ndim == 1:
            raise ValueError("'state' must be a numpy array with a single dimension")
        self.num_variables = np.shape(state)[0]

        if causal_network is not None:
            assert causal_network.dtype == np.bool_
            if not np.shape(causal_network) == (self.num_variables, self.num_variables):
                raise ValueError("causal network must be a square matrix with each dimension equal to the number of variables given by the state input")

        if action_variables is None:
            action_variables = np.full(self.num_variables, fill_value = False)
        for action_variable in variable_names[action_variables]:
            update_functions[action_variable] = self.action_function        
        for key, update_f in update_functions.items():
            if not key in variable_names:
                raise ValueError("Keys of update_function dictionary must correspond to variable names. Default variable names are integer indices")
            assert callable(update_f)            
            ## Check that update function inputs match causal_network
            if causal_network is not None:
                function_parents = list(inspect.signature(update_f).parameters)
                i = np.where(variable_names == key)[0][0] #get parent index (indexes get first match in first dimension)
                if not set(function_parents) == set(variable_names[causal_network[:,i]]): #compare input variables names to children in DBN
                    raise ValueError(f'The update function for {variable_names[i]} does not have input variables that match parents in causal_network')

        if not len(update_functions) == self.num_variables:
            raise ValueError("there must be an update function for each variable in 'state'")


        self.state = state
        self.update_functions = update_functions
        self.variable_names = variable_names
        self.action_variables = action_variables

    def transition(self, action: dict = None):
        if action is None:
            action = {}

        #update action states
        for key, value in action.items():
            if not key in self.variable_names:
                raise ValueError("keys of action dictionary must be environment variable names")
            self.state[self.variable_names == key] = value

        #apply transition functions
        new_states = np.zeros(self.num_variables, dtype = self.state.dtype)
        for variable, update_f in self.update_functions.items():
            required_args = set(inspect.signature(update_f).parameters.keys())
            input_dict = {k: v for k, v in zip(self.variable_names, self.state) if k in required_args}
            new_states[self.variable_names == variable] = update_f(**input_dict)
        self.state = new_states

    def get_observation(self):
        return self.state
    
    def action_function(self, x): #as actions are given as input, the stored update functions for these variables should not do anything
        return x

# %% ../nbs/lib_nbs/03_environments.ipynb 31
class Light_And_Lever(Causal_DBN):
    def __init__(self, interval, state = None):
        self.state_space = {"light": np.array(("off", "green", "blue")),
                            "lever": np.array(("unpressed", "pressed")),
                            "reward_stimulus": np.array(("none", "food", "shock")),
                            "timer": np.array((range(2+interval*2))) #number of states in light cycle, on for green, on for blue, and one for each interval step between the two
                           }
        variable_names = np.array(["light", "lever", "reward_stimulus", "timer"])
        if state is None:
            state = np.array((1,0,0,0)) #default start state green light, unpressed lever, no reward, timer at 0
        update_functions = {"light": self.update_light, "lever": self.bernoulli, "reward_stimulus": self.update_reward, "timer": self.update_timer}
        super().__init__(state, update_functions, variable_names)
        assert np.issubdtype(self.state.dtype, np.integer)
        
    def update_light(self, timer):
        if timer == len(self.state_space["timer"]) -1: #if timer is in last state . . .
            return np.where(self.state_space["light"] == "green")[0][0] #. . . light will turn green. 0 indices get first match in first dimension
        elif timer == len(self.state_space["timer"])/2 - 1: #if timer is one step from half-way . . .
            return np.where(self.state_space["light"] == "blue")[0][0] #. . . light will turn blue
        else:
            return np.where(self.state_space["light"] == "off")[0][0]

    def update_reward(self, light, lever):
        if self.state_space["light"][light] == "green" and self.state_space["lever"][lever] == "pressed":
            return np.where(self.state_space["reward_stimulus"] == "food")[0][0]
        elif self.state_space["light"][light] == "blue" and self.state_space["lever"][lever] == "pressed":
            return np.where(self.state_space["reward_stimulus"] == "shock")[0][0]
        else:
            return np.where(self.state_space["reward_stimulus"] == "none")[0][0]

    def update_timer(self, timer):
        reset_at = len(self.state_space["timer"])
        return (timer + 1) % reset_at

    @staticmethod
    def bernoulli():
        return int(np.random.binomial(1, 0.5)) #1 trial, 50 percept probability 1.

    def get_observation(self):
        return self.state[np.array((0,1,2))] #return tuple of light, lever, and reward
