# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/lib_nbs/02_ECMs.ipynb.

# %% auto 0
__all__ = ['standard_ps_upd', 'Abstract_ECM', 'Two_Layer', 'Priming_ECM', 'Bayesian_Filter', 'Sequence_Memory',
           'Short_Term_Memory', 'sigmoid_fading_rate', 'surprise_advantage_fading_rate', 'Long_Term_Memory']

# %% ../nbs/lib_nbs/02_ECMs.ipynb 5
def standard_ps_upd(reward, hmatrix, gmatrix, h_damp, g_damp):
    """
    Given a reward, updates h-matrix and g-matrix following the standard PS update rule:

    h <- h - h_damp*(h-1)+ reward*g
    g <- (1-g_damp)*g    
    """
    # damping h-matrix
    hmatrix = hmatrix - h_damp*(hmatrix-1.)
    # update h-matrix
    hmatrix += reward*gmatrix
    # update g-matrix
    gmatrix = (1-g_damp)*gmatrix

    return hmatrix, gmatrix

# %% ../nbs/lib_nbs/02_ECMs.ipynb 8
from .methods.lib_helpers import CustomABCMeta
from abc import abstractmethod


class Abstract_ECM(metaclass = CustomABCMeta):
    """
    Abstract agent class any episodic and compositional memory (ECM) should be derived from. Asserts that the necessary methods are implemented.
    """

    def __init__(self):
        '''
        No restrictions on the constructor, as the ECM can be anything that has a sample module.
        '''
        pass

    @abstractmethod
    def sample(self,):
        """
        Performs a random walk through the ECM. Typically, this implies receiving an input percept and returning an action.
        """
        pass

# %% ../nbs/lib_nbs/02_ECMs.ipynb 12
import numpy as np
from .methods.transforms import _softmax

class Two_Layer(Abstract_ECM):
    def __init__(self, 
                 # The number of available actions.
                 num_actions: int, 
                 # The glow damping(or eta) parameter. 
                 g_damp: float, 
                 # The damping (or gamma) parameter. 
                 h_damp: float,
                 # If 'greedy', uses a greedy policy that samples the most action based on the h-matrix. 
                 # If 'softmax', uses a softmax policy that samples an action based on the h-matrix and a temperature parameter (encoded in policy_parameters).
                 # If object, uses this object to sample action. Input must be h_values corresponding to current percept + arbitrary policy_parameters.
                 policy: str = 'greedy',                 
                 # The parameters of the policy.
                 policy_parameters: dict = None,
                 # Method to update the g-matrix. 
                 # If 'sum', adds the new value to the current value.
                 # If 'init', sets the new value to 1.
                 glow_method: str = 'sum',
                ):

        """
        Two layer ECM. First layer, encoding the percepts observed in an environment, is initially empty (e.g. self.num_percepts = 0). As percepts
        are observed, they are added to the ECM and to the percept dictionary self.percepts. 
        The second layer, encoding the actions, has size self.num_actions.
        In practice, the ECM graph is never created. Instead, it is defined indirectly by the h-matrix and g-matrix. 
        Both have size (self.num_percepts, self.num_actions). 
        The input policy (greedy, softmax or other) is used to sample actions based on the h-matrix.

        For an end-to-end example of how to use this class, see the tutorial notebook on Basic PS agents.        
        """

        

        self.num_actions = num_actions

        self.h_damp = h_damp
        self.g_damp = g_damp
        self.glow_method = glow_method

        self.policy = policy
        self.policy_parameters = policy_parameters
        
        # Initialize ECM structures

        #int: current number of percepts.
        self.num_percepts = 0
        #np.ndarray: h-matrix with current h-values. Defaults to all 1.
        self.hmatrix = np.ones([0,self.num_actions])
        #np.ndarray: g-matrix with current glow values. Defaults to all 0.
        self.gmatrix = np.zeros([0,self.num_actions])
        #dict: Dictionary of percepts as {"percept": index}
        self.percepts = {}

    def sample(self, percept: str):
        """
        Given a percept, returns an action and changes the ECM if necessary
        First, if the percept is new, it will be added to the ECM
        Then, an action is selected as a function of the percept and the h-values of edges connected to that percept
        Finally, the g-matrix is updated based on the realized percept-action pair.
        """

        # Add percept to ECM if not already present
        self.add_percept(percept)
        # Get index from dictionary entry
        percept_index = self.percepts[percept]
        # Get h-values
        h_values = self.hmatrix[percept_index]

        # Perform Random Walk through the ECM based on h_values and current policy
        if self.policy == 'greedy': 
            # Sample greedly the action with the highest h-value
            h_values = self.hmatrix[percept_index]
            action = h_values.argmax()   

        elif self.policy == 'softmax':
            # Get probabilities from h-values through a softmax function
            prob = _softmax(self.policy_parameters, h_values)
            # Sample action based on probabilities
            action = np.random.choice(range(self.num_actions), p=prob) 

        else:
            # This considers a custom policy
            action = self.policy(h_values = h_values, **self.policy_parameters)

        # Update g-matrix
        if self.glow_method == 'sum':
            self.gmatrix[int(percept_index),int(action)] += 1.
        if self.glow_method == 'init':
            self.gmatrix[int(percept_index),int(action)] = 1.
            

        return action

    def add_percept(self, percept):
        '''
        Checks if percept is in dictionary and adds to ECM in not
        '''
        if percept not in self.percepts.keys(): 
            self.percepts[percept] = self.num_percepts
            # increment number of percepts
            self.num_percepts += 1
            # add column to h-matrix
            self.hmatrix = np.append(self.hmatrix, 
                                     np.ones([1,self.num_actions]),
                                     axis=0)
            # add column to g-matrix
            self.gmatrix = np.append(self.gmatrix, 
                                    np.zeros([1,self.num_actions]),
                                    axis=0)

    def  learn(self, reward):
        """
        Updates the h-matrix and g-matrix based on the reward received using the standard PS update rule.
        """
        self.hmatrix, self.gmatrix = standard_ps_upd(reward, self.hmatrix, self.gmatrix, self.h_damp, self.g_damp)

# %% ../nbs/lib_nbs/02_ECMs.ipynb 15
from .methods.transforms import _softmax

class Priming_ECM(Two_Layer):
    '''
    This sub-class of the Two-Layer ECM adds a variable for action priming.
    This variable should be a list of floats, each element of which corresponds to an action in the ECM.
    These "priming values" are summed with h-values of any edge connected to the associated action node prior to calculating walk probabilites with the softmax function
    '''
    def __init__(self, 
                 num_actions: int, # The number of available actions.                 
                 glow: float = 0.1, # The glow (or eta) parameter. 
                 damp: float = 0.01, # The damping (or gamma) parameter. 
                 softmax: float = 0.5, # The softmax (or beta) parameter.
                 action_primes: list = None, #weights on the probability that deliberation steps into each action. Defaults to 0 for each action 
                ):
        if action_primes is None:
            action_primes = [0.] * num_actions
        assert len(action_primes) == num_actions

        self.softmax = softmax
        super().__init__(num_actions, glow, damp, 
                         policy = None) # Here I made explicit that the policy is None, as we override the sample method
        self.action_primes = action_primes
        

    def sample(self, percept):
        '''
        Almost identical to the sample function of Two-Layer parent class, but sums h-values and action primes prior to calculating walk probabilities
        '''
        self.add_percept(percept)
        #Perform Random Walk
        # get index from dictionary entry
        percept_index = self.percepts[percept]
        # get h-values
        h_values = self.hmatrix[percept_index]
        #~~~Differences from two-layer sample function within
        assert len(h_values) == len (self.action_primes)
        # get probabilities from h-values and primes through a softmax function
        prob = _softmax(self.softmax, h_values + self.action_primes)
        #~~~~~~~
        # get action
        action = np.random.choice(range(self.num_actions), p=prob)        
        #pdate g-matrix
        self.gmatrix[int(percept_index),int(action)] = 1.
        return action

# %% ../nbs/lib_nbs/02_ECMs.ipynb 18
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from matplotlib.colors import ListedColormap

class Bayesian_Filter(Abstract_ECM):
    """
    Bayesian Filter for recursive state estimation.

    Each state/hypothesis posits:
      1. A probability for all states of K sensory categories (with discrete state spaces).
      2. A probability that each state/hypothesis will be true when the sensory category states are next updated.

    The filter maintains a belief state over these hypotheses/states, assuming they are exhaustive and mutually exclusive.

    Args:
        category_sizes (list): Number of states per sensory category.
        sensory_predictions (np.ndarray): Each row is a hypothesis; columns grouped by category_sizes, each group sums to 1.
        transition_predictions (np.ndarray): Each row is a hypothesis; columns give predicted probabilities for each hypothesis being true at the next step.
        belief_prior (np.ndarray, optional): Initial belief prior. If None, set to uniform.
        log_base (float, optional): Base for logarithms and exponentials.
        data_record (list, optional): List of internal variable names to record each time step. Accepts "all".
        record_until (int, optional): Number of time steps to record. Default -1 disables recording.
    """
    def __init__(
        self,
        category_sizes: list,
        sensory_predictions: np.ndarray,
        transition_predictions: np.ndarray,
        belief_prior: np.ndarray = None,
        log_base=2.,
        data_record: list = [],
        record_until: int = -1
    ):
        self.category_sizes = category_sizes
        self.category_indexer = np.repeat(np.arange(0, len(category_sizes)), category_sizes)
        self.num_hypotheses = np.shape(sensory_predictions)[0]
        self.transition_predictions = transition_predictions
        self.sensory_predictions = sensory_predictions
        self.log_base = log_base

        # --- State initialization ---
        self.percept = np.empty(len(self.category_sizes), dtype=int)
        self.likelihoods = np.zeros(self.num_hypotheses)
        self.belief_prior = np.full(self.num_hypotheses, 1/self.num_hypotheses) if belief_prior is None else belief_prior
        self.sensory_expectation = self.belief_prior @ self.sensory_predictions
        self.belief_posterior = np.full(self.num_hypotheses, 1/self.num_hypotheses)
        self.prepare_data_storage(data_record, record_until)

    def _check_structure(self):
        if sum(self.category_sizes) != np.shape(self.sensory_predictions)[1]:
            raise ValueError("sensory_predictions must have a number of columns equal to the total number of percept category states")

        if np.shape(self.sensory_predictions)[0] != np.shape(self.transition_predictions)[0]:
            raise ValueError("sensory_predictions and transition_predictions must have the same number of rows")
        if np.shape(self.transition_predictions)[0] != np.shape(self.transition_predictions)[1]:
            raise ValueError("transition_predictions must be square")
        
        #~~Enforcement of probability distributions~~
        for k in range(len(self.category_sizes)):
            pred_sum = np.sum(self.sensory_expectation[self.category_indexer == k])
            if not np.abs(pred_sum - 1) < 1e-9:
                raise ValueError(
                    f"Predictions for each sensory category must be a probability distribution. "
                    f"Timer={self.timer}, category={i}"
                )
            for i in range(self.num_hypotheses):
                if not np.abs(np.sum(self.sensory_predictions[i, self.category_indexer == k]) - 1) < 1e-9:
                    raise ValueError("Each sensory category must sum to 1 in every hypothesis")
        if not np.abs(np.sum(self.belief_posterior) - 1) < 1e-9:
            raise ValueError("belief_posterior must sum to 1")
        if not np.abs(np.sum(self.belief_prior) - 1) < 1e-9:
            raise ValueError("belief_prior must sum to 1")
        if not (np.all(self.likelihoods >= 0) and np.all(self.likelihoods <= 1)):
            raise ValueError("likelihoods must be between 0 and 1")
        #~~End enforcement of probability distributions~~

        if self.record_until is not None:
            if self.record_until >= 0 and self.data_timer == self.record_until + 1:
                print("Warning: memory process exceeded steps pre-allocated for data-recording.")

    def prepare_data_storage(self, data_record, record_until):
        """Set up data recording structure for diagnostics and visualization."""
        self.data_dic = {
            "belief_prior": None,
            "likelihoods": None,
            "belief_posterior": None,
            "sensory_expectation": None,
            "surprise": None
        }
        self.data_timer = 0
        self.record_until = record_until

        if "all" in data_record:
            if len(data_record) > 1:
                print("Warning: 'all' in data_record; additional entries ignored")
            data_record = self.data_dic.keys()

        if self.record_until < 0 and len(data_record) > 0:
            print("Warning: data_record set but record_until < 0; no data will be recorded")

        for variable in data_record:
            if variable not in self.data_dic:
                print(f"Warning: {variable} not a valid variable name; ignored")
            elif variable in ["belief_prior", "likelihoods", "belief_posterior"]:
                self.data_dic[variable] = np.full((self.record_until, self.num_hypotheses), -1., dtype=float)
            elif variable == "sensory_expectation":
                self.data_dic[variable] = np.full((self.record_until, np.sum(self.category_sizes)), -1., dtype=float)
            elif variable == "surprise":
                self.data_dic[variable] = np.full((self.record_until, len(self.category_sizes)), -1., dtype=float)

    def sample(self, percept) -> np.ndarray:
        """
        Given a percept, update belief and return a vector of sensory expectations.
        """
        self._check_structure()
        assert isinstance(percept, np.ndarray)
        assert np.issubdtype(percept.dtype, np.integer)
        if self.data_timer < self.record_until and self.data_dic["belief_prior"] is not None:
            self.data_dic["belief_prior"][self.data_timer, :] = self.belief_prior
        if self.data_timer < self.record_until and self.data_dic["sensory_expectation"] is not None:
            self.data_dic["sensory_expectation"][self.data_timer, :] = self.sensory_expectation

        self.percept = percept
        self.update_likelihoods(percept)
        if self.data_timer < self.record_until and self.data_dic["likelihoods"] is not None:
            self.data_dic["likelihoods"][self.data_timer, :] = self.likelihoods

        if self.data_timer < self.record_until and self.data_dic["surprise"] is not None:
            self.data_dic["surprise"][self.data_timer, :] = self.get_surprise()

        self.update_belief_posterior()
        if self.data_timer < self.record_until and self.data_dic["belief_posterior"] is not None:
            self.data_dic["belief_posterior"][self.data_timer, :] = self.belief_posterior
        
        self.learn(percept) # Placeholder for learning step in child classes

        self.update_priors()

        self.data_timer += 1
        return self.sensory_expectation

    def update_likelihoods(self, percept: np.ndarray):
        """Update the likelihoods for each hypothesis/state given the new percept."""
        if percept.shape[0] != len(self.category_sizes):
            raise ValueError(f'Percept vector size of {percept.shape[0]} does not match the number of perceptual categories, {len(self.category_sizes)}.')
        for i in range(len(self.category_sizes)):
            if percept[i] not in range(self.category_sizes[i]):
                raise ValueError("Percept state out of range for category.")
        one_hot_percept = self.get_one_hot_percept() # boolean mask for current percept's one-hot encoding
        category_likelihoods = self.sensory_predictions[:, one_hot_percept] # shape (num_hypotheses, num_categories)
        self.likelihoods = np.prod(category_likelihoods, axis=1) # shape (num_hypotheses,)

    def update_belief_posterior(self):
        """Update the posterior belief over hypotheses."""
        unnormalized_posterior = self.likelihoods * self.belief_prior # element-wise product
        normalization_constant = np.sum(unnormalized_posterior)
        if np.sum(unnormalized_posterior) != 0:
            self.belief_posterior = unnormalized_posterior / normalization_constant
        else:
            print("Warning: unnormalized posterior probabilities sum to 0. Model may be invalid.")
            self.belief_posterior = np.zeros(self.num_hypotheses)
        
    def learn(self, percept = None):
        """
        Placeholder for learning step in child classes.
        """
        pass

    def update_priors(self):
        """Update the prior belief and sensory expectation for the next step."""
        self.update_belief_prior()
        self.sensory_expectation = self.belief_prior @ self.sensory_predictions # sum of sensory state probabilities weighted across states/hypotheses by prior belief

    def update_belief_prior(self):
        """Update the prior belief using the transition predictions and posterior."""
        row_sums = self.transition_predictions.sum(axis=1, keepdims=True)
        assert np.all(np.abs(row_sums - 1) < 1e-9), "Each row of transition_predictions must sum to 1"
        self.belief_prior = self.belief_posterior @ self.transition_predictions

    def get_surprise(self) -> float:
        """Compute the total surprise of the network. 0 likelihood observations (inf surprise) return -1."""
        category_likelihoods = self.sensory_expectation[self.get_one_hot_percept()]
        mask = category_likelihoods == 0. # avoids log(0)
        surprise_values = np.full(category_likelihoods.shape, -1.)
        surprise_values[~mask] = -np.log(category_likelihoods[~mask]) / np.log(self.log_base)
        return surprise_values

    def get_one_hot_percept(self):
        """Return a boolean mask for the current percept's one-hot encoding."""
        one_hot_percept = np.zeros(np.sum(self.category_sizes), dtype=bool)
        one_hot_percept[np.cumsum(self.category_sizes) - self.category_sizes + self.percept] = True
        return one_hot_percept

# %% ../nbs/lib_nbs/02_ECMs.ipynb 23
from .methods.transforms import _logistic, _exponentiated_shift
import numpy as np

class Sequence_Memory(Bayesian_Filter):
    """
    Memory-augmented Bayesian filter that encodes a sequence of percepts as memory traces.
    Each memory trace is a hypothesis about the environment, and the agent can transition between
    non-memory and memory hypotheses. Supports dynamic modification of internal weights to encode temporal traces.

    Args:
        category_sizes (list): Number of sensory input elements.
        memory_capacity (int): Number of memory-based hypotheses.
        memory_bias (float): Probability of transitioning from non-memory to memory hypothesis space.
        sensory_predictions (np.ndarray, optional): Sensory hypotheses matrix. If None, initialized to uniform.
        belief_prior (np.ndarray, optional): Initial belief priors. If None, all prior on non-memory hypothesis.
        transition_predictions (np.ndarray, optional): Hypothesis transition matrix.
        timer (int, optional): Starting memory time index.
        capacity_overflow_method (str, optional): 'loop' or 'stop encoding'.
        data_record (list, optional): List of variable names to log each time step. Accepts "all".
        record_until (int, optional): Number of steps to prepare for data logging. Negative disables recording.
    """
    def __init__(
        self,
        category_sizes: list,
        memory_capacity: int,
        memory_bias: float,
        sensory_predictions: np.ndarray = None,
        belief_prior: np.ndarray = None,
        transition_predictions: np.ndarray = None,
        timer: int = 0,
        capacity_overflow_method="stop encoding",
        data_record: list = [],
        record_until: int = -1
    ):
        """
        Initialize the Sequence_Memory filter.
        """
        self.memory_capacity = memory_capacity
        self.memory_bias = memory_bias
        self.capacity_overflow_method = capacity_overflow_method
        self.num_non_memory_hypotheses = 1
        self.timer = timer
        self.effective_capacity = min(self.timer, self.memory_capacity)

        num_hypotheses = self.num_non_memory_hypotheses + self.memory_capacity
        #default to uniform sensory predictions if not provided
        if sensory_predictions is None:
            sensory_predictions = Initialize_Sensory_Predictions(
                category_sizes=category_sizes,
                num_hypotheses=num_hypotheses
            )

        #default to transition matrix that encodes sequence memory if not provided
        if transition_predictions is None:
            transition_predictions = Initialize_Memory_Based_Transition_Matrix(
                memory_capacity=self.memory_capacity,
                memory_bias=self.memory_bias,
                num_hypotheses=num_hypotheses,
                capacity_overflow_method=self.capacity_overflow_method
            )
        
        super().__init__(
            sensory_predictions=sensory_predictions,
            transition_predictions=transition_predictions,
            category_sizes=category_sizes,
            belief_prior=belief_prior,
            data_record=data_record,
            record_until=record_until
        )

        if belief_prior is None:
            self.belief_prior = np.zeros(self.num_hypotheses)
            self.belief_prior[self.memory_capacity:] = 1 / self.num_non_memory_hypotheses
        
        #overide placeholder belief posterior initialization from parent class to meet new requirements
        self.belief_posterior = np.zeros(self.num_hypotheses)
        self.belief_posterior[self.memory_capacity:] = 1 / self.num_non_memory_hypotheses 

    def _check_structure(self):
        """
        Check that all internal probability distributions are valid (Per Parent class).
        Check that no belief is assigned to unencoded memory slots.
        """
        super()._check_structure()
        if self.effective_capacity < self.memory_capacity:
            if np.sum(self.belief_prior[self.effective_capacity:self.memory_capacity]) > 1e-9:
                raise ValueError("Posterior belief assigned to unencoded memory slot.")

    def sample(self, percept):
        """
        Given a percept; update belief state, update world model, predict next sensor states.

        Args:
            percept (np.ndarray): The observed percept.

        Returns:
            np.ndarray: The predicted sensory expectation for the next step.
        """
        super().sample(percept)
        self.timer = (self.timer + 1) % self.memory_capacity
        return self.sensory_expectation
    
    def learn(self, percept  = None):
        """
        Encode the current percept into memory by updating the sensory and transition predictions.
        """
        if percept is None:
            percept = self.percept
        if self.effective_capacity < self.memory_capacity or not self.capacity_overflow_method == "stop encoding":
            # if last memory slot filled and looping, update transition matrix to loop and decouple non-memory hypotheses
            if self.effective_capacity + 1 == self.memory_capacity:
                self.capacity_reached()
            self.encode_memory(percept)
        self._check_structure()

    def capacity_reached(self):
        """
        Handle encoding of last memory hypothesis when memory capacity is reached.
        """
        if self.capacity_overflow_method == "loop":
            self.transition_predictions[self.effective_capacity, :] = 0
            self.transition_predictions[self.effective_capacity, 0] = 1
            for i in range(self.memory_capacity, self.num_hypotheses):
                self.transition_predictions[i, i] = 1
                self.transition_predictions[i, 0] = 0   

    def encode_memory(self, percept):
        """
        Encode the current percept into memory by updating the sensory and transition predictions.
        """
        if self.effective_capacity < self.memory_capacity:
            self.effective_capacity += 1
            
        categorical_encoding = self.get_one_hot_percept().astype(float)
        self.sensory_predictions[self.timer, :] = categorical_encoding

            

# %% ../nbs/lib_nbs/02_ECMs.ipynb 27
from .methods.transforms import _decay_toward_uniform

class Short_Term_Memory(Sequence_Memory):
    """
    Short-Term Memory filter that extends Sequence_Memory by introducing memory fading.
    Each encoded memory trace fades toward a uniform distribution at a specified rate.
    Supports different schematic transition methods and capacity overflow behaviors.

    Args:
        category_sizes (list): Number of sensory input elements.
        memory_capacity (int): Number of memory nodes.
        memory_bias (float): Transition probability from non-memory to memory hypothesis.
        fading_rate (float): Rate parameter for exponential decay toward uniform for memory traces.
        sensory_predictions (np.ndarray, optional): Optional sensory-to-memory weight matrix.
        belief_prior (np.ndarray, optional): Optional 1d array of prior expectations on memories.
        transition_predictions (np.ndarray, optional): Optional memory transition matrix.
        timer (int, optional): Starting memory time index.
        data_record (list, optional): List of variable names to record each time step. Accepts "all".
        record_until (int, optional): Number of steps to prepare for data recording. Negative disables recording.
        capacity_overflow_method (str, optional): 'loop' or 'stop encoding'.
        schematic_transition_method (str, optional): 'encoded', 'first', or 'learned'.
    """
    def __init__(
        self,
        category_sizes: list,
        memory_capacity: int,
        memory_bias: float,
        fading_rate: float,
        sensory_predictions: np.ndarray = None,
        belief_prior: np.ndarray = None,
        transition_predictions: np.ndarray = None,
        timer: int = 0,
        data_record: list = [],
        record_until: int = -1,
        capacity_overflow_method="loop",
        schematic_transition_method="encoded"
    ):
        """
        Initialize the Short_Term_Memory filter.
        """
        super().__init__(
            category_sizes=category_sizes,
            memory_capacity=memory_capacity,
            memory_bias=memory_bias,
            sensory_predictions=sensory_predictions,
            belief_prior=belief_prior,
            transition_predictions=transition_predictions,
            timer=timer,
            data_record=data_record,
            record_until=record_until,
            capacity_overflow_method=capacity_overflow_method
        )
        self.fading_rate = fading_rate
        self.schematic_transition_method = schematic_transition_method

        if self.schematic_transition_method == "encoded" and transition_predictions is None:
            for i in range(self.memory_capacity, np.shape(self.transition_predictions)[0]):
                self.transition_predictions[i, i] = 1 - self.memory_bias
                self.transition_predictions[i, self.timer] = self.memory_bias

    def learn(self, percept):
        """
        Fade all memory traces, then update memory with the new percept.

        Args:
            percept (np.ndarray): The observed percept.

        Returns:
            np.ndarray: The predicted sensory expectation for the next step.
        """
        self.fade()
        return super().learn(percept)

    def encode_memory(self, percept):
        """
        Encode the current percept into memory, updating sensory and transition predictions.
        Handles schematic transition methods and capacity overflow.
        """
        super().encode_memory(percept)
        if self.schematic_transition_method == "encoded":
            for i in range(self.memory_capacity, self.num_hypotheses):
                self.transition_predictions[i, :self.effective_capacity] = self.memory_bias/self.effective_capacity

    def capacity_reached(self):
        """
        Handle encoding of last memory hypothesis when memory capacity is reached.
        Overwrites method from Sequence Memory to handle schematic transition methods.
        """
        if self.capacity_overflow_method == "loop":
            self.transition_predictions[self.effective_capacity, :] = 0
            self.transition_predictions[self.effective_capacity, 0] = 1
            if not self.schematic_transition_method == "encoded":
                for i in range(self.memory_capacity, self.num_hypotheses):
                    self.transition_predictions[i, i] = 1
                    self.transition_predictions[i, 0] = 0 

    def fade(self):
        """
        Apply exponential decay toward uniform distribution for each memory trace.
        """
        for i in range(len(self.category_sizes)):
            faded_memories = _decay_toward_uniform(
                self.sensory_predictions[:self.memory_capacity, self.category_indexer == i],
                self.fading_rate
            )
            self.sensory_predictions[:self.memory_capacity, self.category_indexer == i] = faded_memories

# %% ../nbs/lib_nbs/02_ECMs.ipynb 34
## Fading Rate Encoders
def sigmoid_fading_rate(gamma, sigma, sensor_state_probabilities, log_base = 2):
    #print(f'probabilites: {sensor_state_probabilities}')
    skew = np.log(gamma/(1-gamma))/np.log(log_base) #sets rate to gamma when logistic component is 0 (i.e. at sigmoid center)
    #print(f'skew: {skew}')
    shift = 2*sensor_state_probabilities - 1 #centers sigmoid at p = 0.5
    #print(f'shift: {shift}')
    scale = -np.log(1-sigma)/np.log(log_base) #slope of sigmoid
    #print(f'scale: {scale}')
    x = skew + scale * shift
    fading_rates = 1/(1 + log_base ** -x)
    #print(f'fading_rates: {fading_rates}')
    return(fading_rates)

def surprise_advantage_fading_rate(gamma,sigma, category_indexes, categorical_predictions, categorical_suprises, log_base = 2):
    sensor_labels, inv = np.unique(category_indexes, return_inverse=True)
    plogp = categorical_predictions * np.log(categorical_predictions)
    prediction_entropies = -np.bincount(inv, weights = plogp, minlength = sensor_labels.size) #sums the plogp for each value in category_indexer and returns as 1d array
    converted_entropies = prediction_entropies/np.log(log_base)
    surprise_gaps = categorical_surprises - converted_entropies
    fading_rates = gamma ** (log_base ** (sigma * surprise_gaps))
    return(fading_rates)

# %% ../nbs/lib_nbs/02_ECMs.ipynb 36
class Long_Term_Memory(Short_Term_Memory):
    """
    Long-Term Memory filter that extends Short_Term_Memory by introducing surprise-modulated memory fading and memory stabilization.
    Each encoded memory trace fades toward a uniform distribution at a rate modulated by the surprise of the encoded percept and by reactivation.
    Reactivation slows fading and can reinforce the memory trace, stabilizing early memories of sequences.

    Args:
        category_sizes (list): Number of sensory input elements.
        memory_capacity (int): Number of memory nodes.
        memory_bias (float): Transition probability from non-memory to memory hypothesis.
        fading_rate (float): Base rate parameter for exponential decay toward uniform for memory traces.
        surprise_factor (float, optional): Scales the degree to which surprise slows down memory fading.
        reuse_factor (float, optional): Scales the degree to which reactivation slows fading.
        sensory_predictions (np.ndarray, optional): Optional sensory-to-memory weight matrix.
        belief_prior (np.ndarray, optional): Optional 1d array of prior expectations on memories.
        transition_predictions (np.ndarray, optional): Optional memory transition matrix.
        timer (int, optional): Starting memory time index.
        data_record (list, optional): List of variable names to record each time step. Accepts "all".
        record_until (int, optional): Number of steps to prepare for data recording. Negative disables recording.
        fading_rate_method (str, optional): Method for computing fading rates ("sigmoid" or "surprise_advantage").
        capacity_overflow_method (str, optional): 'loop' or 'stop encoding'.
        schematic_transition_method (str, optional): 'encoded', 'first', or 'learned'.
    """
    def __init__(
        self,
        category_sizes: list,
        memory_capacity: int,
        memory_bias: float,
        fading_rate: float,
        surprise_factor: float = 0.,
        reuse_factor: float = 0.,
        sensory_predictions: np.ndarray = None,
        belief_prior: np.ndarray = None,
        transition_predictions: np.ndarray = None,
        timer: int = 0,
        data_record: list = [],
        record_until: int = -1,
        fading_rate_method = "sigmoid",
        capacity_overflow_method = "stop encoding",
        schematic_transition_method = "encoded"
    ):
        """
        Initialize the Long_Term_Memory filter.
        """
        super().__init__(
            category_sizes=category_sizes,
            memory_capacity=memory_capacity,
            memory_bias=memory_bias,
            fading_rate=fading_rate,
            sensory_predictions=sensory_predictions,
            belief_prior=belief_prior,
            transition_predictions=transition_predictions,
            timer=timer,
            data_record=data_record,
            record_until=record_until,
            capacity_overflow_method=capacity_overflow_method,
            schematic_transition_method=schematic_transition_method
        )
        assert 0 <= surprise_factor <= 1, "surprise_factor must be in [0, 1]"
        self.surprise_factor = surprise_factor
        self.reuse_factor = reuse_factor
        self.fading_rate_method = fading_rate_method
        self.memory_fade = np.zeros((np.shape(self.sensory_predictions)[0], len(self.category_sizes)))  # fading rates for all memories

    def learn(self, percept):
        """
        Given a percept, update world model and stabilize memories.
        """
        super().learn(percept)
        self.stablize_memories()

    def fade(self):
        """
        Apply exponential decay toward uniform distribution for each memory trace, using memory-specific fading rates.
        """
        for i in range(len(self.category_sizes)):
            faded_memories = _decay_toward_uniform(
                self.sensory_predictions[:, self.category_indexer == i],
                self.memory_fade[:, i]
            )
            self.sensory_predictions[:, self.category_indexer == i] = faded_memories

    def stablize_memories(self):
        """
        Reduce fading rate of reactivated memories according to the weight of reactivation (posterior belief).
        """
        for i in range(self.timer):
            self.memory_fade[i, :] = self.memory_fade[i, :] * (1 - (self.belief_posterior[i] * self.reuse_factor))

    def encode_memory(self, percept=None):
        """
        Encode the current percept into memory, updating sensory and transition predictions.
        Sets the initial fading rates of each category in the new memory.
        """
        super().encode_memory(percept)
        if not self.effective_capacity == self.memory_capacity or not self.capacity_overflow_method == "stop encoding":
            self.memory_fade[self.timer, :] = self.get_fading_rates()

    def get_fading_rates(self):
        """
        Compute the fading rates for the current memory trace using the selected fading_rate_method.
        """
        if self.fading_rate_method == "sigmoid":
            fading_rates = sigmoid_fading_rate(
                gamma=self.fading_rate,
                sigma=self.surprise_factor,
                sensor_state_probabilities=self.sensory_expectation[self.get_one_hot_percept()],
                log_base=self.log_base
            )
        elif self.fading_rate_method == "surprise_advantage":
            fading_rates = surprise_advantage_fading_rate(
                gamma=self.fading_rate,
                sigma=self.surprise_factor,
                category_indexes=self.category_indexer,
                categorical_predictions=self.sensory_expectation,
                categorical_suprises=self.get_surprise(),
                log_base=self.log_base
            )
        else:
            raise ValueError(f'{self.fading_rate_method} is not a valid fading_rate_method')
        return fading_rates

