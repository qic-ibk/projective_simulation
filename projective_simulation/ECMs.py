# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/lib_nbs/02_ECMs.ipynb.

# %% auto 0
__all__ = ['standard_ps_upd', 'Abstract_ECM', 'Two_Layer', 'Priming_ECM', 'Bayesian_Network']

# %% ../nbs/lib_nbs/02_ECMs.ipynb 5
def standard_ps_upd(reward, hmatrix, gmatrix, h_damp, g_damp):
    """
    Given a reward, updates h-matrix and g-matrix following the standard PS update rule:

    h <- h - h_damp*(h-1)+ reward*g
    g <- (1-g_damp)*g    
    """
    # damping h-matrix
    hmatrix = hmatrix - h_damp*(hmatrix-1.)
    # update h-matrix
    hmatrix += reward*gmatrix
    # update g-matrix
    gmatrix = (1-g_damp)*gmatrix

    return hmatrix, gmatrix

# %% ../nbs/lib_nbs/02_ECMs.ipynb 8
from .methods.lib_helpers import CustomABCMeta
from abc import abstractmethod


class Abstract_ECM(metaclass = CustomABCMeta):
    """
    Abstract agent class any episodic and compositional memory (ECM) should be derived from. Asserts that the necessary methods are implemented.
    """

    def __init__(self):
        '''
        No restrictions on the constructor, as the ECM can be anything that has a sample module.
        '''
        pass

    @abstractmethod
    def sample(self,):
        """
        Performs a random walk through the ECM. Typically, this implies receiving an input percept and returning an action.
        """
        pass

# %% ../nbs/lib_nbs/02_ECMs.ipynb 12
import numpy as np
from .methods.transforms import _softmax

class Two_Layer(Abstract_ECM):
    def __init__(self, 
                 # The number of available actions.
                 num_actions: int, 
                 # The glow damping(or eta) parameter. 
                 g_damp: float, 
                 # The damping (or gamma) parameter. 
                 h_damp: float,
                 # If 'greedy', uses a greedy policy that samples the most action based on the h-matrix. 
                 # If 'softmax', uses a softmax policy that samples an action based on the h-matrix and a temperature parameter (encoded in policy_parameters).
                 # If object, uses this object to sample action. Input must be h_values corresponding to current percept + arbitrary policy_parameters.
                 policy: str = 'greedy',                 
                 # The parameters of the policy.
                 policy_parameters: dict = None,
                 # Method to update the g-matrix. 
                 # If 'sum', adds the new value to the current value.
                 # If 'init', sets the new value to 1.
                 glow_method: str = 'sum',
                ):

        """
        Two layer ECM. First layer, encoding the percepts observed in an environment, is initially empty (e.g. self.num_percepts = 0). As percepts
        are observed, they are added to the ECM and to the percept dictionary self.percepts. 
        The second layer, encoding the actions, has size self.num_actions.
        In practice, the ECM graph is never created. Instead, it is defined indirectly by the h-matrix and g-matrix. 
        Both have size (self.num_percepts, self.num_actions). 
        The input policy (greedy, softmax or other) is used to sample actions based on the h-matrix.

        For an end-to-end example of how to use this class, see the tutorial notebook on Basic PS agents.        
        """

        

        self.num_actions = num_actions

        self.h_damp = h_damp
        self.g_damp = g_damp
        self.glow_method = glow_method

        self.policy = policy
        self.policy_parameters = policy_parameters
        
        # Initialize ECM structures

        #int: current number of percepts.
        self.num_percepts = 0
        #np.ndarray: h-matrix with current h-values. Defaults to all 1.
        self.hmatrix = np.ones([0,self.num_actions])
        #np.ndarray: g-matrix with current glow values. Defaults to all 0.
        self.gmatrix = np.zeros([0,self.num_actions])
        #dict: Dictionary of percepts as {"percept": index}
        self.percepts = {}

    def sample(self, percept: str):
        """
        Given a percept, returns an action and changes the ECM if necessary
        First, if the percept is new, it will be added to the ECM
        Then, an action is selected as a function of the percept and the h-values of edges connected to that percept
        Finally, the g-matrix is updated based on the realized percept-action pair.
        """

        # Add percept to ECM if not already present
        self.add_percept(percept)
        # Get index from dictionary entry
        percept_index = self.percepts[percept]
        # Get h-values
        h_values = self.hmatrix[percept_index]

        # Perform Random Walk through the ECM based on h_values and current policy
        if self.policy == 'greedy': 
            # Sample greedly the action with the highest h-value
            h_values = self.hmatrix[percept_index]
            action = h_values.argmax()   

        elif self.policy == 'softmax':
            # Get probabilities from h-values through a softmax function
            prob = _softmax(self.policy_parameters, h_values)
            # Sample action based on probabilities
            action = np.random.choice(range(self.num_actions), p=prob) 

        else:
            # This considers a custom policy
            action = self.policy(h_values = h_values, **self.policy_parameters)

        # Update g-matrix
        if self.glow_method == 'sum':
            self.gmatrix[int(percept_index),int(action)] += 1.
        if self.glow_method == 'init':
            self.gmatrix[int(percept_index),int(action)] = 1.
            

        return action

    def add_percept(self, percept):
        '''
        Checks if percept is in dictionary and adds to ECM in not
        '''
        if percept not in self.percepts.keys(): 
            self.percepts[percept] = self.num_percepts
            # increment number of percepts
            self.num_percepts += 1
            # add column to h-matrix
            self.hmatrix = np.append(self.hmatrix, 
                                     np.ones([1,self.num_actions]),
                                     axis=0)
            # add column to g-matrix
            self.gmatrix = np.append(self.gmatrix, 
                                    np.zeros([1,self.num_actions]),
                                    axis=0)

    def  learn(self, reward):
        """
        Updates the h-matrix and g-matrix based on the reward received using the standard PS update rule.
        """
        self.hmatrix, self.gmatrix = standard_ps_upd(reward, self.hmatrix, self.gmatrix, self.h_damp, self.g_damp)

# %% ../nbs/lib_nbs/02_ECMs.ipynb 15
from .methods import transforms

class Priming_ECM(Two_Layer):
    '''
    This sub-class of the Two-Layer ECM adds a variable for action priming.
    This variable should be a list of floats, each element of which corresponds to an action in the ECM.
    These "priming values" are summed with h-values of any edge connected to the associated action node prior to calculating walk probabilites with the softmax function
    '''
    def __init__(self, 
                 num_actions: int, # The number of available actions.                 
                 glow: float = 0.1, # The glow (or eta) parameter. 
                 damp: float = 0.01, # The damping (or gamma) parameter. 
                 softmax: float = 0.5, # The softmax (or beta) parameter.
                 action_primes: list = None, #weights on the probability that deliberation steps into each action. Defaults to 0 for each action 
                ):
        if action_primes is None:
            action_primes = [0.] * num_actions
        assert len(action_primes) == num_actions

        self.softmax = softmax
        super().__init__(num_actions, glow, damp, 
                         policy = None) # Here I made explicit that the policy is None, as we override the sample method
        self.action_primes = action_primes
        

    def sample(self, percept):
        '''
        Almost identical to the sample function of Two-Layer parent class, but sums h-values and action primes prior to calculating walk probabilities
        '''
        self.add_percept(percept)
        #Perform Random Walk
        # get index from dictionary entry
        percept_index = self.percepts[percept]
        # get h-values
        h_values = self.hmatrix[percept_index]
        #~~~Differences from two-layer sample function within
        assert len(h_values) == len (self.action_primes)
        # get probabilities from h-values and primes through a softmax function
        prob = transforms._softmax(self.softmax, h_values + self.action_primes)
        #~~~~~~~
        # get action
        action = np.random.choice(range(self.num_actions), p=prob)        
        #pdate g-matrix
        self.gmatrix[int(percept_index),int(action)] = 1.
        return action

# %% ../nbs/lib_nbs/02_ECMs.ipynb 20
import numpy as np

class Bayesian_Network(Abstract_ECM):
    """
    Bayesian Network ECM implementation.
    """

    def __init__(
        self,
        W_matrix: np.ndarray,       # Transition matrix from sensory input to m-level.
        C_matrix: np.ndarray,       # Transition matrix between m-level nodes.
        m_expectation: np.ndarray = None,  # Optional initial expectation distribution over m-nodes.
        data_log: bool = False       #stores surprise after each network excitation in a list
    ):
        super().__init__()       
        self.num_sensory_elements = np.shape(W_matrix)[0]
        self.num_m_nodes = np.shape(W_matrix)[1]              
        self.W_matrix = W_matrix
        if not np.shape(W_matrix)[1] == np.shape(C_matrix)[1]:
            raise ValueError("W_matrix and C_matrix must have the same number of columns (m_nodes)")
        if not np.shape(C_matrix)[0] == np.shape(C_matrix)[1]:
            raise ValueError("C_matrix must have the same number of columns and rows (m_nodes)")
        self.C_matrix = C_matrix

        if data_log:
            self.surprise_data = np.empty(0)

        # Initialize sensory and m-level excitations as empty arrays
        self.sensory_excitation = np.empty(self.num_sensory_elements)
        self.m_excitation = np.empty(self.num_m_nodes)

        # Set the initial expectation to uniform if none is provided
        self.m_expectation = np.full(self.num_m_nodes, fill_value=1/self.num_m_nodes) if m_expectation is None else m_expectation

        # Compute initial sensory expectation as a weighted sum over W_matrix
        self.sensory_expectation = np.dot(self.m_expectation, self.W_matrix.T)

        # Placeholder for current m-node activation vector
        self.m_activation = np.zeros(self.num_m_nodes)

    def excite_network(
        self,
        percept: np.ndarray  # Binary input vector representing the current percept.
    ):
        """Sets the sensory excitation equal to the percept vector and updates m_excitation accordingly."""
        # Check input shape
        if percept.shape[0] != self.num_sensory_elements:
            raise ValueError("Percept vector size does not match the number of sensory elements.")

        self.sensory_excitation = percept

        # compute each m_excitation as the product of each sensory element's excitation likelihood,
        self.m_excitation = np.prod(
            np.power(self.W_matrix, self.sensory_excitation[:, np.newaxis]) *  #gives likelihood if sensory element is excited and 1 otherwise
            np.power(1 - self.W_matrix, (1 - self.sensory_excitation)[:, np.newaxis]), #gives likelihood if sensory element is not excited and 1 otherwise
            axis=0
        )

    def activate(self):
        """Sets m_activation based on m_excitation and m_expectation."""
        # Compute the unnormalized activation (Bayesian inference numerator)
        numerator = self.m_excitation * self.m_expectation

        # Normalize to ensure it sums to one (Bayesian posterior)
        denominator = np.sum(numerator)

        if denominator != 0:
            self.m_activation = numerator / denominator
        else:
            # Avoid division by zero if all values are 0
            print("Warning: Activations sum to 0. This implies the agent believes it can not be in any known state and is likely to cause problems")
            self.m_activation = np.zeros(self.num_m_nodes)

    def set_expectations(self):
        """Set m_expectation and sensory_expectation based on activation and weight matrices."""
        
        # Normalize the C_matrix rows to form proper probability distributions
        row_sums = self.C_matrix.sum(axis=1, keepdims=True)
        normalized_C_matrix = np.divide(self.C_matrix, row_sums, where=row_sums != 0)

        # Update m_expectation using the transition matrix and current activation
        self.m_expectation = np.dot(self.m_activation, normalized_C_matrix)

        # Update sensory_expectation using a transformed dot product with W_matrix
        self.sensory_expectation = np.dot(self.m_expectation, self.W_matrix.T)

    def get_surprise(self) -> float:
        """Compute the total surprise of the network."""
        # Compute the surprise of each element using the binary cross-entropy formula
        surprise_values = np.where(
            self.sensory_excitation == 1,
            -np.log2(self.sensory_expectation),
            -np.log2(1 - self.sensory_expectation)
        )

        # Return total surprise as the sum over all sensory elements
        return np.sum(surprise_values)

    def sample(self, percept) -> np.ndarray:
        '''given a percept, updates network states and returns a vector of sensory expectations'''
        self.excite_network(percept)
        if data_log:
            self.surprise_data = np.append(self.surprise_data, self.get_surprise(), axis = 0)
        self.activate()
        self.set_expectations()

        # Return the index of the maximum activated node (greedy policy)
        return self.sensory_expecatations
