# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/lib_nbs/02_ECMs.ipynb.

# %% auto 0
__all__ = ['standard_ps_upd', 'Abstract_ECM', 'Two_Layer', 'Priming_ECM', 'Bayesian_Filter', 'Bayesian_Memory_Filter',
           'Short_Term_Memory', 'sigmoid_fading_rate', 'surprise_advantage_fading_rate', 'Long_Term_Memory',
           'Semantic_Memory']

# %% ../nbs/lib_nbs/02_ECMs.ipynb 5
def standard_ps_upd(reward, hmatrix, gmatrix, h_damp, g_damp):
    """
    Given a reward, updates h-matrix and g-matrix following the standard PS update rule:

    h <- h - h_damp*(h-1)+ reward*g
    g <- (1-g_damp)*g    
    """
    # damping h-matrix
    hmatrix = hmatrix - h_damp*(hmatrix-1.)
    # update h-matrix
    hmatrix += reward*gmatrix
    # update g-matrix
    gmatrix = (1-g_damp)*gmatrix

    return hmatrix, gmatrix

# %% ../nbs/lib_nbs/02_ECMs.ipynb 8
from .methods.lib_helpers import CustomABCMeta
from abc import abstractmethod


class Abstract_ECM(metaclass = CustomABCMeta):
    """
    Abstract agent class any episodic and compositional memory (ECM) should be derived from. Asserts that the necessary methods are implemented.
    """

    def __init__(self):
        '''
        No restrictions on the constructor, as the ECM can be anything that has a sample module.
        '''
        pass

    @abstractmethod
    def sample(self,):
        """
        Performs a random walk through the ECM. Typically, this implies receiving an input percept and returning an action.
        """
        pass

# %% ../nbs/lib_nbs/02_ECMs.ipynb 12
import numpy as np
from .methods.transforms import _softmax

class Two_Layer(Abstract_ECM):
    def __init__(self, 
                 # The number of available actions.
                 num_actions: int, 
                 # The glow damping(or eta) parameter. 
                 g_damp: float, 
                 # The damping (or gamma) parameter. 
                 h_damp: float,
                 # If 'greedy', uses a greedy policy that samples the most action based on the h-matrix. 
                 # If 'softmax', uses a softmax policy that samples an action based on the h-matrix and a temperature parameter (encoded in policy_parameters).
                 # If object, uses this object to sample action. Input must be h_values corresponding to current percept + arbitrary policy_parameters.
                 policy: str = 'greedy',                 
                 # The parameters of the policy.
                 policy_parameters: dict = None,
                 # Method to update the g-matrix. 
                 # If 'sum', adds the new value to the current value.
                 # If 'init', sets the new value to 1.
                 glow_method: str = 'sum',
                ):

        """
        Two layer ECM. First layer, encoding the percepts observed in an environment, is initially empty (e.g. self.num_percepts = 0). As percepts
        are observed, they are added to the ECM and to the percept dictionary self.percepts. 
        The second layer, encoding the actions, has size self.num_actions.
        In practice, the ECM graph is never created. Instead, it is defined indirectly by the h-matrix and g-matrix. 
        Both have size (self.num_percepts, self.num_actions). 
        The input policy (greedy, softmax or other) is used to sample actions based on the h-matrix.

        For an end-to-end example of how to use this class, see the tutorial notebook on Basic PS agents.        
        """

        

        self.num_actions = num_actions

        self.h_damp = h_damp
        self.g_damp = g_damp
        self.glow_method = glow_method

        self.policy = policy
        self.policy_parameters = policy_parameters
        
        # Initialize ECM structures

        #int: current number of percepts.
        self.num_percepts = 0
        #np.ndarray: h-matrix with current h-values. Defaults to all 1.
        self.hmatrix = np.ones([0,self.num_actions])
        #np.ndarray: g-matrix with current glow values. Defaults to all 0.
        self.gmatrix = np.zeros([0,self.num_actions])
        #dict: Dictionary of percepts as {"percept": index}
        self.percepts = {}

    def sample(self, percept: str):
        """
        Given a percept, returns an action and changes the ECM if necessary
        First, if the percept is new, it will be added to the ECM
        Then, an action is selected as a function of the percept and the h-values of edges connected to that percept
        Finally, the g-matrix is updated based on the realized percept-action pair.
        """

        # Add percept to ECM if not already present
        self.add_percept(percept)
        # Get index from dictionary entry
        percept_index = self.percepts[percept]
        # Get h-values
        h_values = self.hmatrix[percept_index]

        # Perform Random Walk through the ECM based on h_values and current policy
        if self.policy == 'greedy': 
            # Sample greedly the action with the highest h-value
            h_values = self.hmatrix[percept_index]
            action = h_values.argmax()   

        elif self.policy == 'softmax':
            # Get probabilities from h-values through a softmax function
            prob = _softmax(self.policy_parameters, h_values)
            # Sample action based on probabilities
            action = np.random.choice(range(self.num_actions), p=prob) 

        else:
            # This considers a custom policy
            action = self.policy(h_values = h_values, **self.policy_parameters)

        # Update g-matrix
        if self.glow_method == 'sum':
            self.gmatrix[int(percept_index),int(action)] += 1.
        if self.glow_method == 'init':
            self.gmatrix[int(percept_index),int(action)] = 1.
            

        return action

    def add_percept(self, percept):
        '''
        Checks if percept is in dictionary and adds to ECM in not
        '''
        if percept not in self.percepts.keys(): 
            self.percepts[percept] = self.num_percepts
            # increment number of percepts
            self.num_percepts += 1
            # add column to h-matrix
            self.hmatrix = np.append(self.hmatrix, 
                                     np.ones([1,self.num_actions]),
                                     axis=0)
            # add column to g-matrix
            self.gmatrix = np.append(self.gmatrix, 
                                    np.zeros([1,self.num_actions]),
                                    axis=0)

    def  learn(self, reward):
        """
        Updates the h-matrix and g-matrix based on the reward received using the standard PS update rule.
        """
        self.hmatrix, self.gmatrix = standard_ps_upd(reward, self.hmatrix, self.gmatrix, self.h_damp, self.g_damp)

# %% ../nbs/lib_nbs/02_ECMs.ipynb 15
from .methods.transforms import _softmax

class Priming_ECM(Two_Layer):
    '''
    This sub-class of the Two-Layer ECM adds a variable for action priming.
    This variable should be a list of floats, each element of which corresponds to an action in the ECM.
    These "priming values" are summed with h-values of any edge connected to the associated action node prior to calculating walk probabilites with the softmax function
    '''
    def __init__(self, 
                 num_actions: int, # The number of available actions.                 
                 glow: float = 0.1, # The glow (or eta) parameter. 
                 damp: float = 0.01, # The damping (or gamma) parameter. 
                 softmax: float = 0.5, # The softmax (or beta) parameter.
                 action_primes: list = None, #weights on the probability that deliberation steps into each action. Defaults to 0 for each action 
                ):
        if action_primes is None:
            action_primes = [0.] * num_actions
        assert len(action_primes) == num_actions

        self.softmax = softmax
        super().__init__(num_actions, glow, damp, 
                         policy = None) # Here I made explicit that the policy is None, as we override the sample method
        self.action_primes = action_primes
        

    def sample(self, percept):
        '''
        Almost identical to the sample function of Two-Layer parent class, but sums h-values and action primes prior to calculating walk probabilities
        '''
        self.add_percept(percept)
        #Perform Random Walk
        # get index from dictionary entry
        percept_index = self.percepts[percept]
        # get h-values
        h_values = self.hmatrix[percept_index]
        #~~~Differences from two-layer sample function within
        assert len(h_values) == len (self.action_primes)
        # get probabilities from h-values and primes through a softmax function
        prob = _softmax(self.softmax, h_values + self.action_primes)
        #~~~~~~~
        # get action
        action = np.random.choice(range(self.num_actions), p=prob)        
        #pdate g-matrix
        self.gmatrix[int(percept_index),int(action)] = 1.
        return action

# %% ../nbs/lib_nbs/02_ECMs.ipynb 18
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
from matplotlib.colors import ListedColormap

class Bayesian_Filter(Abstract_ECM):
    """
    A Bayesian_Hypothesis_Filter is a set of hypotheses where each hypothesis posits:
     (1) a probability for all states of K sensory categories with discrete state spaces and ...
     (2) a probability that each hypothesis will be true when the sensory category states are next updated. 
    The Bayesian_Hypothesis_Filter maintains a belief state over these hypotheses.
    It assumes the set of hypotheses are exhaustive and mutually exclusive. 
    
    The sample function takes the states of each category as input (percept), updates the belief state of the Bayesian_Hypothesis_Filter and ...
    returns: K probability distributions that predict the next state of each sensory category.
    """

    def __init__(
        self,
        category_sizes: list,                # number of states per sensory category
        sensory_predictions: np.ndarray,      # Each row represents an hypothesis, and the columns have groups of sizes given by category_sizes such that the sum of each group for each hypothesis is 1
        transition_predictions: np.ndarray,   # Each row represents an hypothesis: columns give the predicted probabilities under that hypothesis that each hypothesis will be true for the next percept. 
        belief_prior: np.ndarray = None,     # Optional initial belief prior. If none, set to uniform over hypotheses
        log_base = 2.,                       # Sets base to use for logarithms and exponential functions
        data_record: list = [],              # a list of internal variable names (e.g. likelihoods, belief_state) to record each time step. Accepts "all"
        record_until: int = -1               # number of time steps to record. default of -1 results no data recording.
    ):
        
        self.category_sizes = category_sizes
        self.category_indexer = np.repeat(np.arange(0, len(category_sizes)), category_sizes) #an array for labeling/slicing sensory hypotheses by category
        self.num_hypotheses = np.shape(sensory_predictions)[0]              
        self.transition_predictions = transition_predictions
        self.sensory_predictions = sensory_predictions
        self.log_base = log_base
        ##~~~~~~enforce structure
        if not sum(category_sizes) == np.shape(sensory_predictions)[1]:
            raise ValueError("sensory_predictions must have a number of columns equal to the total number of percept category states")
        
        
        #check for category sums of one, allowing for floating point precision
        for i in range(np.shape(sensory_predictions)[0]): #for each hypothesis
            for k in range(len(category_sizes)): #for each sensory category
                if not np.abs(np.sum(sensory_predictions[i,self.category_indexer == k])-1) < 1e-9:
                    raise ValueError("hypothesized probabilities for each sensory category must have a total probability of 1")

        if not np.shape(sensory_predictions)[0] == np.shape(transition_predictions)[0]:
            raise ValueError("sensory_predictions and transition_predictions must have the same number of columns")
        if not np.shape(transition_predictions)[0] == np.shape(transition_predictions)[1]:
            raise ValueError("transition_predictions must have the same number of columns and rows")
        ##~~~~~~


        # Initialize percept object and likelihoods of the percept for each hypothesis as empty arrays
        self.percept = np.empty(len(self.category_sizes), dtype = int)
        self.likelihoods = np.zeros(self.num_hypotheses)

        # Set the initial belief prior to uniform if none is provided
        if belief_prior is None:
            self.belief_prior = np.full(self.num_hypotheses, fill_value = 1/self.num_hypotheses)
        else:
            self.belief_prior = belief_prior

        # Compute initial sensory category predictions as a belief-prior-weighted sum over sensory hypotheses
        self.sensory_expectation = self.belief_prior @ self.sensory_predictions

        # Initialize object for belief posterior - values are placeholder to fullfull requirement that posteriors sums to one and are not used
        self.belief_posterior = np.full(self.num_hypotheses, fill_value = 1/self.num_hypotheses)

        self.prepare_data_storage(data_record, record_until) #for diagnostics
    
    def prepare_data_storage(self, data_record, record_until):
        '''sets up data recording structure for diagnostics and visualizing the filtering process'''
        
        self.data_dic = {"belief_prior": None, 
                     "likelihoods": None, 
                     "belief_posterior": None,
                     "sensory_expectation": None, 
                     "surprise": None}
        
        self.data_timer = 0
        self.record_until = record_until
        
        if "all" in data_record:
            if len(data_record) > 1:
                print("Warning, data_record contains 'all', additional entries will be ignored")
            data_record = self.data_dic.keys()

        if self.record_until < 0 and len(data_record) > 0:
            print("Warning, 'data_record' indicates data should be recorded but a positive value was not given for 'record_until'. No data will be recorded")

        #add empty data set for each variable in data_log
        for variable in data_record:
            if variable not in self.data_dic.keys():
                print("Warning, " + str(variable) + " in data_record not a valid variable name and will be ignored")
            elif variable in ["belief_prior", "likelihoods", "belief_posterior"]:
                self.data_dic[variable] = np.full((self.record_until, self.num_hypotheses), fill_value = -1., dtype = float) #fill value is outside allowable range for variables to indicate unfilled data
            elif variable == "sensory_expectation":
                self.data_dic[variable] = np.full((self.record_until, np.sum(self.category_sizes)), fill_value = -1., dtype = float) #fill value is outside allowable range for variables to indicate unfilled data
            elif variable == "surprise":
                self.data_dic[variable] = np.full((self.record_until, len(self.category_sizes)), fill_value = -1., dtype = float) #fill value is outside allowable range for variables to indicate unfilled data
            else:
                print("Warning, unexpected condition in data_record variables")        
        
    def sample(self, percept) -> np.ndarray:
        '''given a percept, updates network states and returns a vector of sensory expectations'''
        assert isinstance(percept, np.ndarray)
        assert np.issubdtype(percept.dtype, np.integer)
        #record belief prior
        if self.data_timer < self.record_until and self.data_dic["belief_prior"] is not None:
            self.data_dic["belief_prior"][self.data_timer,:] = self.belief_prior

        #update likelihoods and record
        self.update_likelihoods(percept)
        if self.data_timer < self.record_until and self.data_dic["likelihoods"] is not None:
            self.data_dic["likelihoods"][self.data_timer,:] = self.likelihoods

        #record surprise
        if self.data_timer < self.record_until and self.data_dic["surprise"] is not None:
            self.data_dic["surprise"][self.data_timer,:] = self.get_surprise()

        #update belief posterior and record
        self.update_belief_posterior()
        if self.data_timer < self.record_until and self.data_dic["belief_posterior"] is not None:
            self.data_dic["belief_posterior"][self.data_timer,:] = self.belief_posterior

        #predict
        self.update_priors()

        #Advance timer for data logging
        self.data_timer += 1
                                           
        return self.sensory_expectation

    def update_likelihoods(
        self,
        percept: np.ndarray  # a vector indicating the state of each sensory category
    ):
        """Updates the Bayesian Filter's percept object with the new input and updates the likelihood accordingly."""
        # Check input shape
        if percept.shape[0] != len(self.category_sizes):
            raise ValueError("Percept vector size does not match the number of perceptual categories.")
        for i in range(len(self.category_sizes)):
            if not percept[i] in range(self.category_sizes[i]):
                raise ValueError("The state of each percept category should be in {0,1,...,N_i - 1}, where N_i is the number of states for that percept category")
                #using 0 for first state helps with indexing memory array
        self.percept = percept

        # compute each likelihood as the product of each sensory_category's likelihood given the corresponding hypothesis,
        one_hot_percept = self.get_one_hot_percept() #function uses self.percept
        category_likelihoods = self.sensory_predictions[:,one_hot_percept] #gets the probability of each sensory category state under each hypothesis
        self.likelihoods = np.prod(category_likelihoods, axis=1) #likelihood of full percept given each hypothesis by taking product of values in each row

    def update_belief_posterior(self):
        """Sets belief_posterior based on likelihoods and belief prior."""
        # Compute the unnormalized activation (Bayesian inference numerator)
        numerator = self.likelihoods * self.belief_prior

        # Normalize to ensure it sums to one (Bayesian posterior)
        denominator = np.sum(numerator)

        if denominator != 0:
            self.belief_posterior = numerator / denominator
        else:
            # Avoid division by zero if all values are 0
            print("Warning: posterior probabilities sum to 0. This implies a bad model has been given and the agent believes it can not be in any known state. Problems will occur")
            self.belief_posterior = np.zeros(self.num_hypotheses)

    def update_priors(self):
        """Set belief_prior and sensory_expectation based on belief_posterior and transition_predictions."""
        #update belief_prior
        self.update_belief_prior()
        # Update sensory_expectation using a prior-belief-weighted sum of sensory hypotheses
        self.sensory_expectation = self.belief_prior @ self.sensory_predictions

    def update_belief_prior(self):
        row_sums = self.transition_predictions.sum(axis=1, keepdims=True)
        assert(row_sum == 1 for row_sum in row_sums)
        # Update belief prior using the transition hypotheses and belief posterior
        self.belief_prior = self.belief_posterior @ self.transition_predictions

    def get_surprise(self) -> float:
        """Compute the total surprise of the network."""
        # Compute the surprise of each element using the binary cross-entropy formula
        category_likelihoods = self.sensory_expectation[self.get_one_hot_percept()]
        mask = category_likelihoods == 0. #avoids log(0)
        surprise_values = np.full(category_likelihoods.shape, fill_value = -1.) #uses -1., outside range of allowable values, to indicate where log(0) would have been taken
        surprise_values[~mask] = -np.log(category_likelihoods[~mask])/np.log(self.log_base)

        # Return surprise for each percept category
        return surprise_values

    def get_one_hot_percept(self):
        one_hot_percept = np.zeros(np.sum(self.category_sizes), dtype=bool) 
        one_hot_percept[np.cumsum(self.category_sizes) - self.category_sizes + self.percept] = True #add category state to category start index.
        return one_hot_percept

# %% ../nbs/lib_nbs/02_ECMs.ipynb 22
from .methods.transforms import _logistic, _exponentiated_shift
import numpy as np

class Bayesian_Memory_Filter(Bayesian_Filter):
    """
    Bayesian_Memory is a memory-augmented extension of the Bayesian_Network.
    It supports dynamic modification of internal weights to encode temporal traces.
    """
    def __init__(
        self,
        category_sizes: list,                        # Number of sensory input elements.
        memory_capacity: int,                        # Number of memory based hypotheses.
        memory_bias: float,                          # Proporition. Gives hypothesized probability of transitioning from non-memory hypothesis space to memory hypothesis space
        sensory_predictions: np.ndarray = None,       # Optional sensory hypotheses matrix. Initialized to uniform over sensory categories if not provided. First row must represent the non-memory 'catch-all' hypothesis if this matrix is provided
        belief_prior: np.ndarray = None,             # Optional 1d array of initial belief priors. 100% on non-memory hypothesis if not provided
        transition_predictions: np.ndarray = None,    # Optional hypothesis transition matrix.
        timer: int = 0,                              # Starting memory time index.
        data_record: list = [],                      # a list of variable names to log each time step. Accepts "all"
        record_until: int = -1                       # number of steps to prepare for data logging, negative values result in no data recording
    ):
        self.memory_capacity = memory_capacity
        self.memory_bias = memory_bias
        self.num_non_memory_hypotheses = 1 #This 'catch-all' hypothesis approximates the space of all possible hypotheses regarding the space of current and future percepts.
        num_hypotheses = self.num_non_memory_hypotheses + self.memory_capacity
        
        # Default to uniform sensory category state probabilities for memories if none are provided
        if sensory_predictions is None:
            #Create a vector of uniform probabilities over each category space to build default hypotheses
            uniform_probabilities = np.concatenate([np.full(category_size, fill_value = 1/category_size) for category_size in category_sizes])
            #Create sensory hypotheses for each memory slot and non-memory hypothesis using uniform probabilites
            sensory_predictions = np.tile(uniform_probabilities, (num_hypotheses, 1)) #each row is a hypothesis

        #initialize transition hypotheses
        if transition_predictions is None:
            transition_predictions = np.zeros((num_hypotheses, num_hypotheses)) #initialize
            #fill memory transition hypotheses
            for i in range(self.memory_capacity):
                j = (i+1) 
                transition_predictions[i,j] = 1 #initial memory hypotheses predict transition to next memory hypothesis
            #fill non-memory transition hypotheses
            for i in range(self.memory_capacity,num_hypotheses):
                transition_predictions[i,i] = 1 #initial non-memory transition hypotheses all predict transition to non-memory hypotheses space

                    
                    
        self.timer = timer

        if self.timer <= self.memory_capacity:
            self.effective_capacity = self.timer
        else:
            self.effective_capacity = self.memory_capacity
    
        # if some memory have been encoded, but not to capacity, a hypothesized transition between the non-memory hypothesis space and the first memory must be established
        if self.effective_capacity > 0 and self.effective_capacity < self.memory_capacity:
            for i in range(self.memory_capacity,num_hypotheses):
                transition_predictions[i, i] = 1 - self.memory_bias
                transition_predictions[i,self.timer] = self.memory_bias
            
        super().__init__(sensory_predictions = sensory_predictions, 
                         transition_predictions = transition_predictions, 
                         category_sizes = category_sizes, 
                         belief_prior = belief_prior, 
                         data_record = data_record,
                         record_until = record_until
                        )    
     
        if belief_prior is None:
            #overwrite default from super() so that belief prior is set on non-memory hypothesis space
            self.belief_prior = np.zeros(self.num_hypotheses)
            self.belief_prior[self.memory_capacity:] = 1/self.num_non_memory_hypotheses
        
        self.enforce_structure()        

    def enforce_structure(self):
        """
        runs checks to ensure that all variables in the Bayesian_Memory are properly defined
        """
        
        #enforce probability distribution in each percept category of encoded memories and predictions
        category_start_index = 0
        for i in range(len(self.category_sizes)):
            category_sums = np.sum(self.sensory_predictions[:,range(category_start_index, category_start_index + self.category_sizes[i])], axis = 1)
            if not np.all(np.abs(category_sums - 1) < 1e-9): #allows for floating-point precision
                raise ValueError("""Each hypothesis must encode a probability distribution over each category. 
                When the Memory Filter's internal timer was set to """ + str(self.timer) + """
                at least one row in 'sensory_predictions' did not contain values that sum to 1 for category """ + str(i))

            category_sum_prediction = np.sum(self.sensory_expectation[range(category_start_index, category_start_index + self.category_sizes[i])])
            if not np.abs(category_sum_prediction - 1) < 1e-9:
                raise ValueError("""Predictions for each sensory category must be a probability distribution. 
                When the Memory Filter's internal timer was set to """ + str(self.timer) + """
                sensory_expectation did not sum to 1 for category """ + str(i))
                                                                      
            category_start_index += self.category_sizes[i]

        #enforce probability structure in belief posterior, belief prior, and likelihoods
        if not np.abs(np.sum(self.belief_posterior) - 1) < 1e-9:
            raise ValueError("""belief_posterior must be a probability distiribution.
            When the Memory Filter's internal timer was set to """ + str(self.timer) + """
            belief_posterior did not sum to 1""")

        if not np.abs(np.sum(self.belief_prior) -1 ) < 1e-9:
            raise ValueError("""belief_prior must be a probability distiribution.
            When the Memory Filter's internal timer was set to """ + str(self.timer) + """
            belief_prior did not sum to 1""")

        if not np.all(self.likelihoods >= 0) and not np.all(self.likelihoods <= 1):
            raise ValueError("""likelihoods must be a probabilities.
            When the Memory Filters's internal timer was set to """ + str(self.timer) + """
            at least one value of likelihoods was not between 0 and 1""")

        if self.record_until is not None:
            if self.record_until >= 0 and self.data_timer == self.record_until + 1:
                print("Warning, memory process has exceded steps pre-allocated for data-recording. Further variable states will not be stored")

        if self.effective_capacity < self.memory_capacity:
            if np.sum(self.belief_prior[self.effective_capacity:self.memory_capacity]) > 1e-9:
                raise ValueError("""Posterior belief has been assigned to an unencoded memory slot, indicating a process error""")
                
    def sample(self, percept):
        '''
        The sample function takes a percept as input and returns a probability distribution over each sensory category as output.
        This probability distribution is a prediction for the next percept.
        In the process of generating this prediction, the new percept is encoded to memory slot, and a new
        prior belief (expectation) is set over the memory space.
        '''
        #record belief_prior and sensory expectation
        if self.data_timer < self.record_until and self.data_dic["belief_prior"] is not None:
            self.data_dic["belief_prior"][self.data_timer,:] = self.belief_prior

        if self.data_timer < self.record_until and self.data_dic["sensory_expectation"] is not None:
            self.data_dic["sensory_expectation"][self.data_timer,:] = self.sensory_expectation

        #get excitation and record
        self.update_likelihoods(percept)
        if self.data_timer < self.record_until and self.data_dic["likelihoods"] is not None:
            self.data_dic["likelihoods"][self.data_timer,:] = self.likelihoods

        #record surprise
        if self.data_timer < self.record_until and self.data_dic["surprise"] is not None:
            self.data_dic["surprise"][self.data_timer,:] = self.get_surprise()

        #activate and record
        self.update_belief_posterior()
        if self.data_timer < self.record_until and self.data_dic["belief_posterior"] is not None:
            self.data_dic["belief_posterior"][self.data_timer,:] = self.belief_posterior

        #encode memory
        self.encode_memory()

        #predict
        self.update_priors()

        # Advance Memory network's internal timer
        self.timer = (self.timer + 1) % self.memory_capacity

        #Advance timer for data logging
        self.data_timer += 1
        
        self.enforce_structure()
        return self.sensory_expectation
        
    def encode_memory(self):
        """
        Modify sensory_predictions and transition_predictions to encode the current percept into memory.
        This sets the current memory trace's excitation weights and transition weights.
        """
        # Encode current sensory excitation into memory
        categorical_encoding=self.get_one_hot_percept().astype(float) #set active sensory category states to 1
        self.sensory_predictions[self.timer,:] = categorical_encoding

        if self.effective_capacity < self.memory_capacity:
            self.effective_capacity += 1
        
        if self.effective_capacity > 0 and self.effective_capacity < self.memory_capacity:
            for i in range(self.memory_capacity,self.num_hypotheses):
                self.transition_predictions[i, i] = 1 - self.memory_bias
                self.transition_predictions[i,0] = self.memory_bias
        else:
            for i in range(self.memory_capacity,self.num_hypotheses):                
                self.transition_predictions[i,:] = 0
                self.transition_predictions[i, i] = 1

# %% ../nbs/lib_nbs/02_ECMs.ipynb 26
from .methods.transforms import _decay_toward_uniform

class Short_Term_Memory(Bayesian_Memory_Filter):
    def __init__(self,
                 category_sizes: list,                    # Number of sensory input elements.
                 memory_capacity: int,                        # Number of memory nodes.                 
                 memory_bias: float,                      # predicted transition probability from non-memory hypothesis space to unreachable memory hypothesis space
                 fading_rate: float,                  # rate parameter on the exponential decay toward uniform for values in each percept category of memory traces
                 surprise_factor: float = 0.,         #scales the degree to which surprise slows down memory fading
                 sensory_predictions: np.ndarray = None,               # Optional sensory-to-memory weight matrix.
                 belief_prior: np.ndarray = None,        # Optional 1d array of prior expectations on memories
                 transition_predictions: np.ndarray = None,     # Optional memory transition matrix.
                 timer: int = 0,                          # Starting memory time index.
                 data_record: list = [],                      # a list of variable names to record each time step. Accepts "all"
                 record_until: int = -1                    # number of steps to prepare for data recording, negative values result in no data recording
                ):
        super().__init__(category_sizes = category_sizes, 
                         memory_capacity = memory_capacity,
                         memory_bias = memory_bias,
                         sensory_predictions = sensory_predictions, 
                         belief_prior = belief_prior, 
                         transition_predictions = transition_predictions, 
                         timer = timer, 
                         data_record = data_record, 
                         record_until = record_until)
        assert 0 <= surprise_factor and 1 >= surprise_factor
        self.surprise_factor = surprise_factor
        self.fading_rate = fading_rate #initial rate at which all memories fade
        self.memory_fade = np.zeros((np.shape(self.sensory_predictions)[0],len(self.category_sizes))) #current fading rate for all memories (0 when memory is not encoded)

    def sample(self, percept):
        self.fade()
        return super().sample(percept)

    def encode_memory(self):
        super().encode_memory()
        self.memory_fade[self.timer,:] = (1 - self.surprise_factor) * self.fading_rate + self.surprise_factor * self.fading_rate ** self.get_surprise()

    def fade(self):
        for i in range(len(self.category_sizes)):
            category_rates = self.memory_fade[:,i]
            #mix each probability distribution (row) with a uniform distribution.
            faded_memories = _decay_toward_uniform(self.sensory_predictions[:,self.category_indexer == i], category_rates)
            self.sensory_predictions[:,self.category_indexer == i] = faded_memories

# %% ../nbs/lib_nbs/02_ECMs.ipynb 32
## Fading Rate Encoders
def sigmoid_fading_rate(gamma, sigma, sensor_state_probabilities, log_base = 2):
    #print(f'probabilites: {sensor_state_probabilities}')
    skew = np.log(gamma/(1-gamma))/np.log(log_base) #sets rate to gamma when logistic component is 0 (i.e. at sigmoid center)
    #print(f'skew: {skew}')
    shift = 2*sensor_state_probabilities - 1 #centers sigmoid at p = 0.5
    #print(f'shift: {shift}')
    scale = -np.log(1-sigma)/np.log(log_base) #slope of sigmoid
    #print(f'scale: {scale}')
    x = skew + scale * shift
    fading_rates = 1/(1 + log_base ** -x)
    #print(f'fading_rates: {fading_rates}')
    return(fading_rates)

def surprise_advantage_fading_rate(gamma,sigma, category_indexes, categorical_predictions, categorical_suprises, log_base = 2):
    sensor_labels, inv = np.unique(category_indexes, return_inverse=True)
    plogp = categorical_predictions * np.log(categorical_predictions)
    prediction_entropies = -np.bincount(inv, weights = plogp, minlength = sensor_labels.size) #sums the plogp for each value in category_indexer and returns as 1d array
    converted_entropies = prediction_entropies/np.log(log_base)
    surprise_gaps = categorical_surprises - converted_entropies
    fading_rates = gamma ** (log_base ** (sigma * surprise_gaps))
    return(fading_rates)

# %% ../nbs/lib_nbs/02_ECMs.ipynb 34
class Long_Term_Memory(Short_Term_Memory):
    def __init__(self,
                 category_sizes: list,                    # Number of sensory input elements.
                 memory_capacity: int,                        # Number of memory nodes.                 
                 memory_bias: float,                      # predicted transition probability from non-memory hypothesis space to unreachable memory hypothesis space
                 fading_rate: float,                  # rate parameter on the exponential decay toward uniform for values in each percept category of memory traces
                 surprise_factor: float = 0.,         #scales the degree to which surprise slows down memory fading
                 reuse_factor: float = 0.,
                 sensory_predictions: np.ndarray = None,               # Optional sensory-to-memory weight matrix.
                 belief_prior: np.ndarray = None,        # Optional 1d array of prior expectations on memories
                 transition_predictions: np.ndarray = None,     # Optional memory transition matrix.
                 timer: int = 0,                          # Starting memory time index.
                 data_record: list = [],                      # a list of variable names to record each time step. Accepts "all"
                 record_until: int = -1,                   # number of steps to prepare for data recording, negative values result in no data recording
                 fading_rate_method = "sigmoid"
                ):
        super().__init__(category_sizes = category_sizes, 
                         memory_capacity = memory_capacity,
                         memory_bias = memory_bias,
                         fading_rate = fading_rate,
                         surprise_factor = surprise_factor,
                         sensory_predictions = sensory_predictions, 
                         belief_prior = belief_prior, 
                         transition_predictions = transition_predictions, 
                         timer = timer, 
                         data_record = data_record, 
                         record_until = record_until)
        self.reuse_factor = reuse_factor
        self.fading_rate_method = fading_rate_method

    def sample(self, percept):
        super().sample(percept)
        self.stablize_memories()

    def stablize_memories(self):
        for i in range(self.timer):
            #reduce fading rate of reactivated memory accoring to weight of reactivation (posterior belief)
            self.memory_fade[i,:] = self.memory_fade[i,:] * (1-(self.belief_posterior[i] * self.reuse_factor))
    
    def encode_memory(self):
        """
        Modify sensory_predictions and transition_predictions to encode the current percept into memory.
        This sets the current memory trace's excitation weights and transition weights.
        """
        # Encode current sensory excitation into memory
        categorical_encoding=self.get_one_hot_percept().astype(float) #set active sensory category states to 1
        self.sensory_predictions[self.timer,:] = categorical_encoding

        if self.effective_capacity < self.memory_capacity:
            self.effective_capacity += 1
        
        if self.effective_capacity > 0 and self.effective_capacity < self.memory_capacity:
            for i in range(self.memory_capacity,self.num_hypotheses):
                self.transition_predictions[i, i] = 1 - self.memory_bias
                self.transition_predictions[i,0] = self.memory_bias
        else:
            for i in range(self.memory_capacity,self.num_hypotheses):                
                self.transition_predictions[i,:] = 0
                self.transition_predictions[i, i] = 1

        self.memory_fade[self.timer,:] = self.get_fading_rates()

    def get_fading_rates(self):
        if self.fading_rate_method == "sigmoid":
            fading_rates = sigmoid_fading_rate(gamma = self.fading_rate, 
                                                sigma = self.surprise_factor, 
                                                sensor_state_probabilities = self.sensory_expectation[self.get_one_hot_percept()], 
                                                log_base = self.log_base)
        elif self.fading_rate_method == "surprise_advantage":
            fading_rates = surprise_advantage_fading_rate(gamma = self.fading_rate, 
                                                          sigma = self.surprise_factor, 
                                                          category_indexes = self.category_indexer, 
                                                          categorical_predictions = self.sensory_expectation, 
                                                          categorical_suprises = self.get_surprise(),
                                                          log_base = self.log_base)
        else:
            raise ValueError(f'{self.fading_rate_method} is not a valid fading_rate_method')

        return(fading_rates)


# %% ../nbs/lib_nbs/02_ECMs.ipynb 40
class Semantic_Memory(Long_Term_Memory):
    def __init__(self,
                 category_sizes: list,                    # Number of sensory input elements.
                 memory_capacity: int,                        # Number of memory nodes.                 
                 memory_bias: float,                      # predicted transition probability from non-memory hypothesis space to unreachable memory hypothesis space
                 fading_rate: float,                  # rate parameter on the exponential decay toward uniform for values in each percept category of memory traces
                 surprise_factor: float = 0.,         #scales the degree to which surprise slows down memory fading
                 reuse_factor: float = 0.,
                 learning_factor: float = 0.,        #scales influence of prediction differences on transition weights
                 reencoding_factor: float = 0.,    #scales the degree to which re-activation of a memory affects its sensory predictions
                 sensory_predictions: np.ndarray = None,               # Optional sensory-to-memory weight matrix.
                 belief_prior: np.ndarray = None,        # Optional 1d array of prior expectations on memories
                 transition_predictions: np.ndarray = None,     # Optional memory transition matrix.
                 timer: int = 0,                          # Starting memory time index.
                 data_record: list = [],                      # a list of variable names to record each time step. Accepts "all"
                 record_until: int = -1,                    # number of steps to prepare for data recording, negative values result in no data recording
                 fading_rate_method = "sigmoid"
                ):
        super().__init__(category_sizes = category_sizes, 
                         memory_capacity = memory_capacity,
                         memory_bias = memory_bias,
                         fading_rate = fading_rate,
                         surprise_factor = surprise_factor,
                         sensory_predictions = sensory_predictions,
                         reuse_factor = reuse_factor,
                         belief_prior = belief_prior, 
                         transition_predictions = transition_predictions, 
                         timer = timer, 
                         data_record = data_record, 
                         record_until = record_until,
                        fading_rate_method = fading_rate_method)
        self.learning_factor = learning_factor
        self.reencoding_factor = reencoding_factor
        self.transition_weights = self.transition_predictions.copy()
        self.presynaptic_activations = np.zeros_like(self.transition_predictions) #initialize

    def sample(self, percept):
        self.last_posterior = self.belief_posterior.copy() #don't need to make this copy if transition updates is integrated into sample function (must come after posterior update, but before prior update)
        self.presynaptic_activations = self.last_posterior[:,np.newaxis] * self.transition_predictions
        super().sample(percept)
        self.reencode_memories()
        self.update_transitions()

    def reencode_memories(self):
        categorical_encoding=self.get_one_hot_percept().astype(float) #set active sensory category states to 1
        for i in range(self.timer):
            #wieght current sensory state in reincoding
            reencoding_strength = self.reencoding_factor * self.belief_posterior[i] 
            #mix old encoding with new encoding according to weight
            reencoded_memory = reencoding_strength * categorical_encoding + (1-reencoding_strength) * self.sensory_predictions[i,:]
            self.sensory_predictions[i,:] = reencoded_memory

    def update_transitions(self):
        weighted_synapse_differences = self.learning_factor * (np.outer(self.last_posterior, self.belief_posterior)  - self.presynaptic_activations)
        #add synapse differences to transition weights for memory-based hypothesis
        self.transition_weights[:self.memory_capacity,:self.memory_capacity] = np.maximum(self.transition_weights[:self.memory_capacity,:self.memory_capacity] + weighted_synapse_differences[:self.memory_capacity,:self.memory_capacity], 0)
        row_sums = np.sum(self.transition_weights, axis=1, keepdims = True)
        self.transition_predictions = self.transition_weights/row_sums
