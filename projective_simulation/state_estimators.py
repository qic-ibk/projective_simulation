# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/lib_nbs/04_State_Estimators.ipynb.

# %% auto 0
__all__ = ['Bayesian_Filter', 'Initialize_Memory_Based_Transition_Matrix', 'Initialize_Sensory_Predictions', 'Sequence_Memory',
           'Short_Term_Memory', 'sigmoid_fading_rate', 'surprise_advantage_fading_rate', 'Long_Term_Memory']

# %% ../nbs/lib_nbs/04_State_Estimators.ipynb 4
import numpy as np
from .ECMs import Abstract_ECM

class Bayesian_Filter(Abstract_ECM):
    """
    Bayesian Filter for recursive state estimation.

    Each state/hypothesis posits:
      1. A probability for all states of K sensory categories (with discrete state spaces).
      2. A probability that each state/hypothesis will be true when the sensory category states are next updated.

    The filter maintains a belief state over these hypotheses/states, assuming they are exhaustive and mutually exclusive.

    Args:
        category_sizes (list): Number of states per sensory category.
        sensory_predictions (np.ndarray): Each row is a hypothesis; columns grouped by category_sizes, each group sums to 1.
        transition_predictions (np.ndarray): Each row is a hypothesis; columns give predicted probabilities for each hypothesis being true at the next step.
        belief_prior (np.ndarray, optional): Initial belief prior. If None, set to uniform.
        log_base (float, optional): Base for logarithms and exponentials.
        data_record (list, optional): List of internal variable names to record each time step. Accepts "all".
        record_until (int, optional): Number of time steps to record. Default -1 disables recording.
    """
    def __init__(
        self,
        category_sizes: list,
        sensory_predictions: np.ndarray,
        transition_predictions: np.ndarray,
        belief_prior: np.ndarray = None,
        log_base=2.,
        data_record: list = [],
        record_until: int = -1
    ):
        self.category_sizes = category_sizes
        self.category_indexer = np.repeat(np.arange(0, len(category_sizes)), category_sizes)
        self.num_hypotheses = np.shape(sensory_predictions)[0]
        self.transition_predictions = transition_predictions
        self.sensory_predictions = sensory_predictions
        self.log_base = log_base

        # --- State initialization ---
        self.percept = np.empty(len(self.category_sizes), dtype=int)
        self.likelihoods = np.zeros(self.num_hypotheses)
        self.belief_prior = np.full(self.num_hypotheses, 1/self.num_hypotheses) if belief_prior is None else belief_prior
        self.sensory_expectation = self.belief_prior @ self.sensory_predictions
        self.belief_posterior = np.full(self.num_hypotheses, 1/self.num_hypotheses)
        self.prepare_data_storage(data_record, record_until)

    def _check_structure(self):
        if sum(self.category_sizes) != np.shape(self.sensory_predictions)[1]:
            raise ValueError("sensory_predictions must have a number of columns equal to the total number of percept category states")

        if np.shape(self.sensory_predictions)[0] != np.shape(self.transition_predictions)[0]:
            raise ValueError("sensory_predictions and transition_predictions must have the same number of rows")
        if np.shape(self.transition_predictions)[0] != np.shape(self.transition_predictions)[1]:
            raise ValueError("transition_predictions must be square")
        
        #~~Enforcement of probability distributions~~
        for k in range(len(self.category_sizes)):
            pred_sum = np.sum(self.sensory_expectation[self.category_indexer == k])
            if not np.abs(pred_sum - 1) < 1e-9:
                raise ValueError(
                    f"Predictions for each sensory category must be a probability distribution. "
                    f"Timer={self.timer}, category={i}"
                )
            for i in range(self.num_hypotheses):
                if not np.abs(np.sum(self.sensory_predictions[i, self.category_indexer == k]) - 1) < 1e-9:
                    raise ValueError("Each sensory category must sum to 1 in every hypothesis")
        if not np.abs(np.sum(self.belief_posterior) - 1) < 1e-9:
            raise ValueError("belief_posterior must sum to 1")
        if not np.abs(np.sum(self.belief_prior) - 1) < 1e-9:
            raise ValueError("belief_prior must sum to 1")
        if not (np.all(self.likelihoods >= 0) and np.all(self.likelihoods <= 1)):
            raise ValueError("likelihoods must be between 0 and 1")
        #~~End enforcement of probability distributions~~

        if self.record_until is not None:
            if self.record_until >= 0 and self.data_timer == self.record_until + 1:
                print("Warning: memory process exceeded steps pre-allocated for data-recording.")

    def prepare_data_storage(self, data_record, record_until):
        """Set up data recording structure for diagnostics and visualization."""
        self.data_dic = {
            "belief_prior": None,
            "likelihoods": None,
            "belief_posterior": None,
            "sensory_expectation": None,
            "surprise": None
        }
        self.data_timer = 0
        self.record_until = record_until

        if "all" in data_record:
            if len(data_record) > 1:
                print("Warning: 'all' in data_record; additional entries ignored")
            data_record = self.data_dic.keys()

        if self.record_until < 0 and len(data_record) > 0:
            print("Warning: data_record set but record_until < 0; no data will be recorded")

        for variable in data_record:
            if variable not in self.data_dic:
                print(f"Warning: {variable} not a valid variable name; ignored")
            elif variable in ["belief_prior", "likelihoods", "belief_posterior"]:
                self.data_dic[variable] = np.full((self.record_until, self.num_hypotheses), -1., dtype=float)
            elif variable == "sensory_expectation":
                self.data_dic[variable] = np.full((self.record_until, np.sum(self.category_sizes)), -1., dtype=float)
            elif variable == "surprise":
                self.data_dic[variable] = np.full((self.record_until, len(self.category_sizes)), -1., dtype=float)

    def sample(self, percept) -> np.ndarray:
        """
        Given a percept, update belief and return a vector of sensory expectations.
        """
        self._check_structure()
        assert isinstance(percept, np.ndarray)
        assert np.issubdtype(percept.dtype, np.integer)
        if self.data_timer < self.record_until and self.data_dic["belief_prior"] is not None:
            self.data_dic["belief_prior"][self.data_timer, :] = self.belief_prior
        if self.data_timer < self.record_until and self.data_dic["sensory_expectation"] is not None:
            self.data_dic["sensory_expectation"][self.data_timer, :] = self.sensory_expectation

        self.percept = percept
        self.update_likelihoods(percept)
        if self.data_timer < self.record_until and self.data_dic["likelihoods"] is not None:
            self.data_dic["likelihoods"][self.data_timer, :] = self.likelihoods

        if self.data_timer < self.record_until and self.data_dic["surprise"] is not None:
            self.data_dic["surprise"][self.data_timer, :] = self.get_surprise()

        self.update_belief_posterior()
        if self.data_timer < self.record_until and self.data_dic["belief_posterior"] is not None:
            self.data_dic["belief_posterior"][self.data_timer, :] = self.belief_posterior
        
        self.learn(percept) # Placeholder for learning step in child classes

        self.update_priors()

        self.data_timer += 1
        return self.sensory_expectation

    def update_likelihoods(self, percept: np.ndarray):
        """Update the likelihoods for each hypothesis/state given the new percept."""
        if percept.shape[0] != len(self.category_sizes):
            raise ValueError(f'Percept vector size of {percept.shape[0]} does not match the number of perceptual categories, {len(self.category_sizes)}.')
        for i in range(len(self.category_sizes)):
            if percept[i] not in range(self.category_sizes[i]):
                raise ValueError("Percept state out of range for category.")
        one_hot_percept = self.get_one_hot_percept() # boolean mask for current percept's one-hot encoding
        category_likelihoods = self.sensory_predictions[:, one_hot_percept] # shape (num_hypotheses, num_categories)
        self.likelihoods = np.prod(category_likelihoods, axis=1) # shape (num_hypotheses,)

    def update_belief_posterior(self):
        """Update the posterior belief over hypotheses."""
        unnormalized_posterior = self.likelihoods * self.belief_prior # element-wise product
        normalization_constant = np.sum(unnormalized_posterior)
        if np.sum(unnormalized_posterior) != 0:
            self.belief_posterior = unnormalized_posterior / normalization_constant
        else:
            print("Warning: unnormalized posterior probabilities sum to 0. Model may be invalid.")
            self.belief_posterior = np.zeros(self.num_hypotheses)
        
    def learn(self, percept = None):
        """
        Placeholder for learning step in child classes.
        """
        pass

    def update_priors(self):
        """Update the prior belief and sensory expectation for the next step."""
        self.update_belief_prior()
        self.sensory_expectation = self.belief_prior @ self.sensory_predictions # sum of sensory state probabilities weighted across states/hypotheses by prior belief

    def update_belief_prior(self):
        """Update the prior belief using the transition predictions and posterior."""
        row_sums = self.transition_predictions.sum(axis=1, keepdims=True)
        assert np.all(np.abs(row_sums - 1) < 1e-9), "Each row of transition_predictions must sum to 1"
        self.belief_prior = self.belief_posterior @ self.transition_predictions

    def get_surprise(self) -> float:
        """Compute the total surprise of the network. 0 likelihood observations (inf surprise) return -1."""
        category_likelihoods = self.sensory_expectation[self.get_one_hot_percept()]
        mask = category_likelihoods == 0. # avoids log(0)
        surprise_values = np.full(category_likelihoods.shape, -1.)
        surprise_values[~mask] = -np.log(category_likelihoods[~mask]) / np.log(self.log_base)
        return surprise_values

    def get_one_hot_percept(self):
        """Return a boolean mask for the current percept's one-hot encoding."""
        one_hot_percept = np.zeros(np.sum(self.category_sizes), dtype=bool)
        one_hot_percept[np.cumsum(self.category_sizes) - self.category_sizes + self.percept] = True
        return one_hot_percept

# %% ../nbs/lib_nbs/04_State_Estimators.ipynb 8
import numpy as np
def Initialize_Memory_Based_Transition_Matrix(memory_capacity, 
                                              memory_bias, 
                                              num_hypotheses, 
                                              capacity_overflow_method, 
                                              schematic_transition_method="first"):
    """
    Initializes a transition matrix for a memory-based ECM with a given memory capacity and memory bias.
    If transition_predictions is provided, it will be used as the base transition matrix. Otherwise, a default matrix will be created.
    The schematic_transition_method determines how the transition matrix is structured:
        - "encoded": Memory hypotheses have a self-loop with probability 1 - memory_bias and transition to the timer hypothesis with probability memory_bias.
        - "loop": Memory hypotheses transition to the next memory hypothesis in a loop.
        - "stop": Memory hypotheses do not transition to any other hypothesis (self-loop with probability 1).
    """
    transition_predictions = np.zeros((num_hypotheses, num_hypotheses))
    # Fill the shifted diagonal for memory hypotheses (i.e., each i transitions to i+1, last to first)
    for i in range(memory_capacity):
        transition_predictions[i, (i + 1) % memory_capacity] = 1.0

    if capacity_overflow_method == "stop encoding" or schematic_transition_method == "learned":
        transition_predictions[memory_capacity - 1, 0] = memory_bias
        for i in range(memory_capacity, num_hypotheses):
            transition_predictions[i, i] = 1 - memory_bias
            transition_predictions[i, 0] = memory_bias
            transition_predictions[memory_capacity - 1, i] = (1-memory_bias) / (num_hypotheses - memory_capacity)
    elif capacity_overflow_method == "loop":
        for i in range(memory_capacity, num_hypotheses):
            transition_predictions[i, i] = 1 - memory_bias
            transition_predictions[i, 0] = memory_bias
    else:
        raise ValueError(f'{capacity_overflow_method} is not a valid capacity overflow method')

    return transition_predictions

def Initialize_Sensory_Predictions(category_sizes, num_hypotheses):
    """
    Initializes a sensory prediction matrix for a memory-based ECM with a given category sizes and number of hypotheses.
    """
    uniform_probabilities = np.concatenate([
        np.full(category_size, 1 / category_size) for category_size in category_sizes
            ])
    sensory_predictions = np.tile(uniform_probabilities, (num_hypotheses, 1))

    return sensory_predictions

# %% ../nbs/lib_nbs/04_State_Estimators.ipynb 9
from .methods.transforms import _logistic, _exponentiated_shift
import numpy as np

class Sequence_Memory(Bayesian_Filter):
    """
    Memory-augmented Bayesian filter that encodes a sequence of percepts as memory traces.
    Each memory trace is a hypothesis about the environment, and the agent can transition between
    non-memory and memory hypotheses. Supports dynamic modification of internal weights to encode temporal traces.

    Args:
        category_sizes (list): Number of sensory input elements.
        memory_capacity (int): Number of memory-based hypotheses.
        memory_bias (float): Probability of transitioning from non-memory to memory hypothesis space.
        sensory_predictions (np.ndarray, optional): Sensory hypotheses matrix. If None, initialized to uniform.
        belief_prior (np.ndarray, optional): Initial belief priors. If None, all prior on non-memory hypothesis.
        transition_predictions (np.ndarray, optional): Hypothesis transition matrix.
        timer (int, optional): Starting memory time index.
        capacity_overflow_method (str, optional): 'loop' or 'stop encoding'.
        data_record (list, optional): List of variable names to log each time step. Accepts "all".
        record_until (int, optional): Number of steps to prepare for data logging. Negative disables recording.
    """
    def __init__(
        self,
        category_sizes: list,
        memory_capacity: int,
        memory_bias: float,
        sensory_predictions: np.ndarray = None,
        belief_prior: np.ndarray = None,
        transition_predictions: np.ndarray = None,
        timer: int = 0,
        capacity_overflow_method="stop encoding",
        data_record: list = [],
        record_until: int = -1
    ):
        """
        Initialize the Sequence_Memory filter.
        """
        self.memory_capacity = memory_capacity
        self.memory_bias = memory_bias
        self.capacity_overflow_method = capacity_overflow_method
        self.num_non_memory_hypotheses = 1
        self.timer = timer
        self.effective_capacity = min(self.timer, self.memory_capacity)

        num_hypotheses = self.num_non_memory_hypotheses + self.memory_capacity
        #default to uniform sensory predictions if not provided
        if sensory_predictions is None:
            sensory_predictions = Initialize_Sensory_Predictions(
                category_sizes=category_sizes,
                num_hypotheses=num_hypotheses
            )

        #default to transition matrix that encodes sequence memory if not provided
        if transition_predictions is None:
            transition_predictions = Initialize_Memory_Based_Transition_Matrix(
                memory_capacity=self.memory_capacity,
                memory_bias=self.memory_bias,
                num_hypotheses=num_hypotheses,
                capacity_overflow_method=self.capacity_overflow_method
            )
        
        super().__init__(
            sensory_predictions=sensory_predictions,
            transition_predictions=transition_predictions,
            category_sizes=category_sizes,
            belief_prior=belief_prior,
            data_record=data_record,
            record_until=record_until
        )

        if belief_prior is None:
            self.belief_prior = np.zeros(self.num_hypotheses)
            self.belief_prior[self.memory_capacity:] = 1 / self.num_non_memory_hypotheses
        
        #overide placeholder belief posterior initialization from parent class to meet new requirements
        self.belief_posterior = np.zeros(self.num_hypotheses)
        self.belief_posterior[self.memory_capacity:] = 1 / self.num_non_memory_hypotheses 

    def _check_structure(self):
        """
        Check that all internal probability distributions are valid (Per Parent class).
        Check that no belief is assigned to unencoded memory slots.
        """
        super()._check_structure()
        if self.effective_capacity < self.memory_capacity:
            if np.sum(self.belief_prior[self.effective_capacity:self.memory_capacity]) > 1e-9:
                raise ValueError("Posterior belief assigned to unencoded memory slot.")

    def sample(self, percept):
        """
        Given a percept; update belief state, update world model, predict next sensor states.

        Args:
            percept (np.ndarray): The observed percept.

        Returns:
            np.ndarray: The predicted sensory expectation for the next step.
        """
        super().sample(percept)
        self.timer = (self.timer + 1) % self.memory_capacity
        return self.sensory_expectation
    
    def learn(self, percept  = None):
        """
        Encode the current percept into memory by updating the sensory and transition predictions.
        """
        if percept is None:
            percept = self.percept
        if self.effective_capacity < self.memory_capacity or not self.capacity_overflow_method == "stop encoding":
            # if last memory slot filled and looping, update transition matrix to loop and decouple non-memory hypotheses
            if self.effective_capacity + 1 == self.memory_capacity:
                self.capacity_reached()
            self.encode_memory(percept)
        self._check_structure()

    def capacity_reached(self):
        """
        Handle encoding of last memory hypothesis when memory capacity is reached.
        """
        if self.capacity_overflow_method == "loop":
            self.transition_predictions[self.effective_capacity, :] = 0
            self.transition_predictions[self.effective_capacity, 0] = 1
            for i in range(self.memory_capacity, self.num_hypotheses):
                self.transition_predictions[i, i] = 1
                self.transition_predictions[i, 0] = 0   

    def encode_memory(self, percept):
        """
        Encode the current percept into memory by updating the sensory and transition predictions.
        """
        if self.effective_capacity < self.memory_capacity:
            self.effective_capacity += 1
            
        categorical_encoding = self.get_one_hot_percept().astype(float)
        self.sensory_predictions[self.timer, :] = categorical_encoding

            

# %% ../nbs/lib_nbs/04_State_Estimators.ipynb 13
from .methods.transforms import _decay_toward_uniform

class Short_Term_Memory(Sequence_Memory):
    """
    Short-Term Memory filter that extends Sequence_Memory by introducing memory fading.
    Each encoded memory trace fades toward a uniform distribution at a specified rate.
    Supports different schematic transition methods and capacity overflow behaviors.

    Args:
        category_sizes (list): Number of sensory input elements.
        memory_capacity (int): Number of memory nodes.
        memory_bias (float): Transition probability from non-memory to memory hypothesis.
        fading_rate (float): Rate parameter for exponential decay toward uniform for memory traces.
        sensory_predictions (np.ndarray, optional): Optional sensory-to-memory weight matrix.
        belief_prior (np.ndarray, optional): Optional 1d array of prior expectations on memories.
        transition_predictions (np.ndarray, optional): Optional memory transition matrix.
        timer (int, optional): Starting memory time index.
        data_record (list, optional): List of variable names to record each time step. Accepts "all".
        record_until (int, optional): Number of steps to prepare for data recording. Negative disables recording.
        capacity_overflow_method (str, optional): 'loop' or 'stop encoding'.
        schematic_transition_method (str, optional): 'encoded', 'first', or 'learned'.
    """
    def __init__(
        self,
        category_sizes: list,
        memory_capacity: int,
        memory_bias: float,
        fading_rate: float,
        sensory_predictions: np.ndarray = None,
        belief_prior: np.ndarray = None,
        transition_predictions: np.ndarray = None,
        timer: int = 0,
        data_record: list = [],
        record_until: int = -1,
        capacity_overflow_method="loop",
        schematic_transition_method="encoded"
    ):
        """
        Initialize the Short_Term_Memory filter.
        """
        super().__init__(
            category_sizes=category_sizes,
            memory_capacity=memory_capacity,
            memory_bias=memory_bias,
            sensory_predictions=sensory_predictions,
            belief_prior=belief_prior,
            transition_predictions=transition_predictions,
            timer=timer,
            data_record=data_record,
            record_until=record_until,
            capacity_overflow_method=capacity_overflow_method
        )
        self.fading_rate = fading_rate
        self.schematic_transition_method = schematic_transition_method

        if self.schematic_transition_method == "encoded" and transition_predictions is None:
            for i in range(self.memory_capacity, np.shape(self.transition_predictions)[0]):
                self.transition_predictions[i, i] = 1 - self.memory_bias
                self.transition_predictions[i, self.timer] = self.memory_bias

    def learn(self, percept):
        """
        Fade all memory traces, then update memory with the new percept.

        Args:
            percept (np.ndarray): The observed percept.

        Returns:
            np.ndarray: The predicted sensory expectation for the next step.
        """
        self.fade()
        return super().learn(percept)

    def encode_memory(self, percept):
        """
        Encode the current percept into memory, updating sensory and transition predictions.
        Handles schematic transition methods and capacity overflow.
        """
        super().encode_memory(percept)
        if self.schematic_transition_method == "encoded":
            for i in range(self.memory_capacity, self.num_hypotheses):
                self.transition_predictions[i, :self.effective_capacity] = self.memory_bias/self.effective_capacity
        elif self.schematic_transition_method == "learned":
            for i in range(self.memory_capacity,self.num_hypotheses):
                #add weight to transition from non-memory hypotheses to current memory hypothesis then renormalize
                self.transition_predictions[i, self.timer] = self.memory_bias/self.effective_capacity
                self.transition_predictions[i,:] = self.transition_predictions[i,:] / np.sum(self.transition_predictions[i,:])


    def capacity_reached(self):
        """
        Handle encoding of last memory hypothesis when memory capacity is reached.
        Overwrites method from Sequence Memory to handle schematic transition methods.
        """
        if self.capacity_overflow_method == "loop":
            self.transition_predictions[self.effective_capacity, :] = 0
            self.transition_predictions[self.effective_capacity, 0] = 1
            if not self.schematic_transition_method == "encoded":
                for i in range(self.memory_capacity, self.num_hypotheses):
                    self.transition_predictions[i, i] = 1
                    self.transition_predictions[i, 0] = 0 

    def fade(self):
        """
        Apply exponential decay toward uniform distribution for each memory trace.
        """
        for i in range(len(self.category_sizes)):
            faded_memories = _decay_toward_uniform(
                self.sensory_predictions[:self.memory_capacity, self.category_indexer == i],
                self.fading_rate
            )
            self.sensory_predictions[:self.memory_capacity, self.category_indexer == i] = faded_memories

# %% ../nbs/lib_nbs/04_State_Estimators.ipynb 20
## Fading Rate Encoders
def sigmoid_fading_rate(gamma, sigma, sensor_state_probabilities, log_base = 2):
    #print(f'probabilites: {sensor_state_probabilities}')
    skew = np.log(gamma/(1-gamma))/np.log(log_base) #sets rate to gamma when logistic component is 0 (i.e. at sigmoid center)
    #print(f'skew: {skew}')
    shift = 2*sensor_state_probabilities - 1 #centers sigmoid at p = 0.5
    #print(f'shift: {shift}')
    scale = -np.log(1-sigma)/np.log(log_base) #slope of sigmoid
    #print(f'scale: {scale}')
    x = skew + scale * shift
    fading_rates = 1/(1 + log_base ** -x)
    #print(f'fading_rates: {fading_rates}')
    return(fading_rates)

def surprise_advantage_fading_rate(gamma,sigma, category_indexes, categorical_predictions, categorical_suprises, log_base = 2):
    sensor_labels, inv = np.unique(category_indexes, return_inverse=True)
    plogp = categorical_predictions * np.log(categorical_predictions)
    prediction_entropies = -np.bincount(inv, weights = plogp, minlength = sensor_labels.size) #sums the plogp for each value in category_indexer and returns as 1d array
    converted_entropies = prediction_entropies/np.log(log_base)
    surprise_gaps = categorical_surprises - converted_entropies
    fading_rates = gamma ** (log_base ** (sigma * surprise_gaps))
    return(fading_rates)

# %% ../nbs/lib_nbs/04_State_Estimators.ipynb 22
class Long_Term_Memory(Short_Term_Memory):
    """
    Long-Term Memory filter that extends Short_Term_Memory by introducing surprise-modulated memory fading and memory stabilization.
    Each encoded memory trace fades toward a uniform distribution at a rate modulated by the surprise of the encoded percept and by reactivation.
    Reactivation slows fading and can reinforce the memory trace, stabilizing early memories of sequences.

    Args:
        category_sizes (list): Number of sensory input elements.
        memory_capacity (int): Number of memory nodes.
        memory_bias (float): Transition probability from non-memory to memory hypothesis.
        fading_rate (float): Base rate parameter for exponential decay toward uniform for memory traces.
        surprise_factor (float, optional): Scales the degree to which surprise slows down memory fading.
        reuse_factor (float, optional): Scales the degree to which reactivation slows fading.
        sensory_predictions (np.ndarray, optional): Optional sensory-to-memory weight matrix.
        belief_prior (np.ndarray, optional): Optional 1d array of prior expectations on memories.
        transition_predictions (np.ndarray, optional): Optional memory transition matrix.
        timer (int, optional): Starting memory time index.
        data_record (list, optional): List of variable names to record each time step. Accepts "all".
        record_until (int, optional): Number of steps to prepare for data recording. Negative disables recording.
        fading_rate_method (str, optional): Method for computing fading rates ("sigmoid" or "surprise_advantage").
        capacity_overflow_method (str, optional): 'loop' or 'stop encoding'.
        schematic_transition_method (str, optional): 'encoded', 'first', or 'learned'.
    """
    def __init__(
        self,
        category_sizes: list,
        memory_capacity: int,
        memory_bias: float,
        fading_rate: float,
        surprise_factor: float = 0.,
        reuse_factor: float = 0.,
        sensory_predictions: np.ndarray = None,
        belief_prior: np.ndarray = None,
        transition_predictions: np.ndarray = None,
        timer: int = 0,
        data_record: list = [],
        record_until: int = -1,
        fading_rate_method = "sigmoid",
        capacity_overflow_method = "stop encoding",
        schematic_transition_method = "encoded"
    ):
        """
        Initialize the Long_Term_Memory filter.
        """
        super().__init__(
            category_sizes=category_sizes,
            memory_capacity=memory_capacity,
            memory_bias=memory_bias,
            fading_rate=fading_rate,
            sensory_predictions=sensory_predictions,
            belief_prior=belief_prior,
            transition_predictions=transition_predictions,
            timer=timer,
            data_record=data_record,
            record_until=record_until,
            capacity_overflow_method=capacity_overflow_method,
            schematic_transition_method=schematic_transition_method
        )
        assert 0 <= surprise_factor <= 1, "surprise_factor must be in [0, 1]"
        self.surprise_factor = surprise_factor
        self.reuse_factor = reuse_factor
        self.fading_rate_method = fading_rate_method
        self.memory_fade = np.zeros((np.shape(self.sensory_predictions)[0], len(self.category_sizes)))  # fading rates for all memories

    def learn(self, percept):
        """
        Given a percept, update world model and stabilize memories.
        """
        super().learn(percept)
        self.stablize_memories()

    def fade(self):
        """
        Apply exponential decay toward uniform distribution for each memory trace, using memory-specific fading rates.
        """
        for i in range(len(self.category_sizes)):
            faded_memories = _decay_toward_uniform(
                self.sensory_predictions[:, self.category_indexer == i],
                self.memory_fade[:, i]
            )
            self.sensory_predictions[:, self.category_indexer == i] = faded_memories

    def stablize_memories(self):
        """
        Reduce fading rate of reactivated memories according to the weight of reactivation (posterior belief).
        """
        for i in range(self.effective_capacity):
            self.memory_fade[i, :] = self.memory_fade[i, :] * (1 - (self.belief_posterior[i] * self.reuse_factor))

    def encode_memory(self, percept=None):
        """
        Encode the current percept into memory, updating sensory and transition predictions.
        Sets the initial fading rates of each category in the new memory.
        """
        super().encode_memory(percept)
        if not self.effective_capacity == self.memory_capacity or not self.capacity_overflow_method == "stop encoding":
            self.memory_fade[self.timer, :] = self.get_fading_rates()

    def capacity_reached(self):
        """
        Handle encoding of last memory hypothesis when memory capacity is reached.
        """
        super().capacity_reached()
        self.memory_fade[self.timer, :] = self.get_fading_rates()

    def get_fading_rates(self):
        """
        Compute the fading rates for the current memory trace using the selected fading_rate_method.
        """
        if self.fading_rate_method == "sigmoid":
            fading_rates = sigmoid_fading_rate(
                gamma=self.fading_rate,
                sigma=self.surprise_factor,
                sensor_state_probabilities=self.sensory_expectation[self.get_one_hot_percept()],
                log_base=self.log_base
            )
        elif self.fading_rate_method == "surprise_advantage":
            fading_rates = surprise_advantage_fading_rate(
                gamma=self.fading_rate,
                sigma=self.surprise_factor,
                category_indexes=self.category_indexer,
                categorical_predictions=self.sensory_expectation,
                categorical_suprises=self.get_surprise(),
                log_base=self.log_base
            )
        else:
            raise ValueError(f'{self.fading_rate_method} is not a valid fading_rate_method')
        return fading_rates

