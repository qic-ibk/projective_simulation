{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be21eb4-4a89-4509-971e-c0803456b38d",
   "metadata": {},
   "source": [
    "# What is SiPS?\n",
    "\n",
    "SiPS, short for Situated Projective Simulation, is a model of decision making that integrates insights of Bayesian Filtering, Active Inference, and Projective Simulation to understand how cognitive systems learn to navigate in initially unknown environments. We use the word \"Situated\" to describe this agent in reference to the Ecological Pyschology of James Gibson. Accordingly, an assumption of our model is that it is fundamentally impossible for an embodied agent to possess a complete representation of the world in which it navigates. The information a embodied agent receives is a direct function of its position in the world, and necessarily incomplete. We call SiPS a model of an \"Agent\" because the information it recieves may have an influence on its actions, which in turn have differential effects on the agent's position in the world. Though a embodied agent may be able to act in optimal-enough ways using only immediate sensory information and an intrinsic set of priors about the world (given, for example, by a programmer or inherited via a system of descent with modification under selective pressure), a key question relevant to both the study of animal behavior and machine learning is how embodied agents acquire and store information such that it can discover and exploit new relations in the world, and thus become able to represent its \"situation\" in space and time.\n",
    "\n",
    "Bayesian filtering is a normative method for updating one's beliefs about the state of the world (or one's situation within it) given some degree of uncertainty and new, partial evidence regarding the state in question. Active Inference is a normative method for choosing actions that minimimize one's uncertainty by maximizing the expected evidence, given an agent's uncertainty about its state and the outcomes of its actions. Both of these approaches, however, depend on the agent \"knowing what it doesn't know\". The agent must know, for example, the probability of error in its sensory information and whether its current sensory information could be evidence of multiple different situated states. How can agents construct an error model of their world that supports Bayesing filtering and Active Inference if they do not come pre-equipped with such \"inductive biases\"? \n",
    "\n",
    "Most methods to-date use offline learning methods such as gradient descent to update the agent's \"world model\" such that it becomes better able to predict the sequence of sensory information and actions states acquired over the last \"online\" period. These approaches are effective, but they fail to account for the moments of \"online\" insight known to any reader of this document and observed in many animals.\n",
    "\n",
    "Projective Simulation is a model of decision making in which an agent selects its action by replaying \"clips\" of past situational sequences. The probability of the agent repeating the action taken in that clip is a function of the clip's \"emotional valence\". If the agent doesn't take the action, it replays another clip. An essential feature of Projective Simulation is that the selection of replayed clips is stochastic, modeled as a random walk on a graph where the nodes represent replayable clips. This stochasticity has two important outcomes. First, it allows the agent to \"assign credit\" to the deliberative pathway that produced an action and reinforce the traversed edges as a function of outcomes. Thus, Projective Simulation is a reinforcement learning model in which associative connections in a world model may be reinforced instead of, or in addition to, direct stimulus-response connections. And, therefore, learning from one situation may be generalized to another. The second important outcome of the stochastic deliberative process is that it may generate novel combinations of clips, which may themselves become represented as \"imagined\" or \"hypothetical\" nodes in the aforementioned graph. Based on these features, this graph - which provides a Projective Simulation agent with a world model - is called the Episodic and Compositional Memory (ECM).\n",
    "\n",
    "The processes of Bayesian Filtering and Active Inferance may also be represented using a graph structure. SiPS uses a single graph structure that can accomodate all three processes and has a parameter which we call \"focus\" that scales processes on this graph from operating purely as a system of Active Inference with Bayesian filtering to a system of Projective Simulation. In this work, we explore when, why, and how an agent might switch between these two modes, or adopt an intermediate \"focus\" state, to achieve insightful behavior in novel environments without relying on offline and computationally intensive learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e04e41-3ef4-4a23-9481-f7828902ab8b",
   "metadata": {},
   "source": [
    "# How does SiPS work?\n",
    "\n",
    "## Overview\n",
    "\n",
    "Following from the principles of Active Inference, the ECM of SiPS is organized hierarchically such that information passed from higher levels to lower levels represent predictions, and information passed from lower levels to higher levels represent prediction errors. We refer to nodes in the lowest level of the ECM as \"perceptual representations\", which are further divided into \"sensory representations\" and \"action representations\". Higher levels of the ECM interact with the ECM's environment only through the perceptual representations, i.e. the perceptual representations form a Markov Blanket around the rest of the ECM (the ECM's enviroment may be equivalent to the agent's environment, or it may include other elements of the agent's 'body'). Higher level nodes in the ECM represent \"memory traces\" of past situational states; at any given time, the agent's perceptual state is encoded to a single trace by establishing connections between that trace and excited perceptual representations. Over time, the strength of these connections may change as functions of forgetting and reinforcement, and new connections may be formed by the deliberative process. A key goal of the SiPS model is to understand how these three processes can be configured such that connections to a memory trace either decay to obselesence or crytalize into a representation of a learned relation in the world.\n",
    "\n",
    "Figure 1 provides an overview of the SiPS ECM.\n",
    "\n",
    "<figure>\n",
    "<img src=\"..\\figs\\SiPS_structure.png\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"> Figure 1: SiPS Structure Overview. SiPS combines a reflex system with episodic memory structure. The reflex system is defined as a matrix, <b>W</b>, with edges that connect each unique observation to each action in the agent's repertoire. The episodic memory structure can be represented as a graph with three type of nodes: sensory representations, action representations, and memory traces. Sensory representations and action representations are jointly refered to as percept nodes. Percept nodes are connected to memory traces by bidirectional edges called Hebbian weights, given by a matrix <b>H</b>. Memory Traces have recurrent edges with weights given by the matrix <b>M</b>. </figcaption>\n",
    "</figure>\n",
    "\n",
    "## States and Variables\n",
    "\n",
    "Each node in a SiPS graph is associated with multiple states. To denote states associated with sensory representations we will use the letter 's'; to denote states associted with action representations we will use the letter 'a'; to denote the states of percept nodes more generally we will use the letter 'p' (observation); and to denote states associted with memory traces we will use the letter 'b' (beliefs).\n",
    "\n",
    "There are three types of state associated with each node, which we distinguish notationally by using diacritical marks. Here, we describe each type of state breifly. Subsequent sections will explain and demonstrate the dynmaics of these states and their interactions in the ECM in much greater detail.\n",
    "\n",
    "The <b>excitation state</b> of a node reflects the transmission of new sensory information through the graph, and is denoted by a plain letter with no diacritical mark. For example, an excitation state of all sensory representations is denoted $\\boldsymbol{s}$ and an excitation state of a single sensory representation by $s_\\mathrm{j}$. $\\boldsymbol{S}$ denotes the J dimensional random variable associated with sensory representation states, where J is the number of sensory representation nodes. Subscripts of this variable, $S_\\mathrm{j}$, indicate random variables associated with the excitation state of a specific sensory representation.\n",
    "\n",
    "The <b>expectation state</b> of a node reflects the ECM's prediction regarding its next situated state and is denoted using the diacritical ^. For example, $\\hat{b}_\\mathrm{n}$ denotes the expectation state of memory trace n, which reflects the strength of the agent's belief that its next situated state will be well represented by the situated state encoded by memory trace n.\n",
    "\n",
    "The <b>attention</b> state of a node reflects how strongly that node is currently being considered by the agent's deliberative process. Importantly, while all other random variables in a SiPS ECM evolve on a discrete timescale $t$, the attention state evolves at a smaller timescale $\\tau$. The evolution of the attention states across the ECM over an interval of $t$ defines the stochastic process by which the ECM computes the expectation states of memory traces, i.e. predictions about its next situated state.\n",
    "\n",
    "Memory Traces have a fourth associated state: their <b>valence state</b>. The valence of a memory trace reflects how suprised the ECM was by the excitation state of sensory represenetations following the situated state encoded by that trace. The valence of a memory trace determines how much an action by the agent is inhibited or primed by an increase of attention on the given memory trace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d54bcd-4ab3-4e7b-9ad4-13638a709018",
   "metadata": {},
   "source": [
    "## Creating a SiPS agent\n",
    "\n",
    "Let's begin an exploration of how SiPS agents work by using the \"Projective Simulation\" package to create a simple agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d443d12-e182-4e6b-8e38-3f0553b6905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projective_simulation.agents import Situated_Agent\n",
    "import nbdev\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c00a3f-8490-408d-8fbc-ad666eeccd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<projective_simulation.agents.Situated_Agent at 0x1fa8f468860>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_SiPS = Situated_Agent(num_actions = 2, memory_capacity = 100)\n",
    "example_SiPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb58d0-425e-4ec9-a97c-b72403ecff69",
   "metadata": {},
   "source": [
    "There are many parameters that can be used to customize a SiPS agent, but at a minimum the number of actions and memory traces available to the agent must be defined. For now, we will rely on default values for all other parameters. As with all projective simulation agents, a SiPS agent can return an action if given an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99302b1c-cde6-4da1-b561-9c42d7a78c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "test_observations = [1,0,5,1,1]\n",
    "test_actions = [None] * len(test_observations)\n",
    "for t in range(len(test_observations)):\n",
    "    test_actions[t] = example_SiPS.get_action(test_observations[t])\n",
    "print(test_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf790bba-5c67-4c05-bc17-4f8e6d9e491e",
   "metadata": {},
   "source": [
    "The get_action function of SiPS agent has three steps: triggering a reflex, processing the percept, and setting expectations. Each of these steps is handled by a different object in the Situated_Agent class: the reflex_ECM, preprocessor, and episodic_ECM, respectively.\n",
    "\n",
    "Let's cover the reflexes firsts\n",
    "\n",
    "## Reflexes\n",
    "\n",
    "The first thing a SiPS agent does when it recieves a new observation is it selects an action using its reflexes. Note that while we use an independant ECM class type (Priming_ECM) to define the SiPS reflexes, conceptually these reflexes are a *component* of a SiPS ECM. The Priming_ECM is a subclass of a Two_Layer ECM, and we can take a look at the documnetation of these classes to see how they work. We refer the reader to (LINK TO BASIC PS TUTORIAL) for discussion of the role glow and damp parameters in reinforcement learning on a two-layer PS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02cd01e9-8ff8-4ac8-8818-c755e95bb52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/{user}/projective_simulation/blob/master/projective_simulation/ECMs.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Two_Layer\n",
       "\n",
       ">      Two_Layer (num_actions:int, glow:float, damp:float, softmax:float)\n",
       "\n",
       "*A minimal ECM, every agent should be Derived from this class. Primarily serves to enforce that all ECMs have the \"ECM\" class\n",
       "\n",
       "Examples:\n",
       ">>> pass*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| num_actions | int | The number of available actions. |\n",
       "| glow | float | The glow (or eta) parameter. |\n",
       "| damp | float | The damping (or gamma) parameter. |\n",
       "| softmax | float | The softmax (or beta) parameter. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/{user}/projective_simulation/blob/master/projective_simulation/ECMs.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Two_Layer\n",
       "\n",
       ">      Two_Layer (num_actions:int, glow:float, damp:float, softmax:float)\n",
       "\n",
       "*A minimal ECM, every agent should be Derived from this class. Primarily serves to enforce that all ECMs have the \"ECM\" class\n",
       "\n",
       "Examples:\n",
       ">>> pass*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| num_actions | int | The number of available actions. |\n",
       "| glow | float | The glow (or eta) parameter. |\n",
       "| damp | float | The damping (or gamma) parameter. |\n",
       "| softmax | float | The softmax (or beta) parameter. |"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from projective_simulation.ECMs import Two_Layer, Priming_ECM\n",
    "nbdev.show_doc(Two_Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e37ad49b-889e-486c-8171-015879280e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/{user}/projective_simulation/blob/master/projective_simulation/ECMs.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Priming_ECM\n",
       "\n",
       ">      Priming_ECM (num_actions:int, glow:float=0.1, damp:float=0.01,\n",
       ">                   softmax:float=0.5, action_primes:list=None)\n",
       "\n",
       "*This sub-class of the Two-Layer ECM adds a variable for action priming.\n",
       "This variable should be a list of floats, each element of which corresponds to an action in the ECM.\n",
       "These \"priming values\" are summed with h-values of any edge connected to the associated action node prior to calculating walk probabilites with the softmax function*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| num_actions | int |  | The number of available actions. |\n",
       "| glow | float | 0.1 | The glow (or eta) parameter. |\n",
       "| damp | float | 0.01 | The damping (or gamma) parameter. |\n",
       "| softmax | float | 0.5 | The softmax (or beta) parameter. |\n",
       "| action_primes | list | None | weights on the probability that deliberation steps into each action. Defaults to 0 for each action |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/{user}/projective_simulation/blob/master/projective_simulation/ECMs.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Priming_ECM\n",
       "\n",
       ">      Priming_ECM (num_actions:int, glow:float=0.1, damp:float=0.01,\n",
       ">                   softmax:float=0.5, action_primes:list=None)\n",
       "\n",
       "*This sub-class of the Two-Layer ECM adds a variable for action priming.\n",
       "This variable should be a list of floats, each element of which corresponds to an action in the ECM.\n",
       "These \"priming values\" are summed with h-values of any edge connected to the associated action node prior to calculating walk probabilites with the softmax function*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| num_actions | int |  | The number of available actions. |\n",
       "| glow | float | 0.1 | The glow (or eta) parameter. |\n",
       "| damp | float | 0.01 | The damping (or gamma) parameter. |\n",
       "| softmax | float | 0.5 | The softmax (or beta) parameter. |\n",
       "| action_primes | list | None | weights on the probability that deliberation steps into each action. Defaults to 0 for each action |"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(Priming_ECM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338c2a9-48d4-4c28-b459-d5a273b0575e",
   "metadata": {},
   "source": [
    "The addition of action priming is important for SiPS, and is given by the expectation state of action representations. A positive action expectation increases the probability that the priming ECM returns the corresponding action, regardless of the input percept. Likewise, a negative value reduces the probability of returning that action. Take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d9c34f5-0034-449f-87b0-a42db6c06fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "reflexes = Priming_ECM(num_actions = 2, action_primes = [0,2])\n",
    "example_SiPS = Situated_Agent(reflex_ECM = reflexes, memory_capacity = 100)\n",
    "# Notice that we define a reflex ECM for the Situated Agent instead of num_actions.\n",
    "# If a reflex ECM is predefined, the SiPS agent uses that instead of creating a reflex ECM using the num_actions, glow, damp, and reflex_softmax variables\n",
    "\n",
    "test_observations = [0] * 10 + [1] * 10\n",
    "test_actions = [None] * len(test_observations)\n",
    "for t in range(len(test_observations)):\n",
    "    test_actions[t] = example_SiPS.get_action(test_observations[t])\n",
    "print(test_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9141e-af9d-4e86-a789-9c06fbfc3823",
   "metadata": {},
   "source": [
    "Specifically, the probability that the reflex ECM returns action $k$ given a particular observation $o^{(i)}$ is given by the equation\n",
    "\n",
    "\\begin{equation}\n",
    "Pr(\\boldsymbol{A} = \\boldsymbol{a}^{(\\mathrm{k})}|\\boldsymbol{O} = \\boldsymbol{o}^{(\\mathrm{i})}) = \\frac{\\mathrm{e}^{\\beta_\\mathrm{reflex}(W_\\mathrm{ik} + \\hat{A}_\\mathrm{k})}}{\\sum_\\mathrm{k'=1}^\\mathrm{K}{\\mathrm{e}^{\\beta_\\mathrm{reflex}(W_\\mathrm{ik'} + \\hat{A}_\\mathrm{k'})}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7d6ba-6c4e-41df-8895-5de656091aba",
   "metadata": {},
   "source": [
    "Where the Action Space of the agent, $\\mathcal{A}^\\mathrm{K}$, is defined such that for a given number of actions K, a particular action state $\\boldsymbol{a}^{(k)}$ is equivalent to the one-hot excitation of action representations where $a_\\mathrm{k} = 1$; $W_{ik}$ is the reflex weight from observation state $\\boldsymbol{o}^{(\\mathrm{i})}$ to action state $\\boldsymbol{a}^{(\\mathrm{k})}$; and where $\\beta_\\mathrm{reflex}$ is the reflex_softmax paramter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f11e9f6-cd4c-4398-b4db-a0428ec3a6ce",
   "metadata": {},
   "source": [
    "## The Episodic Graph\n",
    "\n",
    "The Priming_ECM does not change the priming of actions on its own - expectation values must come from another system such as an episodic graph. We can create an episodic graph like so\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e4b0a59-0905-463b-b7e3-83620885ba8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<projective_simulation.ECMs.Episodic_Memory at 0x1fa8f46a480>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from projective_simulation.ECMs import Episodic_Memory\n",
    "em_example = Episodic_Memory(num_actions = 2, capacity = 50)\n",
    "em_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecaba85-3209-4762-b2a2-8278dc48f9a8",
   "metadata": {},
   "source": [
    "In SiPS, the episodic graph has three node types, action reprensentations, sensory representations, and memory traces. Sensory representations and action representations (together called percept nodes) are connected to memory traces by edge weights given by the H-matrix. Because the newly intitialized episodic graph has not recieved any observations, it has no sensory representations. It is required, however, to predefine an action space for and episodice graph, and thus the H-matrix is current KxN (num_actions x capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74b492a-0734-4fff-b2d8-85939b942a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(em_example.hmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f97f41-d19a-43c7-a522-a355de1ee861",
   "metadata": {},
   "source": [
    "### Representing the Percept\n",
    "\n",
    "One goal of a SiPS agent is to learn about relations between individual elements or combinations of elements in observations. Thus, the episodic graph creates a sensory representation for each new value observed in *each dimension* of an observation vector. The SiPS agent has a \"Percept Processor\" that keeps track of which value and dimension of an observation each sensory representation is connected to. It can thus convert any observation to a sensory excitation state. The percept of a epsidic graph is defined as the concatenation of the sensory representation exitation state and the action representation excitation state. Let's see what happens when we give the preprocessor of a new SiPS agent a two dimensional observation, and an action triggered by that observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "868442d6-d9a9-4fe6-8366-ec1e5161b567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actions': {'0': 0, '1': 1}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_SiPS = Situated_Agent(num_actions = 2, memory_capacity = 10)\n",
    "example_SiPS.percept_processor.percept_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517612f3-5f84-45e2-a9aa-2f0126350426",
   "metadata": {},
   "source": [
    "Initially, the agent percept deictionary only maps actions '0' and '1' to action representations 0 and 1, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cdc3f81-e629-40d5-ab4c-be28f756403e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_SiPS = Situated_Agent(num_actions = 2, memory_capacity = 10)\n",
    "observation = np.array([0,4])\n",
    "action = 1\n",
    "example_SiPS.percept_processor.get_percept(observation, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0cd618-0cd4-4264-9ec5-d9e89e067055",
   "metadata": {},
   "source": [
    "But after receiving a two-dimensional observation, the percept that the percept processor returns gives the excitation states of four percept nodes. We can see that the two additional nodes correspond to two sensory representations that have been added to the percept dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "139fab46-fec1-4503-98ff-0d90820ab4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actions': {'0': 0, '1': 1}, '0': {'0': 2}, '1': {'4': 3}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_SiPS.percept_processor.percept_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766cc87b-1806-4dae-a388-9c9b34fa2f2d",
   "metadata": {},
   "source": [
    "Note the nested structure of this dictionary. Each value of the percept dictionary is also a dictionary, which keys the sensory representations for a given dimension of the observation (or the action).\n",
    "\n",
    "If the dimensionality of the agent's observations, the percept processor handles this flexibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "849b57c3-f298-4c60-bb11-0dd6d7a47236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_SiPS.percept_processor.get_percept(observation = np.array([1,4,0]), action = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5db3b337-c2b8-4f58-8c15-601280612306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'actions': {'0': 0, '1': 1},\n",
       " '0': {'0': 2, '1': 4},\n",
       " '1': {'4': 3},\n",
       " '2': {'0': 5}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_SiPS.percept_processor.percept_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0accc2c-3aaf-4ff4-94d4-f60718244184",
   "metadata": {},
   "source": [
    "### Deliberation\n",
    "\n",
    "Once a SiPS agent has chosen and action and reprsented its percept, it begins to deliberate. This is the process by which the agent assess its situated state and predicts what will happen next. In the process, it encodes relevant information about its current state to memory and potentially reorganizes existing connection in its episodice memory. We will go through the steps of deliberation one-by-one.\n",
    "\n",
    "#### Adding new sensory representations\n",
    "\n",
    "If the percept passed to the episodic graph by the percept processor is larger than the current number of percept nodes, new nodes are added by adding elements to all of the relevent state variables. See help(Episodic_Memory.add_percept) for details.\n",
    "\n",
    "#### Calculating Suprise\n",
    "\n",
    "Surprise is primary mechanism by which SiPS learns. It is used both the ascribe an emotional valence to memory traces and to reinforce or punish the weights of edges (more on this later). The surprise of each sensory representation is computed by treating its excitation as a sample from a binomial distribution with probability equal to that representations expectation state, i.e.\n",
    "\n",
    "$$\n",
    "I(s_\\mathrm{j}(t)) = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "  -\\mathrm{log}(\\hat{s}_\\mathrm{j}(t-1)) & \\text{if } s_\\mathrm{j}(t) = 1  \\\\\n",
    "  -\\mathrm{log}(1-\\hat{s}_\\mathrm{j}(t-1) & \\text{if } s_\\mathrm{j}(t) = 0,\n",
    "\\end{array}\n",
    "\\right.\n",
    "$$\n",
    "\n",
    "where $s_\\mathrm{j}(t)$ is the current excitation state of sensory representation $\\mathrm{j}$ and $\\hat{s}_\\mathrm{j}(t-1)$ is the expectation state of sensory representation $\\mathrm{j}$ set in the last time step. The agent's total surprise is simply the sum of surprises across all sensory representations.\n",
    "\n",
    "#### Exciting Memory Traces\n",
    "\n",
    "The next step of delibration is to pass excitation information from percept nodes to memory traces. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d288bb0-cc0b-4f6f-8317-1397e4d4b3db",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bb2a6b1-b62a-43bb-bb69-c9e94778f605",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
