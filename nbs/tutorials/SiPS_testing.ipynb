{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be21eb4-4a89-4509-971e-c0803456b38d",
   "metadata": {},
   "source": [
    "# What is SiPS?\n",
    "\n",
    "SiPS, short for Situated Projective Simulation, is a model of decision making that integrates insights of Bayesian Filtering, Active Inference, and Projective Simulation to understand how cognitive systems learn to navigate in initially unknown environments. We use the word \"Situated\" to describe this agent in reference to the Ecological Pyschology of James Gibson. Accordingly, an assumption of our model is that it is fundamentally impossible for an embodied agent to possess a complete representation of the world in which it navigates. The information a embodied agent receives is a direct function of its position in the world, and necessarily incomplete. We call SiPS a model of an \"Agent\" because the information it recieves may have an influence on its actions, which in turn have differential effects on the agent's position in the world. Though a embodied agent may be able to act in optimal-enough ways using only immediate sensory information and an intrinsic set of priors about the world (given, for example, by a programmer or inherited via a system of descent with modification under selective pressure), a key question relevant to both the study of animal behavior and machine learning is how embodied agents acquire and store information such that it can discover and exploit new relations in the world, and thus become able to represent its \"situation\" in space and time.\n",
    "\n",
    "Bayesian filtering is a normative method for updating one's beliefs about the state of the world (or one's situation within it) given some degree of uncertainty and new, partial evidence regarding the state in question. Active Inference is a normative method for choosing actions that minimimize one's uncertainty by maximizing the expected evidence, given an agent's uncertainty about its state and the outcomes of its actions. Both of these approaches, however, depend on the agent \"knowing what it doesn't know\". The agent must know, for example, the probability of error in its sensory information and whether its current sensory information could be evidence of multiple different situated states. How can agents construct an error model of their world that supports Bayesing filtering and Active Inference if they do not come pre-equipped with such \"inductive biases\"? \n",
    "\n",
    "Most methods to-date use offline learning methods such as gradient descent to update the agent's \"world model\" such that it becomes better able to predict the sequence of sensory information and actions states acquired over the last \"online\" period. These approaches are effective, but they fail to account for the moments of \"online\" insight known to any reader of this document and observed in many animals.\n",
    "\n",
    "Projective Simulation is a model of decision making in which an agent selects its action by replaying \"clips\" of past situational sequences. The probability of the agent repeating the action taken in that clip is a function of the clip's \"emotional valence\". If the agent doesn't take the action, it replays another clip. An essential feature of Projective Simulation is that the selection of replayed clips is stochastic, modeled as a random walk on a graph where the nodes represent replayable clips. This stochasticity has two important outcomes. First, it allows the agent to \"assign credit\" to the deliberative pathway that produced an action and reinforce the traversed edges as a function of outcomes. Thus, Projective Simulation is a reinforcement learning model in which associative connections in a world model may be reinforced instead of, or in addition to, direct stimulus-response connections. And, therefore, learning from one situation may be generalized to another. The second important outcome of the stochastic deliberative process is that it may generate novel combinations of clips, which may themselves become represented as \"imagined\" or \"hypothetical\" nodes in the aforementioned graph. Based on these features, this graph - which provides a Projective Simulation agent with a world model - is called the Episodic and Compositional Memory (ECM).\n",
    "\n",
    "The processes of Bayesian Filtering and Active Inferance may also be represented using a graph structure. SiPS uses a single graph structure that can accomodate all three processes and has a parameter which we call \"focus\" that scales processes on this graph from operating purely as a system of Active Inference with Bayesian filtering to a system of Projective Simulation. In this work, we explore when, why, and how an agent might switch between these two modes, or adopt an intermediate \"focus\" state, to achieve insightful behavior in novel environments without relying on offline and computationally intensive learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e04e41-3ef4-4a23-9481-f7828902ab8b",
   "metadata": {},
   "source": [
    "# How does SiPS work?\n",
    "\n",
    "## Overview\n",
    "\n",
    "Following from the principles of Active Inference, the ECM of SiPS is organized hierarchically such that information passed from higher levels to lower levels represent predictions, and information passed from lower levels to higher levels represent prediction errors. We refer to nodes in the lowest level of the ECM as \"perceptual representations\", which are further divided into \"sensory representations\" and \"action representations\". Higher levels of the ECM interact with the ECM's environment only through the perceptual representations, i.e. the perceptual representations form a Markov Blanket around the rest of the ECM (the ECM's enviroment may be equivalent to the agent's environment, or it may include other elements of the agent's 'body'). Higher level nodes in the ECM represent \"memory traces\" of past situational states; at any given time, the agent's perceptual state is encoded to a single trace by establishing connections between that trace and excited perceptual representations. Over time, the strength of these connections may change as functions of forgetting and reinforcement, and new connections may be formed by the deliberative process. A key goal of the SiPS model is to understand how these three processes can be configured such that connections to a memory trace either decay to obselesence or crytalize into a representation of a learned relation in the world.\n",
    "\n",
    "Figure 1 provides an overview of the SiPS ECM. \\[NOTE: THIS IS NOT FULLY ACCURATE. Reflex weights connect Percept States (e.g. $\\boldsymbol{p}^{(i)}$) to action representation nodes. Need to find a nice way to represent the stuctural difference between percepts in the refelx ECM and percepts in the episodic ECM)]\n",
    "\n",
    "<figure>\n",
    "<img src=\"..\\figs\\SiPS_structure.png\" style=\"width:100%\">\n",
    "<figcaption align = \"center\"> Figure 1: SiPS Structure Overview. SiPS uses a graph structure composed of nodes and edges. There are three type of nodes: sensory representations, action representations, and memory traces. Sensory representations and action representations are jointly refered to as percept nodes. Sensory representations are connected to action represenetations by unidirectional edges called reflex weights, given by a matrix <b>W</b>. Precept nodes are connected to memory traces by bidirectional edges called Hebbian weights, given by a matrix <b>H</b>. Memory Traces have recurrent edges given by the matrix <b>M</b>. </figcaption>\n",
    "</figure>\n",
    "\n",
    "## States and Variables\n",
    "\n",
    "Each node in a SiPS graph is associated with multiple states. To denote states associated with sensory representations we will use the letter 's'; to denote states associted with action representations we will use the letter 'a'; to denote the states of percept nodes more generally we will use the letter 'p' (observation); and to denote states associted with memory traces we will use the letter 'b' (beliefs).\n",
    "\n",
    "There are three types of state associated with each node, which we distinguish notationally by using diacritical marks. Here, we describe each type of state breifly. Subsequent sections will explain and demonstrate the dynmaics of these states and their interactions in the ECM in much greater detail.\n",
    "\n",
    "The <b>excitation state</b> of a node reflects the transmission of new sensory information through the graph, and is denoted by a plain letter with no diacritical mark. For example, an excitation state of all sensory representations is denoted $\\boldsymbol{s}$ and an excitation state of a single sensory representation by $s_\\mathrm{j}$. $\\boldsymbol{S}$ denotes the J dimensional random variable associated with sensory representation states, where J is the number of sensory representation nodes. Subscripts of this variable, $S_\\mathrm{j}$, indicate random variables associated with the excitation state of a specific sensory representation.\n",
    "\n",
    "The <b>expectation state</b> of a node reflects the ECM's prediction regarding its next situated state and is denoted using the diacritical ^. For example, $\\hat{b}_\\mathrm{n}$ denotes the expectation state of memory trace n, which reflects the strength of the agent's belief that its next situated state will be well represented by the situated state encoded by memory trace n.\n",
    "\n",
    "The <b>attention</b> state of a node reflects how strongly that node is currently being considered by the agent's deliberative process. Importantly, while all other random variables in a SiPS ECM evolve on a discrete timescale $t$, the attention state evolves at a smaller timescale $\\tau$. The evolution of the attention states across the ECM over an interval of $t$ defines the stochastic process by which the ECM computes the expectation states of memory traces, i.e. predictions about its next situated state.\n",
    "\n",
    "Memory Traces have a fourth associated state: their <b>valence state</b>. The valence of a memory trace reflects how suprised the ECM was by the excitation state of sensory represenetations following the situated state encoded by that trace. The valence of a memory trace determines how much an action by the agent is inhibited or primed by an increase of attention on the given memory trace."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d54bcd-4ab3-4e7b-9ad4-13638a709018",
   "metadata": {},
   "source": [
    "## Creating a SiPS agent\n",
    "\n",
    "Let's begin an exploration of how SiPS agents work by using the \"Projective Simulation\" package to create a simple agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d443d12-e182-4e6b-8e38-3f0553b6905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projective_simulation.agents import Situated_Agent\n",
    "import nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c00a3f-8490-408d-8fbc-ad666eeccd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<projective_simulation.agents.Situated_Agent>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_SiPS = Situated_Agent(num_actions = 2, memory_capacity = 100)\n",
    "example_SiPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb58d0-425e-4ec9-a97c-b72403ecff69",
   "metadata": {},
   "source": [
    "There are many parameters that can be used to customize a SiPS agent, but at a minimum the number of actions and memory traces available to the agent must be defined. For now, we will rely on default values for all other parameters. As with all projective simulation agents, a SiPS agent can return an action if given an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99302b1c-cde6-4da1-b561-9c42d7a78c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "test_observations = [1,0,5,1,1]\n",
    "test_actions = [None] * len(test_observations)\n",
    "for t in range(len(test_observations)):\n",
    "    test_actions[t] = example_SiPS.get_action(test_observations[t])\n",
    "print(test_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf790bba-5c67-4c05-bc17-4f8e6d9e491e",
   "metadata": {},
   "source": [
    "The get_action function of SiPS agent has three steps: triggering a reflex, processing the percept, and setting expectations. Each of these steps is handled by a different object in the Situated_Agent class: the reflex_ECM, preprocessor, and episodic_ECM, respectively.\n",
    "\n",
    "Let's cover the reflexes firsts\n",
    "\n",
    "## Reflexes\n",
    "\n",
    "The first thing a SiPS agent does when it recieves a new observation is it selects an action using its reflexes. Note that while we use an independant ECM class type (Priming_ECM) to define the SiPS reflexes, conceptually these reflexes are only a component of a SiPS ECM. The Priming_ECM is a subclass of a Two_Layer ECM (LINK TO BASIC PS TUTORIAL), and we can take a look at the documnetation of these classes to see how they work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd01e9-8ff8-4ac8-8818-c755e95bb52e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/{user}/projective_simulation/blob/master/projective_simulation/ECMs.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Two_Layer\n",
       "\n",
       ">      Two_Layer (num_actions:int, glow:float, damp:float, softmax:float)\n",
       "\n",
       "*A minimal ECM, every agent should be Derived from this class. Primarily serves to enforce that all ECMs have the \"ECM\" class\n",
       "\n",
       "Examples:\n",
       ">>> pass*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| num_actions | int | The number of available actions. |\n",
       "| glow | float | The glow (or eta) parameter. |\n",
       "| damp | float | The damping (or gamma) parameter. |\n",
       "| softmax | float | The softmax (or beta) parameter. |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/{user}/projective_simulation/blob/master/projective_simulation/ECMs.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Two_Layer\n",
       "\n",
       ">      Two_Layer (num_actions:int, glow:float, damp:float, softmax:float)\n",
       "\n",
       "*A minimal ECM, every agent should be Derived from this class. Primarily serves to enforce that all ECMs have the \"ECM\" class\n",
       "\n",
       "Examples:\n",
       ">>> pass*\n",
       "\n",
       "|    | **Type** | **Details** |\n",
       "| -- | -------- | ----------- |\n",
       "| num_actions | int | The number of available actions. |\n",
       "| glow | float | The glow (or eta) parameter. |\n",
       "| damp | float | The damping (or gamma) parameter. |\n",
       "| softmax | float | The softmax (or beta) parameter. |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from projective_simulation.ECMs import Two_Layer, Priming_ECM\n",
    "nbdev.show_doc(Two_Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ad49b-889e-486c-8171-015879280e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/{user}/projective_simulation/blob/master/projective_simulation/ECMs.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Priming_ECM\n",
       "\n",
       ">      Priming_ECM (num_actions:int, glow:float=0.1, damp:float=0.01,\n",
       ">                   softmax:float=0.5, action_primes:list=None)\n",
       "\n",
       "*This sub-class of the Two-Layer ECM adds a variable for action priming.\n",
       "This variable should be a list of floats, each element of which corresponds to an action in the ECM.\n",
       "These \"priming values\" are summed with h-values of any edge connected to the associated action node prior to calculating walk probabilites with the softmax function*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| num_actions | int |  | The number of available actions. |\n",
       "| glow | float | 0.1 | The glow (or eta) parameter. |\n",
       "| damp | float | 0.01 | The damping (or gamma) parameter. |\n",
       "| softmax | float | 0.5 | The softmax (or beta) parameter. |\n",
       "| action_primes | list | None | weights on the probability that deliberation steps into each action. Defaults to 0 for each action |"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/{user}/projective_simulation/blob/master/projective_simulation/ECMs.py#LNone){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Priming_ECM\n",
       "\n",
       ">      Priming_ECM (num_actions:int, glow:float=0.1, damp:float=0.01,\n",
       ">                   softmax:float=0.5, action_primes:list=None)\n",
       "\n",
       "*This sub-class of the Two-Layer ECM adds a variable for action priming.\n",
       "This variable should be a list of floats, each element of which corresponds to an action in the ECM.\n",
       "These \"priming values\" are summed with h-values of any edge connected to the associated action node prior to calculating walk probabilites with the softmax function*\n",
       "\n",
       "|    | **Type** | **Default** | **Details** |\n",
       "| -- | -------- | ----------- | ----------- |\n",
       "| num_actions | int |  | The number of available actions. |\n",
       "| glow | float | 0.1 | The glow (or eta) parameter. |\n",
       "| damp | float | 0.01 | The damping (or gamma) parameter. |\n",
       "| softmax | float | 0.5 | The softmax (or beta) parameter. |\n",
       "| action_primes | list | None | weights on the probability that deliberation steps into each action. Defaults to 0 for each action |"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nbdev.show_doc(Priming_ECM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338c2a9-48d4-4c28-b459-d5a273b0575e",
   "metadata": {},
   "source": [
    "The addition of action priming is important for SiPS. A positive value in this list increases the probability that the priming ECM returns the corresponding action, regardless of the input percept. Likewise, a negative value reduces the probability of returning that action. Take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c34f5-0034-449f-87b0-a42db6c06fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "reflexes = Priming_ECM(num_actions = 2, action_primes = [0,2])\n",
    "example_SiPS = Situated_Agent(reflex_ECM = reflexes, memory_capacity = 100)\n",
    "# Notice that we define a reflex ECM for the Situated Agent instead of num_actions.\n",
    "# If a reflex ECM is predefined, the SiPS agent uses that instead of creating a reflex ECM using the num_actions, glow, damp, and reflex_softmax variables\n",
    "\n",
    "test_observations = [0] * 10 + [1] * 10\n",
    "test_actions = [None] * len(test_observations)\n",
    "for t in range(len(test_observations)):\n",
    "    test_actions[t] = example_SiPS.get_action(test_observations[t])\n",
    "print(test_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9141e-af9d-4e86-a789-9c06fbfc3823",
   "metadata": {},
   "source": [
    "Specifically, the probability that the reflex ECM returns action $k$ given a particular observation $o^{(i)}$ is given by the equation\n",
    "\n",
    "\\begin{equation}\n",
    "Pr(\\boldsymbol{A} = \\boldsymbol{a}^{(\\mathrm{k})}|O = o^{(\\mathrm{i})}) = \\frac{\\mathrm{e}^{\\beta_\\mathrm{reflex}(W_\\mathrm{ik} + \\hat{A}_\\mathrm{k})}}{\\sum_\\mathrm{k'=1}^\\mathrm{K}{\\mathrm{e}^{\\beta_\\mathrm{reflex}(W_\\mathrm{ik'} + \\hat{A}_\\mathrm{k'})}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7d6ba-6c4e-41df-8895-5de656091aba",
   "metadata": {},
   "source": [
    "Where the Action Space of the agent, $\\mathcal{A}^\\mathrm{K}$, is defined such that for a given number of actions K, a particular action state $\\boldsymbol{a}^{(k)}$ is equivalent to the one-hot excitation of action representations where $a_\\mathrm{k} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59f949c-cf3c-4936-b7b6-077431a79daf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
