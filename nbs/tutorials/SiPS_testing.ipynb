{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8be21eb4-4a89-4509-971e-c0803456b38d",
   "metadata": {},
   "source": [
    "# What is SiPS?\n",
    "\n",
    "SiPS, short for Situated Projective Simulation, is a model of decision making that integrates insights of Bayesian Filtering, Active Inference, and Projective Simulation to understand how cognitive systems learn to navigate in initially unknown environments. We use the word \"Situated\" to describe this agent in reference to the Ecological Pyschology of James Gibson. Accordingly, an assumption of our model is that it is fundamentally impossible for an embodied agent to possess a complete representation of the world in which it navigates. The information an embodied agent receives is a direct function of its position in the world, and necessarily incomplete. We call SiPS a model of an \"Agent\" because the information it recieves may have an influence on its actions, which in turn have differential effects on the agent's position in the world. Though a embodied agent may be able to act in optimal-enough ways using only immediate sensory information and an intrinsic set of priors about the world (given, for example, by a programmer or inherited via a system of decent with modification under selective pressure), a key question relevant to both the study of animal behavior and machine learning is how embodied agents acquire and store information such that it can discover and exploit new relations in the world, and thus become able to represent its \"situation\" in space and time.\n",
    "\n",
    "Bayesian filtering is a normative method for updating one's beliefs about the state of the world (or one's situation within it) given some degree of uncertainty and new, partial evidence regarding the state in question. Active Inference is a normative method for choosing actions that minimimize one's uncertainty by maximizing the expected evidence, given an agent's uncertainty about its state and the outcomes of its actions. Both of these approaches, however, depend on the agent \"knowing what it doesn't know\". The agent must know, for example, the probability of error in its sensory information and whether its current sensory information could be evidence of multiple different situated states. How can agents construct an error model of their world that supports Bayesing filtering and Active Inference if they do not come pre-equipped with such \"inductive biases\"? \n",
    "\n",
    "Most methods to-date use offline learning methods such as gradient descent to update the agent's \"world model\" such that it becomes better able to predict the sequence of sensory information and actions states acquired over the last \"online\" period. These approaches are effective, but they fail to account for the moments of \"online\" insight known to any reader of this document and observed in many animals.\n",
    "\n",
    "Projective Simulation is a model of decision making in which an agent selects its action by replaying \"clips\" of past situational sequences. The probability of the agent repeating the action taken in that clip is a function of the clip's \"emotional valence\". If the agent doesn't take the action, it replays another clip. An essential feature of Projective Simulation is that the selection of replayed clips is stochastic, modeled as a random walk on a graph where the nodes represent replayable clips. This stochasticity has two important outcomes. First, it allows the agent to \"assign credit\" to the deliberative pathway that produced an action and reinforce the traversed edges as a function of outcomes. Thus, Projective Simulation is a reinforcement learning model in which associative connections in a world model may be reinforced instead of, or in addition to, direct stimulus-response connections. And, therefore, learning from one situation may be generalized to another. The second important outcome of the stochastic deliberative process is that it may generate novel combinations of clips, which may themselves become represented as \"imagined\" or \"hypothetical\" nodes in the aforementioned graph. Based on these features, this graph - which provides a Projective Simulation agent with a world model - is called the Episodic and Compositional Memory (ECM).\n",
    "\n",
    "The processes of Bayesian Filtering and Active Inference may also be represented using a graph structure. SiPS uses a single graph structure that can accomodate all three processes and has a parameter which we call \"focus\" that scales processes on this graph from operating purely as a system of Active Inference with Bayesian filtering to a system of Projective Simulation. In this work, we explore when, why, and how an agent might switch between these two modes, or adopt an intermediate \"focus\" state, to achieve insightful behavior in novel environments without relying on offline and computationally intensive learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e04e41-3ef4-4a23-9481-f7828902ab8b",
   "metadata": {},
   "source": [
    "# How does SiPS work?\n",
    "\n",
    "## Overview\n",
    "\n",
    "Like any reinforcement learning model, SiPS takes an input (often called its state, or sensory state) and returns an output (an action). SiPS, however, actually acts as two Reinforecement learning agents nested inside of each other; for clarity, we need to make some adjustments to the terminology. We will refer the state of systems that transmit information from the outside world to the internal processes of a SiPS agent as the \"observation\". We give this observation the random variable $\\boldsymbol{O}$ and denote any discrete observation state $\\boldsymbol{o^{(\\mathrm{i})}}$. The observation state acts as the input for a SiPS agent's reflex system, a simple reinforcement learning system that returns an action. The weights of this system, however, may be tuned by the SiPS agent's Episodic and Compositional Memory (ECM). The ECM is *also* a reinforcement learning system It combines both the agent's observation and its action into an input we call the \"percept\" and returns ephemeral adjustments to the reflex system's weights which we call \"priming\". It does this through a process we call deliberation. Thus, a translation from standard RL looks as follows.\n",
    "\n",
    "![ML_Compare](ML_to_SiPS_translation.png)\n",
    "<figure>\n",
    "<figcaption align = \"center\"> Figure 1: SiPS from a Machine Learning Perspective. </figcaption>\n",
    "</figure>\n",
    "\n",
    "The Episodic and Compositional Memory is best understood as a graph. Following from the principles of Active Inference, the ECM of SiPS is organized hierarchically such that information passed from higher levels to lower levels represent predictions, and information passed from lower levels to higher levels represent prediction errors. We refer to nodes in the lowest level of the ECM as \"perceptual representations\", which are further divided into \"sensory representations\" and \"action representations\". Percepts map to a set of excitation states for these representations. Higher levels of the ECM interact with the ECM's environment only through the perceptual representations. Higher level nodes in the ECM represent \"memory traces\" of past situational states; at any given time, the agent's perceptual state is encoded to a single trace by establishing connections between that trace and excited percept nodes. Over time, the strength of these connections may change as functions of forgetting and reinforcement, and new connections may be formed by the deliberative process. A key goal of the SiPS model is to understand how these three processes can be configured such that connections to a memory trace either decay to obselesence or crytalize into a representation of a learned relation in the world that can be generalized to other, new situations.\n",
    "\n",
    "Figure 2 provides an overview of the SiPS structure, including both the reflex system and the ECM.\n",
    "\n",
    "![Structure_Overview](SiPS_structure.png)\n",
    "<figure>\n",
    "<figcaption align = \"center\"> Figure 2: SiPS Structure Overview. SiPS combines a reflex system with episodic memory structure. The reflex system is defined as a matrix, <b>W</b>, with edges that connect each unique observation to each action in the agent's repertoire. The episodic memory structure can be represented as a graph with three type of nodes: sensory representations, action representations, and memory traces. Sensory representations and action representations are jointly refered to as percept nodes. Percept nodes are connected to memory traces by bidirectional edges called Hebbian weights, given by a matrix <b>H</b>. Memory Traces have recurrent edges with weights given by the matrix <b>M</b>. </figcaption>\n",
    "</figure>\n",
    "\n",
    "## States and Variables\n",
    "\n",
    "Each node in a SiPS ECM is associated with multiple states. To denote states associated with sensory representations we will use the letter 's'; to denote states associted with action representations we will use the letter 'a'; to denote the states of percept nodes more generally we will use the letter 'p'; and to denote states associted with memory traces we will use the letter 'b' (beliefs).\n",
    "\n",
    "There are three types of state associated with each node, which we distinguish notationally by using diacritical marks. Here, we describe each type of state breifly. Subsequent sections will explain and demonstrate the dynmaics of these states and their interactions in the ECM in much greater detail.\n",
    "\n",
    "The <b>excitation state</b> of a node reflects the transmission of new sensory information through the graph, and is denoted by a plain letter with no diacritical mark. For example, the vector of excitation state for all sensory representations is denoted $\\boldsymbol{s}$ and an excitation state of a single sensory representation by $s_\\mathrm{j}$. $\\boldsymbol{S}$ denotes the J dimensional random variable associated with sensory representation states, where J is the number of sensory representation nodes. Subscripts of this variable, $S_\\mathrm{j}$, indicate random variables associated with the excitation state of a specific sensory representation. Likewise, $\\boldsymbol{a}$ denostes the vector of excitation states for all action representations, $a_\\mathrm{k}$ the excitation state of a single action representation, and $\\boldsymbol{A}$ and $A_\\mathrm{k}$ the reandom variables for these states. So on and so forth for the excitation of a percept node $p_\\mathrm{l}$ and a memory trace $b_\\mathrm{n}$. For clarity and convenience, the indices j,k,l, and n will always denote senory representations, action representations, percept nodes, and memory traces, respectively.\n",
    "\n",
    "The <b>expectation state</b> of a node reflects the ECM's prediction regarding its next situated state and is denoted using the diacritical ^. For example, $\\hat{b}_\\mathrm{n}$ denotes the expectation state of memory trace n, which reflects the strength of the agent's belief that its next situated state will be well represented by the situated state encoded by memory trace $\\mathrm{n}$. As with excitation states, $\\hat{\\boldsymbol{P}}$ denotes the random variable associated with the vector of percept node expectation states, and so on and so forth.\n",
    "\n",
    "The <b>activation</b> state of a node reflects how strongly that node is currently being considered by the agent's deliberative process, and is denoted by the diacritical ~ ($\\tilde{A}_\\mathrm{k}$,$\\tilde{\\boldsymbol{s}}$, etc. . .). Importantly, while all other random variables in a SiPS ECM evolve on a discrete timescale $t$, the activation state evolves at a smaller timescale $\\tau$. The evolution of the activation states across the ECM over an interval of $t$ defines the stochastic process by which the ECM computes the expectation states of memory traces, i.e. predictions about its next situated state. I think of this process as modeling the agent's attention, and will often describe it as such, but use the name \"activation state\" to avoid confusion with other well known attention mechanisms in ML.\n",
    "\n",
    "Memory Traces have a fourth associated state: their <b>valence state</b> given by $v_\\mathrm{n}$. The valence of a memory trace reflects how suprised the ECM was by the excitation state of sensory represenetations following the situated state encoded by that trace. The valence of a memory trace determines how much an associated action is inhibited or primed by an increase of the given memory trace's activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d54bcd-4ab3-4e7b-9ad4-13638a709018",
   "metadata": {},
   "source": [
    "## Creating a SiPS agent\n",
    "\n",
    "Let's begin an exploration of how SiPS agents work by using the \"Projective Simulation\" package to create a simple agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d443d12-e182-4e6b-8e38-3f0553b6905e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projective_simulation.agents import Situated_Agent\n",
    "from projective_simulation.environments import Delayed_Response\n",
    "import nbdev\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c00a3f-8490-408d-8fbc-ad666eeccd95",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Situated_Agent.__init__() got an unexpected keyword argument 'memory_capacity'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m example_SiPS \u001b[38;5;241m=\u001b[39m \u001b[43mSituated_Agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_actions\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_capacity\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m example_SiPS\n",
      "\u001b[1;31mTypeError\u001b[0m: Situated_Agent.__init__() got an unexpected keyword argument 'memory_capacity'"
     ]
    }
   ],
   "source": [
    "example_SiPS = Situated_Agent(num_actions = 2, memory_capacity = 100)\n",
    "example_SiPS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb58d0-425e-4ec9-a97c-b72403ecff69",
   "metadata": {},
   "source": [
    "There are many parameters that can be used to customize a SiPS agent, but at a minimum the number of actions and memory traces available to the agent must be defined. For now, we will rely on default values for all other parameters. As with all projective simulation agents, a SiPS agent can return an action if given an observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99302b1c-cde6-4da1-b561-9c42d7a78c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_observations = [1,0,5,1,1]\n",
    "test_actions = [None] * len(test_observations)\n",
    "for t in range(len(test_observations)):\n",
    "    test_actions[t] = example_SiPS.get_action(test_observations[t])\n",
    "print(test_actions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf790bba-5c67-4c05-bc17-4f8e6d9e491e",
   "metadata": {},
   "source": [
    "The get_action function of SiPS agent has three steps: triggering a reflex, processing the percept, and setting expectations. Each of these steps is handled by a different object in the Situated_Agent class: the reflex_ECM, preprocessor, and episodic_ECM, respectively.\n",
    "\n",
    "Let's cover the reflexes firsts\n",
    "\n",
    "## Reflexes\n",
    "\n",
    "The first thing a SiPS agent does when it recieves a new observation is it selects an action using its reflexes. Note that while we use an independant ECM class type (Priming_ECM) to define the SiPS reflexes, conceptually these reflexes are a *component* of a SiPS ECM. The Priming_ECM is a subclass of a Two_Layer ECM, and we can take a look at the documentation of these classes to see how they work. We refer the reader to (LINK TO BASIC PS TUTORIAL) for discussion of the role glow and damp parameters in reinforcement learning on a two-layer PS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cd01e9-8ff8-4ac8-8818-c755e95bb52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projective_simulation.ECMs import Two_Layer, Priming_ECM\n",
    "nbdev.show_doc(Two_Layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ad49b-889e-486c-8171-015879280e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "nbdev.show_doc(Priming_ECM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c338c2a9-48d4-4c28-b459-d5a273b0575e",
   "metadata": {},
   "source": [
    "The addition of action priming is important for SiPS, and is given by the expectation state of action representations. A positive action expectation increases the probability that the priming ECM returns the corresponding action for all input percept. Likewise, a negative value reduces the probability of returning that action. Take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c34f5-0034-449f-87b0-a42db6c06fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "reflexes = Priming_ECM(num_actions = 2, action_primes = [0,4])\n",
    "example_SiPS = Situated_Agent(reflex_ECM = reflexes, memory_capacity = 100)\n",
    "# Notice that we define a reflex ECM for the Situated Agent instead of num_actions.\n",
    "# If a reflex ECM is predefined, the SiPS agent uses that instead of creating a reflex ECM using the num_actions, glow, damp, and reflex_softmax variables\n",
    "\n",
    "test_observations = [0] * 10 + [1] * 10 #observe 0 ten times, then observe 1 ten times\n",
    "test_actions = [None] * len(test_observations)\n",
    "for t in range(len(test_observations)):\n",
    "    test_actions[t] = example_SiPS.get_action(test_observations[t])\n",
    "print(test_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9141e-af9d-4e86-a789-9c06fbfc3823",
   "metadata": {},
   "source": [
    "Specifically, the probability that the reflex ECM returns action $k$ given a particular observation $o^{(i)}$ is given by the equation\n",
    "\n",
    "\\begin{equation}\n",
    "Pr(\\boldsymbol{A} = \\boldsymbol{a}^{(\\mathrm{k})}|\\boldsymbol{O} = \\boldsymbol{o}^{(\\mathrm{i})}) = \\frac{\\mathrm{e}^{\\beta_\\mathrm{reflex}(W_\\mathrm{ik} + \\hat{A}_\\mathrm{k})}}{\\sum_\\mathrm{k'=1}^\\mathrm{K}{\\mathrm{e}^{\\beta_\\mathrm{reflex}(W_\\mathrm{ik'} + \\hat{A}_\\mathrm{k'})}}}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed7d6ba-6c4e-41df-8895-5de656091aba",
   "metadata": {},
   "source": [
    "Where the Action Space of the agent, $\\mathcal{A}^\\mathrm{K}$, is defined such that for a given number of actions K, a particular action state $\\boldsymbol{a}^{(k)}$ is equivalent to the one-hot excitation of action representations where $a_\\mathrm{k} = 1$; $W_{ik}$ is the reflex weight from observation state $\\boldsymbol{o}^{(\\mathrm{i})}$ to action state $\\boldsymbol{a}^{(\\mathrm{k})}$; and where $\\beta_\\mathrm{reflex}$ is the reflex_softmax paramter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f11e9f6-cd4c-4398-b4db-a0428ec3a6ce",
   "metadata": {},
   "source": [
    "## The Episodic Graph\n",
    "\n",
    "The Priming_ECM does not change the priming of actions on its own - expectation values must come from another system such as an episodic graph. We can create an episodic graph as follows.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4b0a59-0905-463b-b7e3-83620885ba8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from projective_simulation.ECMs import Episodic_Memory\n",
    "em_example = Episodic_Memory(num_actions = 2, capacity = 50)\n",
    "em_example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecaba85-3209-4762-b2a2-8278dc48f9a8",
   "metadata": {},
   "source": [
    "In SiPS, the episodic graph has three node types, action reprensentations, sensory representations, and memory traces. Sensory representations and action representations (together called percept nodes) are connected to memory traces by edge weights given by the H-matrix. Because the newly intitialized episodic graph has not recieved any observations, it has no sensory representations. It is required, however, to predefine an action space for an episodic graph, and thus the H-matrix is currently KxN (num_actions by capacity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74b492a-0734-4fff-b2d8-85939b942a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(em_example.hmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f97f41-d19a-43c7-a522-a355de1ee861",
   "metadata": {},
   "source": [
    "### Representing the Percept\n",
    "\n",
    "One goal of a SiPS agent is to learn about relations between individual elements or combinations of elements in observations. Thus, the episodic graph creates a sensory representation for each new value observed in *each dimension* of an observation vector. The SiPS agent has a \"Percept Processor\" that keeps track of which value and dimension of an observation each sensory representation is connected to. It can thus convert any observation to a sensory excitation state. The percept of the ECM is defined as the concatenation of the sensory representation exitation state and the action representation excitation state. Let's see what happens when we give the preprocessor of a new SiPS agent a two dimensional observation, and an action triggered by that observation.\n",
    "\n",
    "Initially, the agent's percept dictionary only maps actions '0' and '1' to action representations 0 and 1, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868442d6-d9a9-4fe6-8366-ec1e5161b567",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS = Situated_Agent(num_actions = 2, memory_capacity = 10)\n",
    "example_SiPS.percept_processor.percept_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517612f3-5f84-45e2-a9aa-2f0126350426",
   "metadata": {},
   "source": [
    "But after receiving a two-dimensional observation, the percept processor returns the excitation states of four percept nodes. We can see that the two additional nodes correspond to two sensory representations that have been added to the percept dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdc3f81-e629-40d5-ab4c-be28f756403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS = Situated_Agent(num_actions = 2, memory_capacity = 10)\n",
    "observation = np.array([0,4])\n",
    "action = 1\n",
    "example_SiPS.percept_processor.get_percept(observation, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139fab46-fec1-4503-98ff-0d90820ab4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.percept_processor.percept_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766cc87b-1806-4dae-a388-9c9b34fa2f2d",
   "metadata": {},
   "source": [
    "Note the nested structure of this dictionary. Each value of the percept dictionary is also a dictionary, which keys the sensory representations for a given dimension of the observation (or the action).\n",
    "\n",
    "If the dimensionality of the agent's observations changes, the percept processor handles this flexibly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849b57c3-f298-4c60-bb11-0dd6d7a47236",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.percept_processor.get_percept(observation = np.array([1,4,0]), action = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db3b337-c2b8-4f58-8c15-601280612306",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.percept_processor.percept_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0accc2c-3aaf-4ff4-94d4-f60718244184",
   "metadata": {},
   "source": [
    "### Pre-Deliberation\n",
    "\n",
    "Once a SiPS agent has chosen and action and represented its percept, it begins to deliberate. This is the process by which the agent assesses its situated state and predicts what will happen next. In the process, it encodes relevant information about its current state to memory and potentially reorganizes existing connection in its episodic memory. We will go through the steps of deliberation one-by-one.\n",
    "\n",
    "#### Adding New Sensory Representations\n",
    "\n",
    "If the percept passed to the episodic graph by the percept processor is larger than the current number of percept nodes, indicating a new sensation, new nodes are constructed by adding elements to all of the relevent state variables. See help(Episodic_Memory.add_percept) for details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f78398-bcdb-4d1f-972f-eb020bc96789",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS = Situated_Agent(num_actions = 2, memory_capacity = 10) # start a new example\n",
    "observation = np.array([0,2,1])\n",
    "action = example_SiPS.reflex_ECM.deliberate(str(observation))\n",
    "percept = example_SiPS.percept_processor.get_percept(observation, action)\n",
    "example_SiPS.ECM.add_percept(percept) #creates new elements of expectation, h-matrix, trace_encoder, and action_encoder to account for new observation\n",
    "print(np.shape(example_SiPS.ECM.hmatrix)) #hmatrix has three new rows (in addition to for actions) because all three dimensions of the observation have new values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387eef8d-12e5-49b5-b6b7-3a918f4762ed",
   "metadata": {},
   "source": [
    "#### Calculating Suprise\n",
    "\n",
    "Surprise is the primary mechanism by which SiPS learns. It is used both the ascribe an emotional valence to memory traces and to reinforce or punish the weights of edges (more on this later). The surprise of each sensory representation is computed by treating its excitation as a Bernoulli trial with probability equal to that representation's expectation state, i.e.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{I}(S_\\mathrm{j}^{(t)}) = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "  -\\mathrm{log}(\\hat{S}_\\mathrm{j}^{(t-1)}) & \\text{if } S_\\mathrm{j}^{(t)} = 1  \\\\\n",
    "  -\\mathrm{log}(1-\\hat{S}_\\mathrm{j}^{(t-1)}) & \\text{if } S_\\mathrm{j}^{(t)} = 0,\n",
    "\\end{array}\n",
    "\\right.\n",
    "\\end{equation}\n",
    "\n",
    "where $s_\\mathrm{j}(t)$ is the current excitation state of sensory representation $\\mathrm{j}$ and $\\hat{s}_\\mathrm{j}(t-1)$ is the expectation state of sensory representation $\\mathrm{j}$ set in the last time step. The agent's total surprise is simply the sum of surprises across all sensory representations.\n",
    "\n",
    "To demonstate, let's compare the suprise if our example agent has an expectation state that strongly predicted the current percept versus the surprise when the expectation state includes weaker or wrong predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d302ca-401c-40a1-ac76-bb6055893757",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.ECM.expectations = np.array([0.99 if percept[l] == 1. else 0.01 for l in range(len(percept))]) #set expectations that strongly predict percept\n",
    "surprise1 = example_SiPS.ECM.get_surprise(percept)\n",
    "example_SiPS.ECM.expectations = np.array([0.01 if percept[l] == 1. else 0.99 for l in range(len(percept))]) #set expectations that wrongly predict percept\n",
    "surprise2 = example_SiPS.ECM.get_surprise(percept)\n",
    "example_SiPS.ECM.expectations = np.array([0.5 for l in range(len(percept))]) #set uncertain expectations\n",
    "surprise3 = example_SiPS.ECM.get_surprise(percept)\n",
    "\n",
    "print(['correct = ' + str(surprise1), 'wrong = ' + str(surprise2), 'uncertain = ' + str(surprise3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f834d0cc-02be-4e39-acb1-3aae3a9058af",
   "metadata": {},
   "source": [
    "Note that the expectation of action representations does not affect the total surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c42421-0a8a-44de-905c-474699bf0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.ECM.expectations[0:2] = [0.99, 0.99] #Becuase there are two actions, the first and second elements of the expectations variable correspond to those two action representations\n",
    "example_SiPS.ECM.surprise = example_SiPS.ECM.get_surprise(percept)\n",
    "print(example_SiPS.ECM.surprise)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb2a6b1-b62a-43bb-bb69-c9e94778f605",
   "metadata": {},
   "source": [
    "#### Exciting Memory Traces\n",
    "\n",
    "The next step of delibration is to pass excitation information from percept nodes to memory traces. The excitation of a memory trace is equal to the likelihood of the current percept if the excitation of each percept node to which that trace is connected is treated like a Bernoulli trial with probability equal to a logistic transformation of the Hebbian edge weight between the two nodes. Formally,\n",
    "\n",
    "\\begin{equation}\n",
    "B^{(t)}_\\mathrm{n} = \\prod_{i \\in \\boldsymbol{h_n}^*} f_\\kappa(H^{(t)}_\\mathrm{ni})^{S^{(t)}_\\mathrm{i}}(1-f_\\kappa(H^{(t)}_\\mathrm{ni}))^{1-S^{(t)}_\\mathrm{i}},\n",
    "\\end{equation}\n",
    "\n",
    "where $h_\\mathrm{n}^*$ is the set of indices denoting which percept nodes are connected to memory trace $\\mathrm{n}$ at time $t$ and $f_\\kappa(h)$ is the standard logistic function with scalable rate parameter $\\kappa$.\n",
    "\n",
    "Because our example agent has not yet encoded any memory traces, there are no edges connecting memory traces to percept nodes; thus memory traces will not be excited by an agent's first percept. In the next section, we introduce memory trace encoding and then return to their excitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4441cbdb-8883-4a79-85cd-40b2a663072a",
   "metadata": {},
   "source": [
    "#### Encoding a New Memory Trace\n",
    "\n",
    "Once all nodes have been excited, a new trace is encoded as a function of those exitations. Three things happen during trace encoding.\n",
    "\n",
    "First, edges are established to all excited percept nodes. I use the trace_encoder matrix to encode which connections have been estalished and the hmatrix to encode the strength of connections. Right now, when an edge is established, its strength is set from 0 to 1 and will revert to 0 as a function of the ECM's damping parameter. Allowing a scalable initial strength is also an important next step for implementation.\n",
    "\n",
    "Second, a temporal edge is established between the current memory trace and the last memory trace encoded. In the current version of SiPS, an internal variable $t$ tracks temporal increments and during encoding the element of the memory matrix $M_{t-1,t}$ is set to 1. Additionally, $M_{t,t+1}$ is set to 0, which breaks a previously established connection in the event that $\\mathrm{T}$ is greater than the agent's memory capacity and memory traces are re-used. An important direction for future work will be to consider other trace selection procedures (e.g. select from traces with lowest excitation) and allowing multiple temporal connections to be established from individual memory traces.\n",
    "\n",
    "Third, the valence of the last encoded memory trace is set to the new total suprise of the ECM.\n",
    "\n",
    "If we look at our example agent prior to encoding, we will see that it has no connections in the trace_encoder and all weights in the hmatrix are 0. After encoding, new connections and weights have been established"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921b80ca-0e31-46ad-90f2-d886412a5000",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_SiPS.ECM.trace_encoder)\n",
    "print(example_SiPS.ECM.hmatrix)\n",
    "example_SiPS.ECM.encode_trace(percept)\n",
    "print(example_SiPS.ECM.trace_encoder)\n",
    "print(example_SiPS.ECM.hmatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc290b1-127b-4f64-85d4-1b24ed553100",
   "metadata": {},
   "source": [
    "Likewise, the last trace has a new valence and the memory matrix has a new temporal connection that points to this trace. Because it is the first time step, however, this trace has no connected edges in the h-matrix and is of no consequence. The valence is high because the agent sets a low expectation on sensory representations by defualt and we did not set new expectations, neither manually nor by completing the process of deliberation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f06ea3-e02c-4c9b-97a7-115a89d1af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(example_SiPS.ECM.mmatrix)\n",
    "print(example_SiPS.ECM.valences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bcd0efe-ceb7-435d-8ebb-b4d56a145685",
   "metadata": {},
   "source": [
    "To understand how trace excitation works, we can set the ECM's internal increment tracker forward and encode a new observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f6df12-dfc6-48fb-8a5f-cfa72aa24255",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.ECM.t += 1\n",
    "observation = np.array([0,5,5]) #the second and third elements of this observation are new, the first element is the same as the first observation\n",
    "#~repeat previously described steps\n",
    "action = example_SiPS.reflex_ECM.deliberate(str(observation))\n",
    "percept = example_SiPS.percept_processor.get_percept(observation, action)\n",
    "example_SiPS.ECM.add_percept(percept)\n",
    "example_SiPS.ECM.excite_traces(percept)\n",
    "#~\n",
    "#Encode new trace\n",
    "example_SiPS.ECM.encode_trace(percept)\n",
    "\n",
    "print(example_SiPS.ECM.hmatrix)\n",
    "print(example_SiPS.ECM.trace_excitations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a1ae2-1d80-4812-a747-44a00e0073a0",
   "metadata": {},
   "source": [
    "Using the default values for $\\kappa$ (3) and a newly established h-value (1), $f_\\kappa(1) \\approx 0.731$. It can then be seen that the excitation of memory trace 0 is $\\approx 0.731^n(1 - 0.731)^{k-n}$, where k is the number of connections from percept nodes to memory trace 0 and n is the number of connected percept nodes that are excited (in this example, $\\mathrm{k} = 4$ and $n$ is either 1 or 2, depending whether or not the agent took the same action in both steps). The SiPS agent's kappa parameter adjusts the scaling parameter of the standard logistic function, such that higher values of kappa means the agent will assign a higher likelihood to the excitation of a percept node given that the connection weight remains constant. If we set kappa very high, for example, the agent will treat the failure to excite even two sensory representations connected to memory trace 0 as extremely unlikely if the current world state were representated by that trace, and thus the excitation of memory trace 0 given our new percept will be much lower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4440ada4-9a6c-4c14-9621-00bbf9c36bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.ECM.kappa = 5\n",
    "example_SiPS.ECM.excite_traces(percept)\n",
    "print(example_SiPS.ECM.trace_excitations)\n",
    "example_SiPS.ECM.kappa = 1 #reset for continued use in vignette\n",
    "example_SiPS.ECM.trace_excitations[1] = 0 #reset for continued use in vignette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaee2a5-7db8-4be8-a897-ed0e13e20b4e",
   "metadata": {},
   "source": [
    "Because we have now encoded the second trace, it also becomes excited. Naturally, the excitation of a trace that encodes the current percept will be maximally close to 1 for the given kappa - it is only connected to excited percept nodes! This is one reason why traces are encoded AFTER they are excited, so as not to bias the agent's attention toward the obvious fact that its current world state looks identical to its current world state)\n",
    "\n",
    "### Projection\n",
    "\n",
    "Having excited the ECM as a function of the Percept, we now initialize the agent's attention, beginning the actual deliberative process. I've used \"activation\" to denote the agent's attention because \"attention\" is a different (but closely related) and well known mechanim in ML, but conceptually it remains useful to the think of the ECMs activation state and reflecting the weight of the agent's attention to the concept reprented by a node, whether it is a sensation, an action, or a memory. The general idea of Delibration is that the agent begins with all of its attention on the just-encoded memory trace; then, the attention diffuses across the ECM as if it were composed of particles performing a random walk on the ECM-graph, thus \"projecting\" its attention to other possible situated states. Currently, activation states are equivelent to the expected proportion of particles on the given node, given the current length of deliberation. This is calculated iteratively. Thus, it would be reasonably straightforward to implement the stochastic random walk of some number of discrete particles.\n",
    "\n",
    "There is also a stochastic element even in the current implementation. This element reflects \"Projective Simulation\" on the graph, and can be scaled by the agent's focus paramter, $\\theta$. Projective Simulation selects a random edge leading from each node of the ECM, with probability equal to that of an independant particle diffusing from the node, and increases the actual probability that particles diffuse along this edge. The probability is increased such that a proportion of particles (equal to the focus parameter) in that node that would be expected to diffuse along other edges diffuse along the selected edge instead. Thus, when focus is equal to 1, the entirety of the agent's attention mass walks along the graph as a single unit, and the process is functionally equivalent to the system of Projective Simulation laid out in Briegel and De La Cuevas 2012.\n",
    "\n",
    "Formally, we define the probability that an edge $l^*$ selected by Projective Simulation connects memory trace $n$ to percept node $l$ as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Pr}_\\beta(l^*=l|H^{(t)},n) = \\frac{\\mathrm{e}^{\\beta H^{(t)}_{l\\mathrm{n}}}}{\\sum_{l'\\in h^*_\\mathrm{n}}{\\mathrm{e}^{\\beta H^{(t)}_{l'\\mathrm{n}}}}}.\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta$ is the episodic graph's softmax paramter and $h^*_\\mathrm{n}$ is the set of indices for percept nodes connected to memory trace $\\mathrm{n}$.\n",
    "\n",
    "Following from this definition and our assumption that $Pr_\\beta(l^* = l)$ is equivalent to the probability that an unbiased particle in memory trace $\\mathrm{n}$ diffuses along connection $l$, the activation state of a percept node $l$ during deliberative step $\\tau > 0$ at time $t$ is thus given as\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{P}^{(t,\\tau)}_l = \\sum_{\\mathrm{n}=1}^\\mathrm{N} \\tilde{B}^{(t,\\tau-1)}_\\mathrm{n} (\\mathrm{Pr}_\\beta(l^* = l|H^{(t)},\\mathrm{n}) + \\theta *\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "  (1-\\mathrm{Pr}_\\beta(l^* = l|H^{(t)},\\mathrm{n})) & \\text{if } l^* = l  \\\\\n",
    "  -\\mathrm{Pr}_\\beta(l^* = l|H^{(t)},\\mathrm{n}) & \\text{otherwise }\n",
    "\\end{array}\n",
    "\\right\\}),\n",
    "\\end{equation}\n",
    "\n",
    "where $\\tilde{B}^{(t,\\tau-1)}_\\mathrm{n}$ is the activation state of memory trace $\\mathrm{n}$ during the previous deliberation step and $\\theta$ is the agent's focus parameter.\n",
    "\n",
    "#### Initial activation\n",
    "\n",
    "Deliberation begins at step $\\tau = 0$ by setting the activation of the just-encoded memory trace to 1 and the activation of all other nodes to 0.\n",
    "\n",
    "Because the just-encoded memory trace is only connected to excited percept nodes and with equal weights, the SiPS agent doesn't bother to run the first step of deliberation as normal, and simply computes the expected proportion of attention that would diffuse to percept nodes from the just-excited trace and initially activates those percept nodes accordingly. Thus, if the focus parameter is zero (the default value) the intitial activation of excited percept nodes will be 1 divided by the number of excited percept nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bc1e5f-ffdd-4e45-b306-4f8f855df64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.ECM.activate()\n",
    "print(\"excitations: \" + str(percept))\n",
    "print(\"activations: \" + str(example_SiPS.ECM.percept_activations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021389e-7f5b-40a4-b573-5530e76f67c8",
   "metadata": {},
   "source": [
    "Adjusting the focus parameter of the agent will shift a proportional amount of this activation to a randomly selected, excited percept node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17068f6-c097-4006-91b7-f09dd61b4bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.ECM.focus = 0.5\n",
    "example_SiPS.ECM.activate()\n",
    "print(\"excitations: \" + str(percept))\n",
    "print(\"focus = 0.5 activations1: \" + str(example_SiPS.ECM.percept_activations))\n",
    "example_SiPS.ECM.activate()\n",
    "print(\"focus = 0.5 activations2: \" + str(example_SiPS.ECM.percept_activations))\n",
    "example_SiPS.ECM.focus = 1\n",
    "example_SiPS.ECM.activate()\n",
    "print(\"focus = 1 activations1: \" + str(example_SiPS.ECM.percept_activations))\n",
    "example_SiPS.ECM.activate()\n",
    "print(\"focus = 1 activations2: \" + str(example_SiPS.ECM.percept_activations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5061f9-3681-4bf7-b8fb-2311ae2a5a73",
   "metadata": {},
   "source": [
    "#### Continuing Projection\n",
    "\n",
    "Once the activation state of a SiPS agent has been thus initiated, we can simply propogate the activation state forward across increments of $\\tau$ - except we have not yet defined how the activation state moves from percept nodes to memory traces! This definition is very similar to that for the diffusion of activation from memory traces to percept nodes. However, in this case the probability of diffusing along an edge is also influenced by both the expectation state and excitation state of the memory trace to which that edge is connected. Here is how to think of this: the probability that a 'particle' of attention moves from a representation of a sensation or action in the agents repretoire to a particular memory trace is proportional to the each of the following: the associative strength between the given perceptual representation and the memory trace, the agent's prior belief that the memory trace will effectively represent its new world state, and the strength of immediate sensory evidence that the memory trace effectively represents the agent's new world state.\n",
    "\n",
    "We have previously defined the expectation state of a memory trace as the agent's prior belief that the given trace will effectively represent its current world state and the excitation state of a memory trace as the probability that the memory trace effectively represents the current world state given new sensory evidence. With this in mind, we define the probability that projective simulation from percept representation $l$ selects memory trace $\\mathrm{n}$ as equivalent to the probability of a particle of attention diffusing from $l$ to $\\mathrm{n}$, given as\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{Pr}_\\beta(\\mathrm{n}^*=\\mathrm{n}|H^{(t)},l) = \\frac{\\mathrm{e}^{\\beta H^{(t)}_{l\\mathrm{n}}B^{(t)}_\\mathrm{n}\\hat{B}^{(t-1)}_\\mathrm{n}}}{\\sum_{\\mathrm{n'\\in h}^*_l}{\\mathrm{e}^{\\beta H^{(t)}_{l\\mathrm{n'}}B^{(t)}_\\mathrm{n'}\\hat{B}^{(t-1)}_\\mathrm{n'}}}}.\n",
    "\\end{equation}\n",
    "\n",
    "where $\\beta$ is the episodic graph's softmax paramter and $h^*_\\mathrm{n}$ is the set of indices for percept nodes connected to memory trace $\\mathrm{n}$. (NOTE: Putting the expectation and excitation states of the trace nodes in the exponent significantly dilutes their effect and divorces them from their meaning as probabilities. However, taking them out of the exponent leads to problems when either is 0. I have some thoughts about changing the overall structure such that these terms can be taken out of the exponent without causing problems, leading to better leaning and more interpretability.) We can then define the activation state of a memory trace $\\mathrm{n}$ during deliberative step $\\tau > 0$ at time $t$ exactly as we did for percept nodes:\n",
    "\n",
    "\\begin{equation}\n",
    "\\tilde{B}^{(t,\\tau)}_\\mathrm{n} = \\sum_{l=1}^{|P^{(t)}|} \\tilde{P}^{(t,\\tau-1)}_l (\\mathrm{Pr}_\\beta(\\mathrm{n}^* = \\mathrm{n}|H^{(t)},l) + \\theta *\n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "  (1-\\mathrm{Pr}_\\beta(\\mathrm{n}^* = \\mathrm{n}|H^{(t)},l)) & \\text{if } \\mathrm{n}^* = \\mathrm{n}  \\\\\n",
    "  -\\mathrm{Pr}_\\beta(\\mathrm{n}^* = \\mathrm{n}|H^{(t)},l) & \\text{otherwise }\n",
    "\\end{array}\n",
    "\\right\\}),\n",
    "\\end{equation}\n",
    "\n",
    "where $|P^{(t)}|$ is the number of percept nodes in the episodic graph at time $t$.\n",
    "\n",
    "Thus, using the equations to update the activation states of both percept nodes and memory traces, the agent's attention can be propogated forward over interal's of $\\tau$ for some number of steps $\\mathrm{D}$, which we call the deliberation length. We will demonstrate shortly how the activation state of the agent when $\\tau = \\mathrm{D}$ is used as an updated belief state to make prediction, but first let us provide an example of deliberation. We start with a case where focus is zero, which emulates normative Bayesian filerting and removes the stochastic element of projective simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a4700-bc4a-489c-8c37-7da9b307e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.ECM.focus = 0\n",
    "example_SiPS.ECM.activate()\n",
    "print(\"percept activations = \", str(example_SiPS.ECM.percept_activations))\n",
    "print(\"trace activations = \", str(example_SiPS.ECM.trace_activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7508c0b8-1f41-402f-af58-416244c74522",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.ECM.hmatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cac5f4-3b2e-4f5a-985f-664c075fdeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.ECM.diffuse_activation()\n",
    "print(\"percept activations = \", str(example_SiPS.ECM.percept_activations))\n",
    "print(\"trace activations = \", str(example_SiPS.ECM.trace_activations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961191e9-1eeb-4bbb-bb02-212a8955b482",
   "metadata": {},
   "source": [
    "You might note that activation in a percept node with connections to both memory traces with an encoding becomes divided evenly between the two traces. This is because the expectation state of both traces is zero (the agent has no information in its first time step with which to make a prediction) and the excitation state of both traces is effectively 0 (the first trace has some negligible excitation because it is connected to two excited percept nodes, but the agent treats the two edges connected to unexcited percept nodes as strong evidence its situation is not well represented by this trace - recall that this is a function of the h-values connecting those nodes and the agent's $\\kappa$ parameter). If we manually add excitation or expectation to a trace, we can observe a difference. First with excitation . . ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455d768d-b0f1-4cca-9920-b312bd8a4901",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.ECM.activate()\n",
    "example_SiPS.ECM.trace_excitations[0:2] =[0.1,0.5]\n",
    "example_SiPS.ECM.beliefs[0:2] = [0.1,0.1]\n",
    "example_SiPS.ECM.diffuse_activation()\n",
    "print(\"trace activations = \", str(example_SiPS.ECM.trace_activations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a490556-e27d-4751-a7c2-c648c65317a5",
   "metadata": {},
   "source": [
    ". . . and the same results with trace expectation/belief weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca6322-f4ce-45b8-bb06-93a5f3fe7fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.ECM.activate()\n",
    "example_SiPS.ECM.trace_excitations[0:2] = [0.1,0.1]\n",
    "example_SiPS.ECM.beliefs[0:2] = [0.1,0.5]\n",
    "example_SiPS.ECM.diffuse_activation()\n",
    "print(\"trace activations = \", str(example_SiPS.ECM.trace_activations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db958a01-a147-4877-b90b-3a9f793859b1",
   "metadata": {},
   "source": [
    "Note that in this example, excitatation and expectation of encoded memory traces were set to non-zero values. Because the edge weight, trace expectation, and trace excitation are multiplied to get transition probabilities, a zero for any of these values means the other two become disregarded. The edge weight will never be zero (it is in the exponent) and the excitation of a previously encoded trace will also never be zero (it is a likelihood derived from events with probabilites given by the logistic function). As the focus parameter approaches 1, however, expectation states on some encoded traces may diminish to zero and *can* be zero if the focus is one or attention is treated as discrete units instead of a proportion. In other words, an agent with a high focus paramter strongly weights it expectation toward a single situtated state (memory trace) and will always direct its attention to that trace when possible, regardless of the sensory evidence. We will come eventually to why this is an important feature of the SiPS agent. For now, must first finish describing the diliberative process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecbd89e-395d-4aa4-946f-102cd0806d4a",
   "metadata": {},
   "source": [
    "### Simulation and Prediction\n",
    "\n",
    "Once the agent has deliberated for $\\mathrm{D}$ steps, it \"simulates\" possible futures based on its updated belief about its situation. First, it predicts its next situation by passing the activation of its memory traces forward along the memory matrix,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{\\boldsymbol{B}}^{(t)} = \\tilde{\\boldsymbol{B}}^{(t,\\mathrm{D})} \\boldsymbol{M}^{(t)}\n",
    "\\end{equation}\n",
    "\n",
    "With the structure of the Memory matrix in the current implementation, this means that the new expectation state of an encoded memory trace $\\mathrm{n}$ is simply set to the final activation state of memory trace $\\mathrm{n} - 1$.\n",
    "\n",
    "The new expectation states of memory traces are then passed to the expectation state of percept nodes as a function of the associative connections in the h-matrix, \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{S}^{(t)}_\\mathrm{j} = f_\\epsilon(\\alpha_j, \\sum_{\\mathrm{n} \\in \\mathrm{h^*_j}} \\hat{B}^{(t)}_\\mathrm{n}f_\\kappa(H^{(t)}_\\mathrm{jn}))\n",
    "\\end{equation}\n",
    "\n",
    "where $\\mathrm{h^*_j}$ is the set of indices for memory traces that are connected to sensory representation $\\mathrm{j}$, $\\alpha_\\mathrm{j}$ is the intrinsic expectation of sensory representation $\\mathrm{j}$, and $f_\\epsilon(\\cdot)$ is an exponentiation with shift by a small constant $\\epsilon$, which SiPS sets to $10^-2$. This function takes the form\n",
    "\n",
    "\\begin{equation}\n",
    "f_\\epsilon(\\alpha,x) = \\frac{(x + \\epsilon)^{\\mathrm{e}^\\alpha}}{(1 + \\epsilon)^{\\mathrm{e}^\\alpha}}.\n",
    "\\end{equation}\n",
    "\n",
    "Why use this transformation? Recall that $f_\\kappa(H^{(t)}_\\mathrm{jn})$ can be understood as the probability with which the agent believes perceptual representation $\\mathrm{j}$ will be excited, given that memory trace $n$ is an effective representation of its situated state, and that $\\hat{B}^{(t)}_\\mathrm{n}$ can be understood as the probabilty with which the agent believes its next situated state will be well represented by memory trace $\\mathrm{n}$. Summing the product of these terms over all memory traces gives the probablity with which the agent believes sensory representation $\\mathrm{j}$ will be excited in its next situated state. We want an ECM's positive *intrinsic expectation* for a particular sensation to increase the expectation state of that sensation's representation even if the agent believes there is a zero percent chance that it will be excited. Likewise, a negative intrinsic expectation should decrease the expectation state of a representation, except if it is already 0, and no intrinsic expectation should leave the expectation state unchanged. However, we don't want the agent's intrinsic expectation to cause a sensory representation's expectation state to leave the space between 0 and 1. The shifted exponential achieves this (there is techinically a very small increase when $\\alpha = 0$, I'm open to suggestions), while also ensuring the expecatation state of a sensory representation can never be zero, which result in infinite surprise.\n",
    "\n",
    "Setting the expectation states for action representations is less straightforward. A key goal of SiPS is to model the way animals make fast-and-frugal decisions in real time to enable continuous action. As such, a SiPS agent does not stop to deliberate between receiving new sensory information and acting; it acts reflexively and uses concurrent cognitive processing (deliberation) tp *prime* or *inhibit* certain actions based on its expected situation. Thus, the expecation state of an action representation should carry information about the *value* of that action given the expected situation(s). In accordance with the Free Energy Principle, value for a SiPS agent is measured by surprise, or more precisely: the *reduction of expected surprise*. The valence state of a memory trace denotes how surprised the agent was by the sensory evidence that followed the situated state that the trace represents. Here, it hopefully becomes apparent why it is important that the SiPS agent includes its action when encoding a situated state in a memory trace - if it expects to be in a situation that is well represented by that trace, it can prime or inhibit associated actions as a function of the trace's valence. The world is generally surprising however (surprise is positive-real by definition), and the agent needs some threshold to determine when to prime and when to inhibit actions. For now, the SiPS agent simply compares the valence of a memory trace against the average valence of all memory traces. The log quotient of these two values is called the surprise advantage (NOTE: is \"advantage\" only the difference?); it is negative when the the valence of a trace is greater than the average (events after the trace were more suprising than average) leading to inhibition and positive when the valence is lower than average.\n",
    "\n",
    "\n",
    "$$\n",
    "\\hat{A}^{(t)}_\\mathrm{k} = \\sum_{\\mathrm{n} \\in \\mathrm{h^*_k}} \\hat{B}^{(t)}_\\mathrm{n}H^{(t)}_\\mathrm{kn}log(\\frac{\\bar{\\boldsymbol{V}}^{(t)}}{v^{(t)}_n}).\n",
    "$$\n",
    "\n",
    "A SiPS agent updates all three of these expectation variables when it calls the predict function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541c1ce0-bb8f-4d5d-95be-3b207bb1f855",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"prior trace expectation (set manually): \" + str(example_SiPS.ECM.beliefs))\n",
    "print(\"activation state: \"+ str(np.round(example_SiPS.ECM.trace_activations, decimals = 3)))\n",
    "example_SiPS.ECM.predict()\n",
    "print(\"new trace expectation: \" + str(example_SiPS.ECM.beliefs))\n",
    "print(\"new sensory expectation :\" + str([np.round(example_SiPS.ECM.expectations[i], decimals = 6) for i in range(len(example_SiPS.ECM.expectations)) if not example_SiPS.ECM.action_encoder[i]]))\n",
    "print(\"new action priming :\" + str([np.round(example_SiPS.ECM.expectations[i], decimals = 6) for i in range(len(example_SiPS.ECM.expectations)) if example_SiPS.ECM.action_encoder[i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b528b100-ea61-49a3-8300-e6de7c15deef",
   "metadata": {},
   "source": [
    "There are are few things to note here. \n",
    "\n",
    "First, the activation of the just-encoded memory trace goes nowhere because that trace has no forward connections in the memory matrix. One can think of this like the agent failing to make a prediction because it has no relevant previous experience. We artificially inflated the activation of this trace for the sake of our example, but because the just-encoded trace will normally have no expectation or excitation weight, it should draw very little of the agent's attention.\n",
    "\n",
    "Second, the just-encoded memory trace does not yet have a valence and therefor will not prime actions (the surprise advantage term is set to zero).\n",
    "\n",
    "Third, the effect is very small. This is related to my note about putting expectation and excitation states of memory traces in the exponent of the softmax function. For now, it can be addressed by giving the ECM high softmax value, but I have some more elegant solutions in mind.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9665e5-35de-4f53-8eb8-d4a749ab3412",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS.ECM.valences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a8647-da06-4993-9b58-d2fdbfa879ed",
   "metadata": {},
   "source": [
    "Let's create a new agent and run it for three steps to see how action priming works. We will look at the state of the trace activations after projection but before it makes predictions in the fourth step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12540e15-ab74-49b4-bedd-eaf27861de14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize\n",
    "example_SiPS2 = Situated_Agent(num_actions = 2, memory_capacity = 10, PS_softmax = 5) # start a new example\n",
    "#step 1\n",
    "observation = np.array([0])\n",
    "action = 0\n",
    "percept = example_SiPS2.percept_processor.get_percept(observation, action)\n",
    "example_SiPS2.ECM.deliberate(percept)\n",
    "#step 2. Different percept from step 1\n",
    "observation = np.array([1])\n",
    "action = 0\n",
    "percept = example_SiPS2.percept_processor.get_percept(observation, action)\n",
    "example_SiPS2.ECM.deliberate(percept)\n",
    "#step 3. Differnt percept from steps 1 and 2\n",
    "observation = np.array([0])\n",
    "action = 1\n",
    "percept = example_SiPS2.percept_processor.get_percept(observation, action)\n",
    "example_SiPS2.ECM.deliberate(percept)\n",
    "#step 4 (test). Same percept as step two - we should see expectation on trace three and associated percept nodes\n",
    "observation = np.array([1])\n",
    "action = 0\n",
    "percept = example_SiPS2.percept_processor.get_percept(observation, action)\n",
    "example_SiPS2.ECM.add_percept(percept)\n",
    "example_SiPS2.ECM.surprise = example_SiPS2.ECM.get_surprise(percept)\n",
    "example_SiPS2.ECM.excite_traces(percept)\n",
    "example_SiPS2.ECM.encode_trace(percept)\n",
    "example_SiPS2.ECM.activate()\n",
    "for deliberation_step in range(example_SiPS2.ECM.deliberation_length):\n",
    "    example_SiPS2.ECM.diffuse_activation()\n",
    "    \n",
    "print(example_SiPS2.ECM.trace_activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a8a3c6-9f06-4023-b382-96480ecc37a8",
   "metadata": {},
   "source": [
    "Before prediction, we can see that deliberation has moved the majority of the agent's attention to the second memory trace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820ea81-b8a7-46b3-8dd9-784470c1a66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS2.ECM.predict()\n",
    "print(\"trace expectations: \" + str(example_SiPS2.ECM.beliefs))\n",
    "print(\"percept expectations: \" + str(example_SiPS2.ECM.expectations))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab59406-f44c-4469-a35d-8904fd6522e2",
   "metadata": {},
   "source": [
    "Indeed, we can see that the new expectation of the third memory and the sensory representation with which it is associated (the third percept node) are higher than other nodes of the same type. Based on the priming of the actions (first two percept nodes), we can see that action zero (fisrts pecept node - associated with the third memory trace) has been inhibited. From this, we can infer that the valence assigned to memory trace three is slightly larger than average. Action 1 (second percept node), on the other hand, has been primed, despite the only trace with which it is associated having lower expectation. We can infer, then, that this trace has a much lower valence than other traces. Let's double check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948e5fbf-f6a3-47f4-a426-12fe6a24f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_SiPS2.ECM.valences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d851da-301d-4298-8960-b0ff54537553",
   "metadata": {},
   "source": [
    "Why do the traces have these valences? Look back at the order of percepts the agent received and let's consider again how prediction and suprise works. After the agent takes its first action, it has no expereince with which to predict the outcome. It thus has low expectations overall and is necessarily surprised by what it obsereves. After the agent takes its second action it has only one experience to consider; in the first time step it took the same action as its current action, then observed a one - so it adds expectation to observing a 1. But it observes a zero, so the outcome of its second action is even more surprising then its first. Finally, in its third time step, the agent takes action 1 for the first time (so it can't use this information to make predictions) but its first step has the same sensory state as its current one, and that was followed by a 1. The agent's deliberation thus adds expectation to sensory representation 1, and when the agent observes a 1 it is much less surprised then it ever has been before. Hence, the low valence and the priming of the action that it took (1) when that trace gains expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda1d4d8-3266-4993-8a1c-1c56d55d653a",
   "metadata": {},
   "source": [
    "### How Does SiPS Learn?\n",
    "\n",
    "Using only the properties described so far, SiPS agents *should* learn in certain environments with out the need for projcetive simulation or reinforcement. Specifically, a SiPS agent with a focus parameter of zero and no reinforcement of either reflexes or hebbian associations should learn to maximize the excitation of sensory reprsentations with intrinsic expectations if the state of the agent's sensors as it interacts with the environment can be described by a Markov Decision Process. Such a SiPS agent should still learn if its interactions with the environment can be described as a partially observable Markov Decision process, so long as reaching an expected sensory state does not require taking correct actions in *sequences* of aliased states.\n",
    "\n",
    "However, the issues described in notes regarding how attention is affected by the excitation and expectation states of memory traces prove fatal to effective learning.\n",
    "\n",
    "I have set up an environment for testing, below.\n",
    "\n",
    "The Delayed Response environment progresses in terms of trials. At the start of each trial, the agent is presented with a stimulus, which subsequently disappears. The agent must wait some number of steps before taking a correct action associated with the stimulus that was presented at the beginning of the trial. If it takes that action, it is presented with a \"reward\" before the next trial begins. If it chooses an action associted with a different stimulus, the next trial begins immediately without a \"reward\" being presented. We put reward in quotes here because this reward is not used to directly reinforce weights internal to the agent; it is simply an environmental state that is passed to the agent as part of its observation and for which it is assumed the SiPS agent has an intrinsic expectation. We first consider the case where the agent is presented with one of two possible stimuli, and must wait 1 step (no more than 10) before choosing an action (taking an action before the delay period does not cause the trial to end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933ecae8-0588-4fe5-aae1-66c37541204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize agent and environment\n",
    "test_env = Delayed_Response(W = 1, N = 2, max_trial_length=2)\n",
    "# The delayed response environment give a two-dimensional observation. \n",
    "# The first dimension gives the current stimulus (1-N on first step of trial, 0 thereafter)\n",
    "# The second dimension gives the reward availability (1 means the reward has been attained)\n",
    "# Thus, we need to give the agent an intrinsic expectation for \"1\" in the first index of its observation, established using the nested dictionary structure of the agents percept library\n",
    "test_SiPS = Situated_Agent(episode_ECM = Episodic_Memory(num_actions = 3, capacity = 100, intrinsic_expectations = {\"1\": {\"1\":2}}, softmax = 10, kappa = 4), \n",
    "                           reflex_softmax = 10,\n",
    "                           num_actions = 3) #agent will need three actions, one to wait and one for each test stimulus in the environment\n",
    "\n",
    "\n",
    "#set up experiment monitoring and data collection variables\n",
    "trial_num = 0\n",
    "reward_attained = False\n",
    "total_trials = 50\n",
    "action = None\n",
    "trial_data = np.empty(shape = (total_trials,4)) #each trial, will collect the presented stimulus, the selected action, the trial length, and wether the reward was attained\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e88dcb-7c3d-4d5e-92fe-aacf29864edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "while trial_num < total_trials:\n",
    "    \n",
    "    if test_env.state[\"trial_time\"] == 0: #check if new trial\n",
    "        trial_data[trial_num,0] = test_env.state[\"current_stimulus\"] #record which stimulus the agent was presented with\n",
    "        \n",
    "    observation = test_env.get_observation()\n",
    "    action = test_SiPS.get_action(observation)\n",
    "    #check if agent acted after wait period to collect data\n",
    "    if test_env.state[\"trial_time\"] >= test_env.W and not action == 0: \n",
    "        trial_data[trial_num,1] = action\n",
    "        trial_data[trial_num,2] = test_env.state[\"trial_time\"]\n",
    "        trial_data[trial_num,3] = test_env.state[\"rewarded_action\"] == action\n",
    "\n",
    "    test_env.transition(action)\n",
    "    #if transition starts new trial, increase counter (ends experiment if final trial)\n",
    "    if test_env.state[\"trial_time\"] == 0: #check if new trial\n",
    "        print(\"trial \" + str(trial_num) + \" results: \" + str(trial_data[trial_num,0:]))\n",
    "        trial_num += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e636b539-b979-46f0-b9f1-99b6422f1527",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP BY STEP TESTING\n",
    "\n",
    "# if test_env.state[\"trial_time\"] == 0: #check if new trial\n",
    "#     trial_data[trial_num,0] = test_env.state[\"current_stimulus\"] #record which stimulus the agent was presented with\n",
    "\n",
    "# print(test_env.state)\n",
    "# print(\"beliefs\")\n",
    "# print(test_SiPS.ECM.beliefs[0:20])\n",
    "# print(\"expectations\")\n",
    "# print(test_SiPS.ECM.expectations)\n",
    "\n",
    "# observation = test_env.get_observation()\n",
    "# action = test_SiPS.get_action(observation)\n",
    "# print(\"Action: \" + str(action))\n",
    "# #check if agent acted after wait period to collect data\n",
    "# if test_env.state[\"trial_time\"] >= test_env.W and not action == 0: \n",
    "#     trial_data[trial_num,1] = action\n",
    "#     trial_data[trial_num,2] = test_env.state[\"trial_time\"]\n",
    "#     trial_data[trial_num,3] = test_env.state[\"rewarded_action\"] == action\n",
    "\n",
    "# test_env.transition(action)\n",
    "# #if transition starts new trial, increase counter (ends experiment if final trial)\n",
    "# if test_env.state[\"trial_time\"] == 0: #check if new trial\n",
    "#     trial_num += 1\n",
    "\n",
    "\n",
    "\n",
    "# print(\"excitations\")\n",
    "# print(test_SiPS.ECM.trace_excitations[0:20])\n",
    "# print(\"activations\")\n",
    "# print(test_SiPS.ECM.trace_activations[0:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd6ab2f-d346-4184-aacf-735760586a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_SiPS.reflex_ECM.action_primes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b322d4-d25d-4a1f-a537-09ce9e53a063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07805f55-1417-4f07-9232-2f1985a35e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_SiPS.ECM.valences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e52843-bbae-46c7-9dc9-a6dea26feee3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
