{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train an RL agent, we need to have (i) an environment and (ii) a learning method. In this work, we define a foraging environment where the goal of the agent is to find as many targets as possible in a given time. We consider environments with non-destructive -or replenishable- targets, which we implement by displacing the agent a distance $l_\\textrm{c}$ from the center of the found target.\n",
    "\n",
    "As for the agent, we use Projective Simulation (PS) to model its decision making process and learning method. However, other algorithms that work with stochastic policies can also be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the classes that define the environment (`TargetEnv`), the forager dynamics (`Forager`), and its learning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from projective_simulation.agents.foraging import Forager\n",
    "from projective_simulation.envs.foraging import TargetEnv\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the class `Forager` as it currently is inherits the methods of a PS agent for decision making and learning. However, other learning algorithms can be directly implemented by changing this inheritance. The learning algorithm should contain a method for decision making, called `deliberate`, which inputs a state; and another one for updating the policy, called `learn`, which inputs a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the parameters defining the length of the episodes (number of RL steps) and the number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_EP = 200 #time steps per episode\n",
    "EPISODES = 1200 #number of episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment parameters\n",
    "Nt = 100 #number of targets\n",
    "L = 100 #world size\n",
    "r = 0.5 #target detection radius\n",
    "lc = np.array([[1.0],[1]]) #cutoff length\n",
    "\n",
    "#Initialize environment\n",
    "env = TargetEnv(Nt, L, r, lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the agent. As states, the agent perceives the value of an internal counter that keeps track of the number of small steps that it has performed without turning. The possible actions are continue walking in the same direction or turning. The agent performs a small step of length $d=1$ in any case after making a decision. Let's define the parameters of the PS forager agent and initialize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = 2 # continue in the same direction, turn\n",
    "SIZE_STATE_SPACE = np.array([TIME_EP]) # one state per value that the counter may possibly have within an episode.\n",
    "#--the last two entries are just placeholders here, but the code is general enough to implement ensembles of interacting agents that forage together.--\n",
    "GAMMA = 0.00001 #forgetting parameter in PS\n",
    "ETA_GLOW = 0.1 #glow damping parameter in PS\n",
    "\n",
    "#set a different initialization policy\n",
    "INITIAL_DISTR = np.ones((2, TIME_EP))\n",
    "INITIAL_DISTR[0, :] = 0.99\n",
    "INITIAL_DISTR[1, :] = 0.01\n",
    "    \n",
    "\n",
    "#Initialize agent\n",
    "agent = Forager(num_actions=NUM_ACTIONS,\n",
    "                size_state_space=SIZE_STATE_SPACE,\n",
    "                gamma_damping=GAMMA,\n",
    "                eta_glow_damping=ETA_GLOW,\n",
    "                initial_prob_distr=INITIAL_DISTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "478d44b2e57a4742ae20142199232b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for e in tqdm(range(EPISODES)):\n",
    "        \n",
    "    #restart environment and agent's counter and g matrix\n",
    "    env.init_env()\n",
    "    agent.agent_state = 0\n",
    "    agent.reset_g()\n",
    "\n",
    "    for t in range(TIME_EP):\n",
    "        \n",
    "        #step to set counter to its min. value n=1\n",
    "        if t == 0 or env.kicked[0]:\n",
    "            #do one step with random direction (no learning in this step)\n",
    "            env.update_pos(1)\n",
    "            #check boundary conditions\n",
    "            env.check_bc()\n",
    "            #reset counter\n",
    "            agent.agent_state = 0\n",
    "            #set kicked value to false again\n",
    "            env.kicked[0] = 0\n",
    "            \n",
    "        else:\n",
    "            #get perception\n",
    "            state = agent.get_state()\n",
    "            #decide\n",
    "            action = agent.deliberate(state)\n",
    "            #act (update counter)\n",
    "            agent.act(action)\n",
    "            \n",
    "            #update positions\n",
    "            env.update_pos(action)\n",
    "            #check if target was found + kick if it is\n",
    "            reward = env.check_encounter()\n",
    "                \n",
    "            #check boundary conditions\n",
    "            env.check_bc()\n",
    "            #learn\n",
    "            agent.learn(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For more details, please look at the `rl_opts` repository ([link](https://github.com/gorkamunoz/rl_opts)), which was developed for this project, and from which we inherited all functions. Shortly that library will be deprecated and everything will be run from `projective_simulation`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "ps"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
