{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train an RL agent, we need to have (i) an environment and (ii) a learning method. In this work, we define a foraging environment where the goal of the agent is to find as many targets as possible in a given time. We consider environments with non-destructive -or replenishable- targets, which we implement by displacing the agent a distance $l_\\textrm{c}$ from the center of the found target.\n",
    "\n",
    "As for the agent, we use Projective Simulation (PS) to model its decision making process and learning method. However, other algorithms that work with stochastic policies can also be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we import the classes that define the environment (`TargetEnv`), the forager dynamics (`Forager`), and its learning method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from projective_simulation.agents.foraging import Forager\n",
    "from projective_simulation.envs.foraging import TargetEnv\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the class `Forager` as it currently is inherits the methods of a PS agent for decision making and learning. However, other learning algorithms can be directly implemented by changing this inheritance. The learning algorithm should contain a method for decision making, called `deliberate`, which inputs a state; and another one for updating the policy, called `learn`, which inputs a reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up the parameters defining the length of the episodes (number of RL steps) and the number of episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_EP = 200 #time steps per episode\n",
    "EPISODES = 1200 #number of episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Environment parameters\n",
    "Nt = 100 #number of targets\n",
    "L = 100 #world size\n",
    "r = 0.5 #target detection radius\n",
    "lc = np.array([[1.0],[1]]) #cutoff length\n",
    "\n",
    "#Initialize environment\n",
    "env = TargetEnv(Nt, L, r, lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize the agent. As states, the agent perceives the value of an internal counter that keeps track of the number of small steps that it has performed without turning. The possible actions are continue walking in the same direction or turning. The agent performs a small step of length $d=1$ in any case after making a decision. Let's define the parameters of the PS forager agent and initialize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ACTIONS = 2 # continue in the same direction, turn\n",
    "SIZE_STATE_SPACE = np.array([TIME_EP]) # one state per value that the counter may possibly have within an episode.\n",
    "#--the last two entries are just placeholders here, but the code is general enough to implement ensembles of interacting agents that forage together.--\n",
    "GAMMA = 0.00001 #forgetting parameter in PS\n",
    "ETA_GLOW = 0.1 #glow damping parameter in PS\n",
    "\n",
    "#set a different initialization policy\n",
    "INITIAL_DISTR = np.ones((TIME_EP, 2))\n",
    "INITIAL_DISTR[:,0] = 0.99\n",
    "INITIAL_DISTR[:,1] = 0.01\n",
    "    \n",
    "\n",
    "#Initialize agent\n",
    "agent = Forager(num_actions=NUM_ACTIONS,\n",
    "                size_state_space=SIZE_STATE_SPACE,\n",
    "                gamma_damping=GAMMA,\n",
    "                eta_glow_damping=ETA_GLOW,\n",
    "                initial_prob_distr=INITIAL_DISTR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6b7735a5d5b40d7830d99a61db7e409",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 12 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m state \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_state()\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#decide\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mdeliberate(state)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m#act (update counter)\u001b[39;00m\n\u001b[1;32m     27\u001b[0m agent\u001b[38;5;241m.\u001b[39mact(action)\n",
      "File \u001b[0;32m~/github/projective_simulation/projective_simulation/agents/foraging.py:264\u001b[0m, in \u001b[0;36mForager.deliberate\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    262\u001b[0m     current_h_mat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_H_upd_single_percept(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN_upd_H, percept)\n\u001b[1;32m    263\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobability_distr(percept, h_matrix \u001b[38;5;241m=\u001b[39m current_h_mat)        \n\u001b[0;32m--> 264\u001b[0m action \u001b[38;5;241m=\u001b[39m rand_choice_nb(arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions), prob \u001b[38;5;241m=\u001b[39m probs)\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Update the G matrix for current (s,a) tuple\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_G_upd_single_percept(percept, action)\n",
      "File \u001b[0;32m~/github/projective_simulation/projective_simulation/agents/foraging.py:58\u001b[0m, in \u001b[0;36mrand_choice_nb\u001b[0;34m(arr, prob)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;129m@maybe_njit\u001b[39m()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrand_choice_nb\u001b[39m(arr, prob):\n\u001b[1;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    :param arr: A 1D numpy array of values to sample from.\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    :param prob: A 1D numpy array of probabilities for the given samples.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    :return: A random sample from the given array with a given probability.\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr[np\u001b[38;5;241m.\u001b[39msearchsorted(np\u001b[38;5;241m.\u001b[39mcumsum(prob), np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom(), side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mright\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[0;31mIndexError\u001b[0m: index 12 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "for e in tqdm(range(EPISODES)):\n",
    "        \n",
    "    #restart environment and agent's counter and g matrix\n",
    "    env.init_env()\n",
    "    agent.agent_state = 0\n",
    "    agent.reset_g()\n",
    "\n",
    "    for t in range(TIME_EP):\n",
    "        \n",
    "        #step to set counter to its min. value n=1\n",
    "        if t == 0 or env.kicked[0]:\n",
    "            #do one step with random direction (no learning in this step)\n",
    "            env.update_pos(1)\n",
    "            #check boundary conditions\n",
    "            env.check_bc()\n",
    "            #reset counter\n",
    "            agent.agent_state = 0\n",
    "            #set kicked value to false again\n",
    "            env.kicked[0] = 0\n",
    "            \n",
    "        else:\n",
    "            #get perception\n",
    "            state = agent.get_state()\n",
    "            #decide\n",
    "            action = agent.deliberate(state)\n",
    "            #act (update counter)\n",
    "            agent.act(action)\n",
    "            \n",
    "            #update positions\n",
    "            env.update_pos(action)\n",
    "            #check if target was found + kick if it is\n",
    "            reward = env.check_encounter()\n",
    "                \n",
    "            #check boundary conditions\n",
    "            env.check_bc()\n",
    "            #learn\n",
    "            agent.learn(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the code can directly accomodate environments with several agents that interact. For this reason, you will find methods in both the environment class `TargetEnv` and the forager class `Forager` that deal with agents that have visual cones and can perceive the presence of other agents in their surroundings. However, these features are not used in this work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reproduction of results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we explain how to reproduce the results of the paper that concern the training of RL agents in the foraging environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can run the training that is detailed above by means of the method ```learning```, which also saves the agent's memory periodically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, import ```learning```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.learn_and_bench import learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```learning``` inputs a configuration dictionary (config), a path to the folder where the results are saved (results_path) and the agent's identifier (run). Let us detail each input separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Configuration dictionary (config): it contains the parameters to initialize both the environment and the agent. For each set of parameters we ran, there is an identifier of the form \"exp_numconfig\" (e.g. exp_0) that uniquely identifies the config file. The config files for the experiments that give the results of the paper can be found in the directory 'configurations/learning/'. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the parameters that you can find in the config files:\n",
    "\n",
    "`NUM_TARGETS` : number of targets \\\n",
    "`WORLD_SIZE` : side of the square that defines the world (with periodic boundary conditions) \\\n",
    "`r` : target detection radius \\\n",
    "`lc` : cutoff length \\\n",
    "`MAX_STEP_L` : maximum value of the step counter (which coincides with the number of RL steps per episode) \\\n",
    "`NUM_BINS` : number of bins in which the state space is split. This is set to have one state per value of the counter \\\n",
    "`NUM_ACTIONS` : number of actions \\\n",
    "`GAMMA` : forgetting parameter $\\gamma$ in PS \\\n",
    "`ETA_GLOW` : glow damping parameter $\\eta_g$ in PS \\\n",
    "`PI_INIT` : policy initialization $\\pi_0$ ($\\forall n$). Note that it is given as $\\pi_0(\\uparrow|n)$ \\\n",
    "`NUM_EPISODES` : number of episodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We study foraging in enviroments with different cutoff lengths $l_\\textrm{c}$. Exp_0 corresponds to $l_\\textrm{c}=0.6$. Exp_1..10 correspond to $l_\\textrm{c}=1..10$, respectively. In experiments exp_0..10, the initialization policy is $\\pi_0(\\Rsh|n)=0.01$ $\\forall n$. Exp_11 and exp_12 correspond to experiments where the initialization policy is $\\pi_0(\\Rsh|n)=0.5$ $\\forall n$. Each experiment is run with 10 independent, different agents (run $\\in [0,9]$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, you can import the configuration from experiment exp_8 by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.utils import get_config\n",
    "\n",
    "config = get_config('exp_8.cfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can also define your own config dictionary with the parameters detailed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_config = {'NUM_TARGETS' : 100,\n",
    "             'WORLD_SIZE' : 100,\n",
    "             'r' : 0.5,\n",
    "             'lc' : 2,\n",
    "             'MAX_STEP_L' : 100,\n",
    "             'NUM_BINS' : 100,\n",
    "             'NUM_ACTIONS' : 2,\n",
    "             'GAMMA' : 0.00001,\n",
    "             'ETA_GLOW' : 0.1,\n",
    "             'PI_INIT' : 0.99,\n",
    "             'NUM_EPISODES' : 500}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Results path (results_path): Path where you want to save the results. The agent's memory (h matrix) is saved every 500 episodes on the file 'memory_agent...' (e.g. 'memory_agent_0_episode_500.npy')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_path = 'results/learning/test/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Agent's identifier (run): integer that identifies the agent. With this identifier, you can later retrieve the agent's memory or its performance (see the following section on Postlearning analysis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the inputs, you can run the learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning(my_config, results_path, run=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the training is finished, you can get the policy of the agent (as $\\pi(\\uparrow|n)$) at any of the episodes in which the memory was saved by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.utils import get_policy\n",
    "\n",
    "saved_policy = get_policy(results_path, run=0, training_episode=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: in the code, the policies are always given as $\\pi(\\uparrow|n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fig. 3 and Fig. 4 show the policies of the agents at the end of a training consisting of 12000 episodes of 20000 RL steps each. The policies can be retrieved with ```get_policy``` as detailed above, by setting ```training_episode = 12000``` and the corresponding agent identifier. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postlearning analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fairly compare the performance of the RL agents throughout the training with that of the benchmark models (Fig. 2), we need to run the same number of walks. In the training, the agent's policy changes from one episode to the next one, and taking the efficiency of just one episode -i.e. one walk- is not enough since we consider $10^4$ walks for the benchmark policies. Thus, we save the agent's policy at different stages of the training and then, in a postlearning analysis, we run $10^4$ walks with that frozen policy to get a more accurate evaluation of its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance analysis is done with the method ```agent_efficiency```, which is imported by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.learn_and_bench import agent_efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run it, you first need to define:\n",
    "\n",
    "1. The results path from where it retrieves the agent's memory at different stages of the training. Thus, it needs to be the same path where you saved the results of the training. The results of this analysis are also saved there.\n",
    "\n",
    "2. The configuration file you used to train the agent. To reproduce the results from Fig. 2, first get the corresponding config file as detailed in the previous section.\n",
    "\n",
    "3. The agent's identifier.\n",
    "\n",
    "4. The number of walks. To reproduce the results from Fig. 2, set this parameter to 10000.\n",
    "\n",
    "5. An episode interval. This function analyzes the performance of the agent at different stages of the training. To reproduce our results from Fig. 2, you should set this parameter to 2000, which means the performance is analyzed every 2000 episodes, until the end of the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do the postlearning analysis on the example of the previous section, you run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_efficiency(results_path, my_config, run=0, num_walks=100, episode_interval=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, this analysis is carried out by the method `walk_from_policy`, which inputs a policy (that is not changing) and runs the walks in parallel. It outputs a list with the efficiency achieved in each walk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find the results of, for example, the last episode, in the file 'performance_post_training_agent_0_episode_500.npy'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an array with the average performances (over the number of walks) of several agents throughout the training, you can run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_opts.utils import get_performance\n",
    "\n",
    "ag_list = [0] #in this example, we only ran one agent, but you can input here the identifiers of all the agents you ran.\n",
    "ep_list = [500] #get the performance at episode 500 of the agents in ag_list.\n",
    "\n",
    "av_performance, sem = get_performance(results_path, agent_list=ag_list, episode_list=ep_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "ps"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
