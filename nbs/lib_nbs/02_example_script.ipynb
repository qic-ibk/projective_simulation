{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d045a7-c45e-4d85-96da-b78d56262381",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_softmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[0;32m     15\u001b[0m     observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_observation()\n\u001b[1;32m---> 16\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     reward \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_reward(action)\n\u001b[0;32m     18\u001b[0m     agent\u001b[38;5;241m.\u001b[39mupdate(reward)\n",
      "File \u001b[1;32mc:\\users\\alexa\\documents\\projective_simulation\\projective_simulation\\agents\\basic_PS.py:48\u001b[0m, in \u001b[0;36mBasic_PSAgent.get_action\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03mGiven a percept, returns an action. For basic PS, these processess are mainly executed by the ECM's deliberate function\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     47\u001b[0m percept \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpercept_processor(observation)\n\u001b[1;32m---> 48\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mECM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeliberate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpercept\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[1;32mc:\\users\\alexa\\documents\\projective_simulation\\projective_simulation\\ECMs\\two_layer.py:63\u001b[0m, in \u001b[0;36mTwo_Layer.deliberate\u001b[1;34m(self, percept)\u001b[0m\n\u001b[0;32m     61\u001b[0m h_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhmatrix[percept_index]\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# get probabilities from h-values through a softmax function\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m prob \u001b[38;5;241m=\u001b[39m \u001b[43m_softmax\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax, h_values)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# get action\u001b[39;00m\n\u001b[0;32m     65\u001b[0m action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions), p\u001b[38;5;241m=\u001b[39mprob)        \n",
      "\u001b[1;31mNameError\u001b[0m: name '_softmax' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#projective simulation must be installed locally for the following imports to work\n",
    "from projective_simulation.agents.basic_PS import Basic_PSAgent\n",
    "from projective_simulation.environments.rlgl import RLGL\n",
    "\n",
    "#Number of steps to run simulation\n",
    "T = 100\n",
    "\n",
    "env = RLGL() #create a default red-light-green-light environment\n",
    "agent = Basic_PSAgent(num_actions = 2, glow = 1, damp = 0., softmax = 0.5) # create a basic PS agent with two actions\n",
    "data_log = [None] * T\n",
    "\n",
    "for t in range(T):\n",
    "    observation = env.get_observation()\n",
    "    action = agent.get_action(observation)\n",
    "    reward = env.get_reward(action)\n",
    "    agent.update(reward)\n",
    "    data_log[t] = {\"env_state\": env.state, \"action\": action, \"reward\": reward}\n",
    "    env.transition(action)\n",
    "\n",
    "plt.plot(range(T), [np.mean([data_log[step][\"reward\"] for step in range(i-10,i+1) if step >= 0]) for i in range(T)]) #plot a 10 step moving average of the reward       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
