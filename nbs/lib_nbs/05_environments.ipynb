{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6596b-bec3-426d-a1f7-a19307ea6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e3fe2-a50c-484a-b531-253024bf4940",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a72016-f636-4b71-93aa-2ec6e0a43efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133dfe8-d868-4dc5-a2df-bdb1914ab1c7",
   "metadata": {},
   "source": [
    "# Abstract Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14b271-d931-4907-a605-7f009d2956de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Abstract_Env(ABC):\n",
    "    \"\"\"A minimal Environment, every environment should be Derived from this class.\n",
    "\n",
    "    Examples:\n",
    "    >>> pass\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 state: object):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: an object that defines the state of the environment            \n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "\n",
    "    @abstractmethod\n",
    "    def transition(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: an action (or actions) to process\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        should determine and return an observation for an agent or agents as a function of self.state\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf478d5-759e-4ca2-8ae0-4b0824a6791b",
   "metadata": {},
   "source": [
    "# RLGL\n",
    "\n",
    "A description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff298852-78ee-4800-b426-4e4afbab6c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RLGL(Abstract_Env):\n",
    "    def __init__(self, state = 0, transition_matrix = None):\n",
    "        self.state = state\n",
    "        self.state_labels = {0: \"red\", 1: \"green\"}\n",
    "        if transition_matrix is None:\n",
    "            #create random uniform transition probabilities\n",
    "            transition_matrix = np.array([[0.5,0.5],[0.5,0.5]])\n",
    "        assert np.shape(transition_matrix) == (2,2)\n",
    "        self.transition_matrix = transition_matrix            \n",
    "\n",
    "    def transition(self, action):\n",
    "        '''\n",
    "        In this environment the agents action determines the reward but does not determine the state\n",
    "        '''\n",
    "        self.state = np.random.choice(range(2), p = self.transition_matrix[self.state,])\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.state_labels[self.state]\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        if action == self.state:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c7dcc-d445-4c40-8538-c731135075e1",
   "metadata": {},
   "source": [
    "A minimal example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed391df2-9f49-43f0-aa5a-ca12a675cb11",
   "metadata": {},
   "source": [
    "# Delayed Response\n",
    "\n",
    "The delayed response environments presents agents with a one stimulus from a set of stimuli of size N. Each stiumus is associated with a different correct response. However, taking the correct response immediately will have no effect - the agent must wait some number of steps W (wait time), during which no stimulus is available, before the correct action will produce a reward. There is no limit to the number steps the agent can wait before selecting a response, but after W steps any incorrect response will cause the environment to present a \"fail\" stimulus and then reset without reward. A maximum on the number of steps before a trial resets prevents trials from running forever if the agent learns to always wait (to avoid the surprising event of a new random stimulus, for example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44faed7c-36c8-43d9-8261-0042cab04992",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Delayed_Response(Abstract_Env):\n",
    "    def __init__(self, \n",
    "                 W: int, #number of steps agent must wait before response is evaluated\n",
    "                 N: int, #number of stimuli (and associated actions) that are presented by random selection to agent at start of trial\n",
    "                 max_trial_length: int = 10,\n",
    "                 current_stimulus: int = None, \n",
    "                 rewarded_action: int = None, \n",
    "                 reward_available: bool = False,\n",
    "                 trial_time: int = None\n",
    "                ):\n",
    "        super().__init__(state = {\"current_stimulus\": current_stimulus, \"rewarded_action\": rewarded_action, \"reward_available\": reward_available, \"trial_time\": trial_time}) #assigns state to self.state\n",
    "        self.W = W\n",
    "        self.N = N\n",
    "        self.max_trial_length = max_trial_length\n",
    "        assert self.max_trial_length >= self.W\n",
    "        if self.state[\"current_stimulus\"] is None:\n",
    "            self.state[\"current_stimulus\"] = np.random.randint(self.N) + 1  #this function randomly selects an interger from 0 to N-1. Add one because 0 is used to denote no stimulus\n",
    "        if self.state[\"rewarded_action\"] is None:\n",
    "            self.state[\"rewarded_action\"] = self.state[\"current_stimulus\"]  #stores which stimulus was presented, and thus the correct action for the agent. \n",
    "            #Generally, if the user imputs \"current_stimulus = 0\", \"rewarded_action\" should not be 0 or None. This won't break the code, but it will give reward for waiting after W steps, which the environment otherwise will not do \n",
    "        if self.state[\"trial_time\"] is None:\n",
    "            self.state[\"trial_time\"] = 0  #at trial time 0 the agent is presented with a stimulus. \n",
    "\n",
    "    def transition(self, action):\n",
    "        if self.state[\"reward_available\"] or self.state[\"current_stimulus\"] == -1: #reset after reward or fail\n",
    "            self.reset()\n",
    "            \n",
    "        #if agent acted, check if wait time has passed and if so, if action was correct\n",
    "        elif not action == 0: \n",
    "            if self.state[\"trial_time\"] < self.W:\n",
    "                self.step()\n",
    "            elif action == self.state[\"rewarded_action\"]: #correct action, give reward\n",
    "                self.state[\"reward_available\"] = True\n",
    "                self.step()\n",
    "            #incorrect action, set fail state\n",
    "            else:\n",
    "                self.state[\"current_stimulus\"] = -1\n",
    "                self.step()\n",
    "\n",
    "        #if agent did not act, continue trial\n",
    "        else:\n",
    "            self.step()      \n",
    "            \n",
    "    def step(self):\n",
    "        #moves the trial one step forward\n",
    "        if not self.state[\"current_stimulus\"] == -1: #unless in fail state, agent only receives stimulus on trial reset\n",
    "            self.state[\"current_stimulus\"] = 0\n",
    "        self.state[\"trial_time\"] += 1\n",
    "        if self.state[\"trial_time\"] > self.max_trial_length:\n",
    "            self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        #stats new trial, presenting agent with a new random stimulus\n",
    "        self.state[\"trial_time\"] = 0\n",
    "        self.state[\"current_stimulus\"] = np.random.randint(self.N) + 1\n",
    "        self.state[\"rewarded_action\"] = self.state[\"current_stimulus\"]\n",
    "        self.state[\"reward_available\"] = False\n",
    "\n",
    "    def get_observation(self):\n",
    "        return [self.state[\"current_stimulus\"], int(self.state[\"reward_available\"])] #agents gets current stimulus and reward availability as observation\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacdf8d3-7f3f-44ec-9da3-cdf70fc3cfa7",
   "metadata": {},
   "source": [
    "## Timed Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b77f0e-8bd0-493c-893b-240b3e14eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Timed_Response(Abstract_Env):\n",
    "    def __init__(self,\n",
    "                 delay: int = 1,\n",
    "                 start_state: int = 0):\n",
    "        super().__init__(state = start_state)\n",
    "        self.delay = delay\n",
    "\n",
    "    def transition(self, action):\n",
    "        if self.state == 0: #start state, no light no reward\n",
    "            self.state = 1 #always turn on light\n",
    "        elif self.state <= self.delay: #light on, delay not finished\n",
    "            if action == 1:\n",
    "                self.state = 0  #reset if push before delay is finished\n",
    "            else:\n",
    "                self.state += 1 #if wait, light stays on and delay progresses\n",
    "        elif self.state == self.delay + 1: #light on, delay finished\n",
    "            if action == 0:\n",
    "                self.state = 0 #reset if wait after delay is finished\n",
    "            else:\n",
    "                self.state += 1 #if push, move to reward state\n",
    "        elif self.state == self.delay + 2: #reward state\n",
    "            self.state = 1\n",
    "\n",
    "    def get_observation(self):\n",
    "        if self.state == 0:\n",
    "            return [0,0]\n",
    "        elif self.state <= self.delay + 1:\n",
    "            return [1,0]\n",
    "        else:\n",
    "            return [0,1]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87fef98-c7be-43df-afd3-fe26420ed7ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
