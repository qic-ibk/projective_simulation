{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6596b-bec3-426d-a1f7-a19307ea6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e3fe2-a50c-484a-b531-253024bf4940",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a72016-f636-4b71-93aa-2ec6e0a43efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133dfe8-d868-4dc5-a2df-bdb1914ab1c7",
   "metadata": {},
   "source": [
    "# Abstract Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14b271-d931-4907-a605-7f009d2956de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Abstract_Env(ABC):\n",
    "    \"\"\"A minimal Environment, every environment should be Derived from this class.\n",
    "\n",
    "    Examples:\n",
    "    >>> pass\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 state: object):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: an object that defines the state of the environment            \n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "\n",
    "    @abstractmethod\n",
    "    def transition(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: an action (or actions) to process\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        should determine and return an observation for an agent or agents as a function of self.state\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d3237a-7c1a-4694-ad17-f25cf12ec027",
   "metadata": {},
   "source": [
    "## Discrete Partially Observable Markov Decision Process\n",
    "\n",
    "Here, we define the general structure for a Partially Observable Markov Decision Process using the Projective Simulation Framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21e323d-2e97-4506-84d1-d211d0591b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class POMDP(Abstract_Env):\n",
    "    def __init__(self,\n",
    "                 percepts: np.ndarray,             #An SxK array where S is the number of Percepts and K is the number of categories for each percept. If 1d, converted to Sx1\n",
    "                 observation_function: np.ndarray, #An NxS array, where N is the number of states in the cycle and S is the number of possible percepts. Rows contain probability distributions\n",
    "                 transition_function: np.ndarray,  #An NxNxA array, where A is the number of actions. Rows in each slice contain probability distributions.\n",
    "                 initial_state: int = 0            #Start state of POMDP\n",
    "                ):\n",
    "        \"\"\"\n",
    "        assert that dimensions of input arguments align and assign input arguments to self\n",
    "        \"\"\"\n",
    "        \n",
    "        assert isinstance(percepts, np.ndarray)\n",
    "        if percepts.ndim == 1:\n",
    "            percepts = percepts[:,np.newaxis]\n",
    "        assert np.shape(percepts)[0] == np.shape(observation_function)[1]\n",
    "        assert np.shape(transition_function)[0] == np.shape(observation_function)[0]\n",
    "        assert np.shape(transition_function)[0] == np.shape(transition_function)[1]\n",
    "        assert transition_function.ndim == 3\n",
    "        assert np.isclose(np.sum(transition_function,axis =1), 1, atol=1e-9).all() #all rows sum to 1\n",
    "        assert np.isclose(np.sum(observation_function,axis =1), 1, atol=1e-9).all() #all rows sum to 1\n",
    "        assert initial_state in range(np.shape(observation_function)[0])\n",
    "         \n",
    "        self.percepts = percepts\n",
    "        self.observation_function = observation_function\n",
    "        self.transition_function = transition_function\n",
    "        state = initial_state\n",
    "        super().__init__(state = state)\n",
    "\n",
    "    def transition(self, action):\n",
    "        \"\"\"\n",
    "        randomly select a new state using transition probabilites from current state and action\n",
    "        \"\"\"\n",
    "        if not action in range(np.shape(self.transition_function)[2]):\n",
    "            raise ValueError(\"The action input for POMDP transition must be integer valued and within the scope of the transition function\")\n",
    "        transition_probs = self.transition_function[self.state,:, action]\n",
    "        self.state = np.random.choice(len(transition_probs), p = transition_probs)\n",
    "\n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        randomly select a percept using observation probabilites from current state\n",
    "        \"\"\"\n",
    "        percept_probs = self.observation_function[self.state,:]\n",
    "        percept_index = np.random.choice(len(percept_probs), p = percept_probs)\n",
    "        return self.percepts[percept_index,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9219469a-1adf-48b7-8591-d22ea4defe3f",
   "metadata": {},
   "source": [
    "### POMDP Example\n",
    "\n",
    "Here we define a simple Markov Decision Process in which an agent must match one of two actions to the first category of its percept. If it does, the environment moves to a state that produces an observable reward. If it does, the environment moves to a state without reward. Note that this environment is fully observable - each state maps to a unique percept. The POMDP class is general and can handle such a case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc03ae",
   "metadata": {},
   "source": [
    "## Invader Game"
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5bfac-0ead-4af8-ab70-4a649583d63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 90: percept = [1 1]; action = 1\n",
      "Step 91: percept = [1 1]; action = 1\n",
      "Step 92: percept = [1 1]; action = 1\n",
      "Step 93: percept = [0 1]; action = 0\n",
      "Step 94: percept = [1 1]; action = 1\n",
      "Step 95: percept = [1 1]; action = 1\n",
      "Step 96: percept = [1 1]; action = 1\n",
      "Step 97: percept = [1 1]; action = 1\n",
      "Step 98: percept = [0 1]; action = 0\n",
      "Step 99: percept = [1 1]; action = 1\n"
     ]
    }
   ],
   "source": [
    "#Create a percept for each combination of signal and reward\n",
    "percepts = np.array([[0,0],\n",
    "                     [0,1],\n",
    "                     [1,0],\n",
    "                     [1,1]]\n",
    "                   )\n",
    "#Create observation function for 4-state environment, each selects producing one of the perceprs\n",
    "observation_function = np.array([[1.,0.,0.,0.],\n",
    "                                 [0.,1.,0.,0.],\n",
    "                                 [0.,0.,1.,0.],\n",
    "                                 [0.,0.,0.,1.]]                                \n",
    "                               )\n",
    "#Create transition function for 4-state, 2-action environment, where transitions are uniform-random to states with the appropriate reward\n",
    "transition_function = np.array([[[0.,0.5], #in state 0, action 0 transition either to state 1 or 3. Action 1 transitions either to state 0 or 2.\n",
    "                                 [0.5,0.],\n",
    "                                 [0.,0.5],\n",
    "                                 [0.5,0.]],\n",
    "                                [[0.,0.5], #in state 1, action 0 transition either to state 1 or 3. Action 1 transitions either to state 0 or 2.\n",
    "                                 [0.5,0.],\n",
    "                                 [0.,0.5],\n",
    "                                 [0.5,0.]],\n",
    "                                [[0.5,0.], #in state 2, action 0 transition either to state 0 or 2. Action 1 transitions either to state 1 or 3.\n",
    "                                 [0.,0.5],\n",
    "                                 [0.5,0.],\n",
    "                                 [0.,0.5]],\n",
    "                                [[0.5,0.], #in state 3, action 0 transition either to state 0 or 2. Action 1 transitions either to state 1 or 3.\n",
    "                                 [0.,0.5],\n",
    "                                 [0.5,0.],\n",
    "                                 [0.,0.5]]]\n",
    "                              )\n",
    "#Initialize POMDP\n",
    "example_env = POMDP(percepts = percepts, observation_function = observation_function, transition_function = transition_function)\n",
    "\n",
    "#Initialize Agent\n",
    "from projective_simulation.agents import Basic_2Layer_Agent\n",
    "test_agent = Basic_2Layer_Agent(num_actions = np.shape(transition_function)[2], glow = 1., policy = 'softmax', policy_parameters = 1)\n",
    "\n",
    "#run percept action loop for 50 steps\n",
    "T = 100\n",
    "percepts = [None] * T #empty list for storing percepts\n",
    "actions = [None] * T  #empty list for storing actions\n",
    "for t in range(T):\n",
    "    percept = example_env.get_observation()\n",
    "    percepts[t] = percept\n",
    "    reward = percept[1]\n",
    "    test_agent.update(reward)\n",
    "    action = test_agent.deliberate(str(percept))\n",
    "    actions[t] = action\n",
    "    example_env.transition(action)\n",
    "\n",
    "\n",
    "#show final 10 percepts and actions. Agent should have learned to select actions that produce rewards\n",
    "for t in range(T-10,T):\n",
    "    print(f'Step {t}: percept = {percepts[t]}; action = {actions[t]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648aea3",
   "metadata": {},
   "source": [
    "Add a description"
   "id": "486754ff-e1ce-4846-b83f-11ce8bcf034c",
   "metadata": {},
   "source": [
    "### Prebuilt POMDPs\n",
    "\n",
    "#### Cyclic Environment\n",
    "\n",
    "This envirnoment defines a fixed sequence of percepts that may be passed to an agent. Primarily useful for testing predictive ECMs in Hidden Markov Processes with no observation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7be3c",
   "id": "ba9191cc-4621-474c-ad99-5f834b848421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Invader_Game_Env(Abstract_Env):\n",
    "    def __init__(self, \n",
    "                 state = None, \n",
    "                 transition_matrix = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: defines the direction of the sign of the invader, either right (0) or left (1)\n",
    "            transition_matrix: a 2x2 matrix with the probabilities of transitioning from one state to another.\n",
    "        If None, a random uniform transition matrix is created.  \n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        self.state_labels = {0: \"right\", 1: \"left\"}\n",
    "        #transition_matrix is a 2x2 matrix with the probabilities of transitioning from one state\n",
    "        if transition_matrix is None:\n",
    "            #create random uniform transition probabilities\n",
    "            transition_matrix = np.array([[0.5,0.5],[0.5,0.5]])\n",
    "        assert np.shape(transition_matrix) == (2,2)\n",
    "        self.transition_matrix = transition_matrix\n",
    "\n",
    "    def transition(self, action):\n",
    "        '''\n",
    "        In this environment the agents action determines the reward but does not determine the state.\n",
    "        '''\n",
    "        self.state = np.random.choice(range(2), p = self.transition_matrix[self.state,])\n",
    "\n",
    "    def get_observation(self):\n",
    "        if self.state is None:\n",
    "            self.state = np.random.choice(range(2))\n",
    "        return self.state_labels[self.state]\n",
    "    \n",
    "    def get_reward(self, action, liar = False):\n",
    "        \"\"\" Args:\n",
    "            action: the action taken by the agent\n",
    "            liar: if True, the agent is lying and the reward is always 0\n",
    "        \"\"\"\n",
    "        if not liar:\n",
    "            if action == self.state:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            if action != self.state:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0"
    "class Cyclic_Env(POMDP):\n",
    "    \"\"\"\n",
    "    An environment that cycles deterministically through a sequence of percepts that may be passed to an agent\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 percept_cycle: np.ndarray, #an N x K array, where N is the number of states in the cycles and K is the number of categories in a percept. If 1d, will be converted to Nx1\n",
    "                 initial_state: int = 0,\n",
    "                 supress_warning: bool = False\n",
    "                ):\n",
    "        '''\n",
    "        Sets up a determinstic cycles using the general format of a POMDP\n",
    "        '''\n",
    "        if percept_cycle.ndim == 1:\n",
    "            percept_cycle = percept_cycle[:, np.newaxis]\n",
    "        \n",
    "        # Get unique rows and, for each row of percept_cycle, the index of its unique representative\n",
    "        percepts, inverse = np.unique(percept_cycle, axis=0, return_inverse=True)\n",
    "        \n",
    "        # One-hot encode those indices to form the observation function\n",
    "        observation_function = np.eye(percepts.shape[0], dtype=int)[inverse]       \n",
    "\n",
    "        #create a cyclic shifted diagonal matrix\n",
    "        S = np.shape(percept_cycle)[0]\n",
    "        transition_function = np.roll(np.eye(N = S), shift = 1, axis = 1) \n",
    "        transition_function = transition_function[:,:,np.newaxis] #adds a dimension to allow for a single action\n",
    "        \n",
    "        super().__init__(percepts = percepts, observation_function = observation_function, transition_function = transition_function, initial_state = initial_state)\n",
    "        self.supress_warning = supress_warning\n",
    "        \n",
    "    def transition(self, action):\n",
    "        if not action == 0 and not self.supress_warning:\n",
    "            print('Warning: Cyclic_Env is not action mediated. Action input has been converted to 0')\n",
    "        action = 0\n",
    "        super().transition(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8ea93",
   "metadata": {},
   "source": [
    "## Grid World Environment\n",
    "\n",
    "Grid world is a navigation task in a 2D environment configurated as a grid, with obstacles. The goal is for the agent to reach a target at a certain position, which entails the agent exploring enough and remembering some of its past actions. Observations consist of the agent's coordinates and actions of the set of allowed displacements in the grid."
   "id": "0644659b-6c4e-4557-b203-88cf5981cf5c",
   "metadata": {},
   "source": [
    "##### Cyclic Environment Example\n",
    "In this example, we set up a cyclical environment in which a light turns green, turns off, turns blue, turns off, and then repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2557a867",
   "id": "b6b38655-fc22-4b86-9ce8-547eb499899e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percept_cycle = np.array([\"green\", \"off\", \"blue\", \"off\"])\n",
    "light_cycle_instance1 = Cyclic_Env(percept_cycle)\n",
    "T = 8 #total time steps to simulate\n",
    "observed_percepts = [\"None\"] * T #data structure for storing observations\n",
    "\n",
    "#simulate for T steps and store observations\n",
    "for t in range(T):\n",
    "    observed_percepts[t] = light_cycle_instance1.get_observation()\n",
    "    light_cycle_instance1.transition(0) #use action 0 for transition\n",
    "\n",
    "observed_percepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a92b1c-3bb5-48b4-a455-b3a9a450c60a",
   "metadata": {},
   "source": [
    "We can also choose to initiate anywhere in the cycle by giving the desired index. In this example, we start in State 2, which returns the \"blue\" percept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f35da0-0d0e-4e36-b709-dc11478ba33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light_cycle_instance2 = Cyclic_Env(percept_cycle, initial_state = 2)\n",
    "T = 8 #total time steps to simulate\n",
    "observed_percepts = [\"None\"] * T #data structure for storing observations\n",
    "\n",
    "#simulate for T steps and store observations\n",
    "for t in range(T):\n",
    "    observed_percepts[t] = light_cycle_instance2.get_observation()\n",
    "    light_cycle_instance2.transition(0) #use action 0 for transition\n",
    "    \n",
    "observed_percepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35a1e99-f302-4636-9915-83afe59bdda4",
   "metadata": {},
   "source": [
    "#### Noisy Cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad5be5e-9822-49cc-aaa9-8eef7d626e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Grid_World_Env(Abstract_Env):\n",
    "\n",
    "    def __init__(self, \n",
    "                 dimensions = (10, 10),\n",
    "                 N_obstacles = 10,\n",
    "                 obstacle_locations = None,\n",
    "                 reward_location = None,\n",
    "                 state = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimensions: a tuple defining the dimensions of the grid world (rows, columns)\n",
    "            N_obstacles: number of obstacles in the grid world\n",
    "            state: defines the current position of the agent in the grid world as a tuple (row, column)\n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        self.dimensions = dimensions\n",
    "        self.N_obstacles = N_obstacles\n",
    "\n",
    "        self.action_labels = {0: \"up\",\n",
    "                              1: \"down\", \n",
    "                              2: \"left\", \n",
    "                              3: \"right\"}\n",
    "        # Place the reward at a random location in the grid world\n",
    "        if reward_location is None:\n",
    "            reward_location = (np.random.randint(0, dimensions[0]), np.random.randint(0, dimensions[1]))\n",
    "        self.reward_location = reward_location\n",
    "\n",
    "        # Generate random obstacle locations\n",
    "        if obstacle_locations is None:\n",
    "            obstacle_locations = set()\n",
    "            while len(obstacle_locations) < N_obstacles:\n",
    "                loc = (np.random.randint(0, dimensions[0]), np.random.randint(0, dimensions[1]))\n",
    "                if loc != reward_location:  # Ensure the obstacle is not placed on the reward location\n",
    "                    obstacle_locations.add(loc)\n",
    "        self.obstacle_locations = list(obstacle_locations)\n",
    "    \n",
    "    def reset_position(self, initial_state=None):\n",
    "        \"\"\"\n",
    "        Reset the agent's position to a random location in the grid world.\n",
    "        \"\"\"\n",
    "        if initial_state is None:\n",
    "            self.state = (np.random.randint(0, self.dimensions[0]), np.random.randint(0, self.dimensions[1]))\n",
    "            while self.state in self.obstacle_locations:\n",
    "                self.state = (np.random.randint(0, self.dimensions[0]), np.random.randint(0, self.dimensions[1]))\n",
    "        else:\n",
    "            self.state = initial_state\n",
    "\n",
    "    def transition(self, action, periodic = False):\n",
    "        '''\n",
    "        Move the agent in the grid world based on the action taken.\n",
    "        '''\n",
    "        action = self.action_labels[action]\n",
    "        if periodic:\n",
    "            if action == \"right\":\n",
    "                new_state = ((self.state[0])%self.dimensions[0] + 1, self.state[1])\n",
    "            elif action == \"left\":\n",
    "                new_state = ((self.state[0]-1)%self.dimensions[0], self.state[1])\n",
    "            elif action == \"down\":\n",
    "                new_state = (self.state[0], (self.state[1]-1)%self.dimensions[1])\n",
    "            elif action == \"up\":\n",
    "                new_state = (self.state[0], (self.state[1]+1)%self.dimensions[1])\n",
    "        else:\n",
    "            if action == \"right\":\n",
    "                new_state = (min(self.state[0] + 1, self.dimensions[0] - 1), self.state[1])\n",
    "            elif action == \"left\":\n",
    "                new_state = (max(self.state[0] - 1, 0), self.state[1])\n",
    "            elif action == \"down\":\n",
    "                new_state = (self.state[0], max(self.state[1] - 1, 0))\n",
    "            elif action == \"up\":\n",
    "                new_state = (self.state[0], min(self.state[1] + 1, self.dimensions[1] - 1))\n",
    "        # Change the state only if the new state is not an obstacle\n",
    "        if new_state not in self.obstacle_locations:# and new_state != self.reward_location:\n",
    "            self.state = new_state\n",
    "\n",
    "    def get_observation(self):\n",
    "        return str(self.state)\n",
    "    \n",
    "    def get_reward(self):\n",
    "        if self.state == self.reward_location:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
    "class Noisy_Cycle(POMDP):\n",
    "    def __init__(self,\n",
    "                 percepts: np.ndarray,      #an SxK array where S is the number of Percepts and K is the number of categories for each percept\n",
    "                                            #if 1d, converted to Sx1\n",
    "                 observation_function: np.ndarray, #An NxS array, where N is the number of states in the cycle and S is the number of possible percepts\n",
    "                 initial_state: int = 0,\n",
    "                 supress_warning = False\n",
    "                ):\n",
    "        if percepts.ndim == 1:\n",
    "            percepts = percepts[:,np.newaxis]\n",
    "            \n",
    "        #create a cyclic shifted diagonal matrix for transition_function\n",
    "        S = np.shape(observation_function)[0] #number of percepts\n",
    "        transition_function = np.roll(np.eye(N = S), shift = 1, axis = 1)\n",
    "        transition_function = transition_function[:,:,np.newaxis] #adds a dimension to allow for a single action\n",
    "\n",
    "        super().__init__(percepts = percepts, observation_function = observation_function, transition_function = transition_function, initial_state = initial_state)\n",
    "        self.supress_warning = supress_warning\n",
    "        \n",
    "    def transition(self, action):\n",
    "        if not action == 0 and not self.supress_warning:\n",
    "            print('Warning: Cyclic_Env is not action mediated. Action input has been converted to 0')\n",
    "        action = 0\n",
    "        super().transition(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef9d33",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# nbdev export\n"
   "id": "1cc9d648-df06-42dd-a886-5fc27eee0857",
   "metadata": {},
   "source": [
    "##### Noisy Cycle Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fb60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
   "id": "639145b9-e124-41c4-86a7-240fe9f1c249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percepts = np.array([\"green\",\"off\",\"blue\"])\n",
    "observation_function = np.array([[0.9,0.,0.1],\n",
    "                                 [0.,1.,0.],\n",
    "                                 [0.1,0.,0.9],\n",
    "                                 [0.,1.,0.]])\n",
    "noisy_light_cycle = Noisy_Cycle(percepts, observation_function)\n",
    "T = 20 #total time steps to simulate\n",
    "observed_percepts = [\"None\"] * T #data structure for storing observations\n",
    "\n",
    "#simulate for T steps and store observations\n",
    "for t in range(T):\n",
    "    observed_percepts[t] = noisy_light_cycle.get_observation()\n",
    "    noisy_light_cycle.transition(0) #use action 0\n",
    "\n",
    "observed_percepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e95f7-55fc-4cd6-8ec8-249d4669be0e",
   "metadata": {},
   "source": [
    "## Causal Dynamic Bayesian Network\n",
    "This environment implements a specific kind of Dynamic Bayesian Network in which variables are not allowed to have parents in the same time step. This the state of all variables at a given time are conditionally independent given the state of the system in the previous time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3c1fe8-4eac-42ae-ab55-bf04e414849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import inspect\n",
    "class Causal_DBN(Abstract_Env):\n",
    "    def __init__(self,\n",
    "                 state: np.ndarray, #a one dimensional array. Each element gives the state of a variable.\n",
    "                 update_functions: dict, #a dictionary of the functions used to update each variable\n",
    "                 variable_names: np.ndarray = None, #an optional list of variables names. Default is integers. Must match keys of update functions\n",
    "                 action_variables: np.ndarray = None, #indicates which system variables are under the control of an agent. Used to ensure inputs to transition function are correct\n",
    "                 causal_network: np.ndarray = None #a square boolean array that indicates whether a variable at t is a parent of another variable at t+1. Optional: merely used to check transition function inputs\n",
    "                ):\n",
    "        if variable_names is None:\n",
    "            variable_names = np.array(range(self.num_variables))\n",
    "\n",
    "        #check variables\n",
    "        if not state.ndim == 1:\n",
    "            raise ValueError(\"'state' must be a numpy array with a single dimension\")\n",
    "        self.num_variables = np.shape(state)[0]\n",
    "\n",
    "        if causal_network is not None:\n",
    "            assert causal_network.dtype == np.bool_\n",
    "            if not np.shape(causal_network) == (self.num_variables, self.num_variables):\n",
    "                raise ValueError(\"causal network must be a square matrix with each dimension equal to the number of variables given by the state input\")\n",
    "\n",
    "        if action_variables is None:\n",
    "            action_variables = np.full(self.num_variables, fill_value = False)\n",
    "        for action_variable in variable_names[action_variables]:\n",
    "            update_functions[action_variable] = self.action_function        \n",
    "        for key, update_f in update_functions.items():\n",
    "            if not key in variable_names:\n",
    "                raise ValueError(\"Keys of update_function dictionary must correspond to variable names. Default variable names are integer indices\")\n",
    "            assert callable(update_f)            \n",
    "            ## Check that update function inputs match causal_network\n",
    "            if causal_network is not None:\n",
    "                function_parents = list(inspect.signature(update_f).parameters)\n",
    "                i = np.where(variable_names == key)[0][0] #get parent index (indexes get first match in first dimension)\n",
    "                if not set(function_parents) == set(variable_names[causal_network[:,i]]): #compare input variables names to children in DBN\n",
    "                    raise ValueError(f'The update function for {variable_names[i]} does not have input variables that match parents in causal_network')\n",
    "\n",
    "        if not len(update_functions) == self.num_variables:\n",
    "            raise ValueError(\"there must be an update function for each variable in 'state'\")\n",
    "\n",
    "\n",
    "        self.state = state\n",
    "        self.update_functions = update_functions\n",
    "        self.variable_names = variable_names\n",
    "        self.action_variables = action_variables\n",
    "\n",
    "    def transition(self, action: dict = None):\n",
    "        if action is None:\n",
    "            action = {}\n",
    "\n",
    "        #update action states\n",
    "        for key, value in action.items():\n",
    "            if not key in self.variable_names:\n",
    "                raise ValueError(\"keys of action dictionary must be environment variable names\")\n",
    "            self.state[self.variable_names == key] = value\n",
    "\n",
    "        #apply transition functions\n",
    "        new_states = np.zeros(self.num_variables, dtype = self.state.dtype)\n",
    "        for variable, update_f in self.update_functions.items():\n",
    "            required_args = set(inspect.signature(update_f).parameters.keys())\n",
    "            input_dict = {k: v for k, v in zip(self.variable_names, self.state) if k in required_args}\n",
    "            new_states[self.variable_names == variable] = update_f(**input_dict)\n",
    "        self.state = new_states\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.state\n",
    "    \n",
    "    def action_function(self, x): #as actions are given as input, the stored update functions for these variables should not do anything\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682565d-fba1-4b12-b2f3-5311734e54a0",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9710a-77a8-4f7b-8a2b-6266e857207c",
   "metadata": {},
   "source": [
    "In the following example, we create a Dynamic Bayesian Network with independant variables that take a state based on a bernoulli trial, and a third variable that is true if the first variables matched in the previous step and false if they did not. Note that because the variable states are mixed in type, numpy outmatically converts the state array to the highest order subtype. The Causal_DBN class is primarily meant to be used as a base class from which subclasses that implement specific DBNs can be built. These subclasses will typically have methods to map each element of the state variable to proper types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda83d8c-92fc-482b-803b-4bb292a28883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 0.])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.array((1.,2.,True))\n",
    "causal_network = np.array([[False,False,True],\n",
    "                           [False,False,True],\n",
    "                           [False,False,False]]\n",
    "                         )\n",
    "def Bernoulli():\n",
    "    return np.random.binomial(1,0.5)\n",
    "def Pair_Match(a,b):\n",
    "    return a == b\n",
    "update_functions = {\"a\": Bernoulli, \"b\": Bernoulli, \"test\": Pair_Match}\n",
    "variable_names = np.array([\"a\", \"b\", \"test\"])\n",
    "test_DBN = Causal_DBN(state, update_functions, variable_names, causal_network = causal_network)\n",
    "test_DBN.transition()\n",
    "test_DBN.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b580f-5ae2-4951-9476-d35f645ae2e8",
   "metadata": {},
   "source": [
    "### Light and Lever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d746cd5-01a9-4863-a2ee-864171c9559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Light_And_Lever(Causal_DBN):\n",
    "    def __init__(self, interval, state = None):\n",
    "        self.state_space = {\"light\": np.array((\"off\", \"green\", \"blue\")),\n",
    "                            \"lever\": np.array((\"unpressed\", \"pressed\")),\n",
    "                            \"reward_stimulus\": np.array((\"none\", \"food\", \"shock\")),\n",
    "                            \"timer\": np.array((range(2+interval*2))) #number of states in light cycle, on for green, on for blue, and one for each interval step between the two\n",
    "                           }\n",
    "        variable_names = np.array([\"light\", \"lever\", \"reward_stimulus\", \"timer\"])\n",
    "        if state is None:\n",
    "            state = np.array((1,0,0,0)) #default start state green light, unpressed lever, no reward, timer at 0\n",
    "        update_functions = {\"light\": self.update_light, \"lever\": self.bernoulli, \"reward_stimulus\": self.update_reward, \"timer\": self.update_timer}\n",
    "        super().__init__(state, update_functions, variable_names)\n",
    "        assert np.issubdtype(self.state.dtype, np.integer)\n",
    "        \n",
    "    def update_light(self, timer):\n",
    "        if timer == len(self.state_space[\"timer\"]) -1: #if timer is in last state . . .\n",
    "            return np.where(self.state_space[\"light\"] == \"green\")[0][0] #. . . light will turn green. 0 indices get first match in first dimension\n",
    "        elif timer == len(self.state_space[\"timer\"])/2 - 1: #if timer is one step from half-way . . .\n",
    "            return np.where(self.state_space[\"light\"] == \"blue\")[0][0] #. . . light will turn blue\n",
    "        else:\n",
    "            return np.where(self.state_space[\"light\"] == \"off\")[0][0]\n",
    "\n",
    "    def update_reward(self, light, lever):\n",
    "        if self.state_space[\"light\"][light] == \"green\" and self.state_space[\"lever\"][lever] == \"pressed\":\n",
    "            return np.where(self.state_space[\"reward_stimulus\"] == \"food\")[0][0]\n",
    "        elif self.state_space[\"light\"][light] == \"blue\" and self.state_space[\"lever\"][lever] == \"pressed\":\n",
    "            return np.where(self.state_space[\"reward_stimulus\"] == \"shock\")[0][0]\n",
    "        else:\n",
    "            return np.where(self.state_space[\"reward_stimulus\"] == \"none\")[0][0]\n",
    "\n",
    "    def update_timer(self, timer):\n",
    "        reset_at = len(self.state_space[\"timer\"])\n",
    "        return (timer + 1) % reset_at\n",
    "\n",
    "    @staticmethod\n",
    "    def bernoulli():\n",
    "        return int(np.random.binomial(1, 0.5)) #1 trial, 50 percept probability 1.\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.state[np.array((0,1,2))] #return tuple of light, lever, and reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395ea05-5707-4f5e-821a-beab73241af0",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f6ff85-a770-4273-bf72-f39844ae5d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n",
      "[0 0 0]\n",
      "[0 1 0]\n",
      "[2 0 0]\n",
      "[0 1 0]\n",
      "[0 0 0]\n",
      "[1 1 0]\n",
      "[0 0 1]\n",
      "[0 1 0]\n",
      "[2 1 0]\n"
     ]
    }
   ],
   "source": [
    "test_env = Light_And_Lever(interval = 2)\n",
    "for t in range(10):\n",
    "    print(test_env.get_observation())\n",
    "    test_env.transition()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1183752b-a80d-43e2-8064-66617a80e277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env.update_reward(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1c67f-46de-4e57-bc46-3e4d3d2d55d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
