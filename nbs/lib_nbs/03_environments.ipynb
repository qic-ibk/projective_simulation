{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6596b-bec3-426d-a1f7-a19307ea6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e3fe2-a50c-484a-b531-253024bf4940",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a72016-f636-4b71-93aa-2ec6e0a43efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133dfe8-d868-4dc5-a2df-bdb1914ab1c7",
   "metadata": {},
   "source": [
    "# Abstract Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14b271-d931-4907-a605-7f009d2956de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Abstract_Env(ABC):\n",
    "    \"\"\"A minimal Environment, every environment should be Derived from this class.\n",
    "\n",
    "    Examples:\n",
    "    >>> pass\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 state: object):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: an object that defines the state of the environment            \n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "\n",
    "    @abstractmethod\n",
    "    def transition(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: an action (or actions) to process\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        should determine and return an observation for an agent or agents as a function of self.state\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486754ff-e1ce-4846-b83f-11ce8bcf034c",
   "metadata": {},
   "source": [
    "# Cyclic Environment\n",
    "\n",
    "This envirnoment defines a fixed sequence of percepts that may be passed to an agent. Primarily useful for testing predictive ECMs in Hidden Markov Processes with no observation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9191cc-4621-474c-ad99-5f834b848421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Cyclic_Env(Abstract_Env):\n",
    "    \"\"\"\n",
    "    An environment that cycles deterministically through a sequence of percepts that may be passed to an agent\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 percept_cycle: np.ndarray,\n",
    "                 initial_state: int = 0):\n",
    "        self.percept_cycle = percept_cycle\n",
    "        state = initial_state\n",
    "        super().__init__(state = state)\n",
    "\n",
    "    def transition(self):\n",
    "        \"\"\"\n",
    "        This environment has deterministic transitions and does not take actions as input\n",
    "        \"\"\"\n",
    "        self.state = (self.state + 1) % len(self.percept_cycle)\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.percept_cycle[self.state:self.state+1] #slicing returns array instead of scalar    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644659b-6c4e-4557-b203-88cf5981cf5c",
   "metadata": {},
   "source": [
    "## Example\n",
    "In this example, we set up a cyclical environment in which a light turns green, turns off, turns blue, turns off, and then repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b38655-fc22-4b86-9ce8-547eb499899e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percept_cycle = np.array([\"green\", \"off\", \"blue\", \"off\"])\n",
    "light_cycle_instance1 = Cyclic_Env(percept_cycle)\n",
    "T = 8 #total time steps to simulate\n",
    "observed_percepts = [\"None\"] * T #data structure for storing observations\n",
    "\n",
    "#simulate for T steps and store observations\n",
    "for t in range(T):\n",
    "    observed_percepts[t] = light_cycle_instance1.get_observation()\n",
    "    light_cycle_instance1.transition()\n",
    "\n",
    "observed_percepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a92b1c-3bb5-48b4-a455-b3a9a450c60a",
   "metadata": {},
   "source": [
    "We can also choose to initiate anywhere in the cycle by giving the desired index. In this example, we start in State 2, which returns the \"blue\" percept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f35da0-0d0e-4e36-b709-dc11478ba33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5')]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light_cycle_instance2 = Cyclic_Env(percept_cycle, initial_state = 2)\n",
    "T = 8 #total time steps to simulate\n",
    "observed_percepts = [\"None\"] * T #data structure for storing observations\n",
    "\n",
    "#simulate for T steps and store observations\n",
    "for t in range(T):\n",
    "    observed_percepts[t] = light_cycle_instance2.get_observation()\n",
    "    light_cycle_instance2.transition()\n",
    "    \n",
    "observed_percepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf478d5-759e-4ca2-8ae0-4b0824a6791b",
   "metadata": {},
   "source": [
    "# RLGL\n",
    "\n",
    "A description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff298852-78ee-4800-b426-4e4afbab6c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RLGL(Abstract_Env):\n",
    "    def __init__(self, state = 0, transition_matrix = None):\n",
    "        self.state = state\n",
    "        self.state_labels = {0: \"red\", 1: \"green\"}\n",
    "        if transition_matrix is None:\n",
    "            #create random uniform transition probabilities\n",
    "            transition_matrix = np.array([[0.5,0.5],[0.5,0.5]])\n",
    "        assert np.shape(transition_matrix) == (2,2)\n",
    "        self.transition_matrix = transition_matrix            \n",
    "\n",
    "    def transition(self, action):\n",
    "        '''\n",
    "        In this environment the agents action determines the reward but does not determine the state\n",
    "        '''\n",
    "        self.state = np.random.choice(range(2), p = self.transition_matrix[self.state,])\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.state_labels[self.state]\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        if action == self.state:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c7dcc-d445-4c40-8538-c731135075e1",
   "metadata": {},
   "source": [
    "A minimal example"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
