{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30b6596b-bec3-426d-a1f7-a19307ea6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp environments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9e3fe2-a50c-484a-b531-253024bf4940",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7a72016-f636-4b71-93aa-2ec6e0a43efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133dfe8-d868-4dc5-a2df-bdb1914ab1c7",
   "metadata": {},
   "source": [
    "# Abstract Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db14b271-d931-4907-a605-7f009d2956de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Abstract_Env(ABC):\n",
    "    \"\"\"A minimal Environment, every environment should be Derived from this class.\n",
    "\n",
    "    Examples:\n",
    "    >>> pass\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 state: object):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: an object that defines the state of the environment            \n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "\n",
    "    @abstractmethod\n",
    "    def transition(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: an action (or actions) to process\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        should determine and return an observation for an agent or agents as a function of self.state\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486754ff-e1ce-4846-b83f-11ce8bcf034c",
   "metadata": {},
   "source": [
    "# Cyclic Environment\n",
    "\n",
    "This envirnoment defines a fixed sequence of percepts that may be passed to an agent. Primarily useful for testing predictive ECMs in Hidden Markov Processes with no observation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba9191cc-4621-474c-ad99-5f834b848421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Cyclic_Env(Abstract_Env):\n",
    "    \"\"\"\n",
    "    An environment that cycles deterministically through a sequence of percepts that may be passed to an agent\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 percept_cycle: np.ndarray,\n",
    "                 initial_state: int = 0):\n",
    "        self.percept_cycle = percept_cycle\n",
    "        state = initial_state\n",
    "        super().__init__(state = state)\n",
    "\n",
    "    def transition(self):\n",
    "        \"\"\"\n",
    "        This environment has deterministic transitions and does not take actions as input\n",
    "        \"\"\"\n",
    "        self.state = (self.state + 1) % len(self.percept_cycle)\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.percept_cycle[self.state:self.state+1] #slicing returns array instead of scalar    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644659b-6c4e-4557-b203-88cf5981cf5c",
   "metadata": {},
   "source": [
    "## Example\n",
    "In this example, we set up a cyclical environment in which a light turns green, turns off, turns blue, turns off, and then repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6b38655-fc22-4b86-9ce8-547eb499899e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percept_cycle = np.array([\"green\", \"off\", \"blue\", \"off\"])\n",
    "light_cycle_instance1 = Cyclic_Env(percept_cycle)\n",
    "T = 8 #total time steps to simulate\n",
    "observed_percepts = [\"None\"] * T #data structure for storing observations\n",
    "\n",
    "#simulate for T steps and store observations\n",
    "for t in range(T):\n",
    "    observed_percepts[t] = light_cycle_instance1.get_observation()\n",
    "    light_cycle_instance1.transition()\n",
    "\n",
    "observed_percepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a92b1c-3bb5-48b4-a455-b3a9a450c60a",
   "metadata": {},
   "source": [
    "We can also choose to initiate anywhere in the cycle by giving the desired index. In this example, we start in State 2, which returns the \"blue\" percept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68f35da0-0d0e-4e36-b709-dc11478ba33f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['blue'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5'),\n",
       " array(['green'], dtype='<U5'),\n",
       " array(['off'], dtype='<U5')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "light_cycle_instance2 = Cyclic_Env(percept_cycle, initial_state = 2)\n",
    "T = 8 #total time steps to simulate\n",
    "observed_percepts = [\"None\"] * T #data structure for storing observations\n",
    "\n",
    "#simulate for T steps and store observations\n",
    "for t in range(T):\n",
    "    observed_percepts[t] = light_cycle_instance2.get_observation()\n",
    "    light_cycle_instance2.transition()\n",
    "    \n",
    "observed_percepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf478d5-759e-4ca2-8ae0-4b0824a6791b",
   "metadata": {},
   "source": [
    "# RLGL\n",
    "\n",
    "A description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff298852-78ee-4800-b426-4e4afbab6c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RLGL(Abstract_Env):\n",
    "    def __init__(self, state = 0, transition_matrix = None):\n",
    "        self.state = state\n",
    "        self.state_labels = {0: \"red\", 1: \"green\"}\n",
    "        if transition_matrix is None:\n",
    "            #create random uniform transition probabilities\n",
    "            transition_matrix = np.array([[0.5,0.5],[0.5,0.5]])\n",
    "        assert np.shape(transition_matrix) == (2,2)\n",
    "        self.transition_matrix = transition_matrix            \n",
    "\n",
    "    def transition(self, action):\n",
    "        '''\n",
    "        In this environment the agents action determines the reward but does not determine the state\n",
    "        '''\n",
    "        self.state = np.random.choice(range(2), p = self.transition_matrix[self.state,])\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.state_labels[self.state]\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        if action == self.state:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c7dcc-d445-4c40-8538-c731135075e1",
   "metadata": {},
   "source": [
    "A minimal example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48e95f7-55fc-4cd6-8ec8-249d4669be0e",
   "metadata": {},
   "source": [
    "# Causal Dynamic Bayesian Network\n",
    "This environment implements a specific kind of Dynamic Bayesian Network in which variables are not allowed to have parents in the same time step. This the state of all variables at a given time are conditionally independent given the state of the system in the previous time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bd3c1fe8-4eac-42ae-ab55-bf04e414849c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "import inspect\n",
    "class Causal_DBN(Abstract_Env):\n",
    "    def __init__(self,\n",
    "                 state: np.ndarray, #a one dimensional array. Each element gives the state of a variable.\n",
    "                 update_functions: dict, #a dictionary of the functions used to update each variable\n",
    "                 variable_names: np.ndarray = None, #an optional list of variables names. Default is integers. Must match keys of update functions\n",
    "                 action_variables: np.ndarray = None, #indicates which system variables are under the control of an agent. Used to ensure inputs to transition function are correct\n",
    "                 causal_network: np.ndarray = None #a square boolean array that indicates whether a variable at t is a parent of another variable at t+1. Optional: merely used to check transition function inputs\n",
    "                ):\n",
    "        if variable_names is None:\n",
    "            variable_names = np.array(range(self.num_variables))\n",
    "\n",
    "        #check variables\n",
    "        if not state.ndim == 1:\n",
    "            raise ValueError(\"'state' must be a numpy array with a single dimension\")\n",
    "        self.num_variables = np.shape(state)[0]\n",
    "\n",
    "        if causal_network is not None:\n",
    "            assert causal_network.dtype == np.bool_\n",
    "            if not np.shape(causal_network) == (self.num_variables, self.num_variables):\n",
    "                raise ValueError(\"causal network must be a square matrix with each dimension equal to the number of variables given by the state input\")\n",
    "\n",
    "        if action_variables is None:\n",
    "            action_variables = np.full(self.num_variables, fill_value = False)\n",
    "        for action_variable in variable_names[action_variables]:\n",
    "            update_functions[action_variable] = self.action_function        \n",
    "        for key, update_f in update_functions.items():\n",
    "            if not key in variable_names:\n",
    "                raise ValueError(\"Keys of update_function dictionary must correspond to variable names. Default variable names are integer indices\")\n",
    "            assert callable(update_f)            \n",
    "            ## Check that update function inputs match causal_network\n",
    "            if causal_network is not None:\n",
    "                function_parents = list(inspect.signature(update_f).parameters)\n",
    "                i = np.where(variable_names == key)[0][0] #get parent index (indexes get first match in first dimension)\n",
    "                if not set(function_parents) == set(variable_names[causal_network[:,i]]): #compare input variables names to children in DBN\n",
    "                    raise ValueError(f'The update function for {variable_names[i]} does not have input variables that match parents in causal_network')\n",
    "\n",
    "        if not len(update_functions) == self.num_variables:\n",
    "            raise ValueError(\"there must be an update function for each variable in 'state'\")\n",
    "\n",
    "\n",
    "        self.state = state\n",
    "        self.update_functions = update_functions\n",
    "        self.variable_names = variable_names\n",
    "        self.action_variables = action_variables\n",
    "\n",
    "    def transition(self, action: dict = None):\n",
    "        if action is None:\n",
    "            action = {}\n",
    "\n",
    "        #update action states\n",
    "        for key, value in action.items():\n",
    "            if not key in self.variable_names:\n",
    "                raise ValueError(\"keys of action dictionary must be environment variable names\")\n",
    "            self.state[self.variable_names == key] = value\n",
    "\n",
    "        #apply transition functions\n",
    "        new_states = np.zeros(self.num_variables, dtype = self.state.dtype)\n",
    "        for variable, update_f in self.update_functions.items():\n",
    "            required_args = set(inspect.signature(update_f).parameters.keys())\n",
    "            input_dict = {k: v for k, v in zip(self.variable_names, self.state) if k in required_args}\n",
    "            new_states[self.variable_names == variable] = update_f(**input_dict)\n",
    "        self.state = new_states\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.state\n",
    "    \n",
    "    def action_function(self, x): #as actions are given as input, the stored update functions for these variables should not do anything\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2682565d-fba1-4b12-b2f3-5311734e54a0",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e9710a-77a8-4f7b-8a2b-6266e857207c",
   "metadata": {},
   "source": [
    "In the following example, we create a Dynamic Bayesian Network with independant variables that take a state based on a bernoulli trial, and a third variable that is true if the first variables matched in the precious step and false if they did not. Note that because the variable states are mixed in type, numpy outmatically converts the state array to the highest order subtype. The Causal_DBN class is primarily meant to be used as a base class from which subclasses that implement specific DBNs can be built. These subclasses will typically have methods to map each element of the state variable to proper types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fda83d8c-92fc-482b-803b-4bb292a28883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0.])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = np.array((1.,2.,True))\n",
    "causal_network = np.array([[False,False,True],\n",
    "                           [False,False,True],\n",
    "                           [False,False,False]]\n",
    "                         )\n",
    "def Bernoulli():\n",
    "    return np.random.binomial(1,0.5)\n",
    "def Pair_Match(a,b):\n",
    "    return a == b\n",
    "update_functions = {\"a\": Bernoulli, \"b\": Bernoulli, \"test\": Pair_Match}\n",
    "variable_names = np.array([\"a\", \"b\", \"test\"])\n",
    "test_DBN = Causal_DBN(state, update_functions, variable_names, causal_network = causal_network)\n",
    "test_DBN.transition()\n",
    "test_DBN.state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682b580f-5ae2-4951-9476-d35f645ae2e8",
   "metadata": {},
   "source": [
    "### Light and Lever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "1d746cd5-01a9-4863-a2ee-864171c9559c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Light_And_Lever(Causal_DBN):\n",
    "    def __init__(self, interval, state = None):\n",
    "        self.state_space = {\"light\": np.array((\"off\", \"green\", \"blue\")),\n",
    "                            \"lever\": np.array((\"unpressed\", \"pressed\")),\n",
    "                            \"reward_stimulus\": np.array((\"none\", \"food\", \"shock\")),\n",
    "                            \"timer\": np.array((range(2+interval*2))) #number of states in light cycle, on for green, on for blue, and one for each interval step between the two\n",
    "                           }\n",
    "        variable_names = np.array([\"light\", \"lever\", \"reward_stimulus\", \"timer\"])\n",
    "        if state is None:\n",
    "            state = np.array((1,0,0,0)) #default start state green light, unpressed lever, no reward, timer at 0\n",
    "        update_functions = {\"light\": self.update_light, \"lever\": self.bernoulli, \"reward_stimulus\": self.update_reward, \"timer\": self.update_timer}\n",
    "        super().__init__(state, update_functions, variable_names)\n",
    "        assert np.issubdtype(self.state.dtype, np.integer)\n",
    "        \n",
    "    def update_light(self, timer):\n",
    "        if timer == len(self.state_space[\"timer\"]) -1: #if timer is in last state . . .\n",
    "            return np.where(self.state_space[\"light\"] == \"green\")[0][0] #. . . light will turn green. 0 indices get first match in first dimension\n",
    "        elif timer == len(self.state_space[\"timer\"])/2 - 1: #if timer is one step from half-way . . .\n",
    "            return np.where(self.state_space[\"light\"] == \"blue\")[0][0] #. . . light will turn blue\n",
    "        else:\n",
    "            return np.where(self.state_space[\"light\"] == \"off\")[0][0]\n",
    "\n",
    "    def update_reward(self, light, lever):\n",
    "        if self.state_space[\"light\"][light] == \"green\" and self.state_space[\"lever\"][lever] == \"pressed\":\n",
    "            return np.where(self.state_space[\"reward_stimulus\"] == \"food\")[0][0]\n",
    "        elif self.state_space[\"light\"][light] == \"blue\" and self.state_space[\"lever\"][lever] == \"pressed\":\n",
    "            return np.where(self.state_space[\"reward_stimulus\"] == \"shock\")[0][0]\n",
    "        else:\n",
    "            return np.where(self.state_space[\"reward_stimulus\"] == \"none\")[0][0]\n",
    "\n",
    "    def update_timer(self, timer):\n",
    "        reset_at = len(self.state_space[\"timer\"])\n",
    "        return (timer + 1) % reset_at\n",
    "\n",
    "    @staticmethod\n",
    "    def bernoulli():\n",
    "        return int(np.random.binomial(1, 0.5)) #1 trial, 50 percept probability 1.\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.state[np.array((0,1,2))] #return tuple of light, lever, and reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3395ea05-5707-4f5e-821a-beab73241af0",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e0f6ff85-a770-4273-bf72-f39844ae5d33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 0]\n",
      "[0 1 0]\n",
      "[0 0 0]\n",
      "[2 0 0]\n",
      "[0 0 0]\n",
      "[0 0 0]\n",
      "[1 0 0]\n",
      "[0 0 0]\n",
      "[0 0 0]\n",
      "[2 1 0]\n"
     ]
    }
   ],
   "source": [
    "test_env = Light_And_Lever(interval = 2)\n",
    "for t in range(10):\n",
    "    print(test_env.get_observation())\n",
    "    test_env.transition()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1183752b-a80d-43e2-8064-66617a80e277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_env.update_reward(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea1c67f-46de-4e57-bc46-3e4d3d2d55d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
