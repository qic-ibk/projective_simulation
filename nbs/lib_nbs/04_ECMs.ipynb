{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c27c4-41b7-4367-814f-1ac318d674fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp ECMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5475b4-a17f-44f1-ba9c-3ace43286a47",
   "metadata": {},
   "source": [
    "## Abstract ECM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea1d597-03d0-481a-9278-3570ab8ee12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "from sys import version_info\n",
    "import projective_simulation.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if version_info >= (3, 4):  # compatibility\n",
    "    from abc import ABC, abstractmethod\n",
    "    ABC = ABC\n",
    "else:\n",
    "    from abc import ABCMeta, abstractmethod\n",
    "    ABC = ABCMeta('ABC', (), {})\n",
    "\n",
    "class ECM(ABC):\n",
    "    \"\"\"A minimal ECM, every agent should be Derived from this class. Primarily serves to enforce that all ECMs have the \"ECM\" class\n",
    "\n",
    "    Examples:\n",
    "    >>> pass\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_actions: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            ECM: The ECM Object to use\n",
    "            percept_processor: An optional object for transforming observations prior to passing to ECM as a percept. Must have method \"preprocess\"\n",
    "            action_processor: An optional object for transforming actions prior to passing to Environment as an actuator state. Must have method \"postprocess\"            \n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def deliberate(self, percept: str):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            percept: A string corresponding to an existing or new (will be added) key in the ECM percept dictionary\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a93a56c-8778-4f41-93c7-6e917f669250",
   "metadata": {},
   "source": [
    "## Two Layer ECMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97738bf4-5dc1-4b6d-b893-c9057675e02f",
   "metadata": {},
   "source": [
    "### Basic Two Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deeec79-d8e1-451f-a275-c01f306a184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Two_Layer(ECM):\n",
    "    def __init__(self, \n",
    "                 num_actions: int, # The number of available actions.\n",
    "                 glow: float, # The glow (or eta) parameter. \n",
    "                 damp: float, # The damping (or gamma) parameter. \n",
    "                 softmax: float # The softmax (or beta) parameter.\n",
    "                ):\n",
    "\n",
    "        \"\"\"\n",
    "        Simple, 2-layered ECM. We initialize an h-matrix with a single row of `num_actions` \n",
    "        entries corresponding to a dummy percept clip being connected to all possible actions with h-values of all 1. We \n",
    "        initialize a g-matrix with a single row of `num_actions` entries with all 0s corresponding to the *glow* values \n",
    "        of percept-action transitions.\n",
    "\n",
    "        percepts must be created from new observations with a preprocessor, e.g. add_percepts\n",
    "                      \n",
    "        NOTE: This simple version misses some features such as clip deletion, emotion tags or generalization mechanisms.\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.glow = glow\n",
    "        self.damp = damp\n",
    "        self.softmax = softmax\n",
    "        #int: current number of percepts.\n",
    "        self.num_percepts = 0\n",
    "        #np.ndarray: h-matrix with current h-values. Defaults to all 1.\n",
    "        self.hmatrix = np.ones([1,self.num_actions])\n",
    "        #np.ndarray: g-matrix with current glow values. Defaults to all 0.\n",
    "        self.gmatrix = np.zeros([1,self.num_actions])\n",
    "        #dict: Dictionary of percepts as {\"percept\": index}\n",
    "        self.percepts = {}\n",
    "\n",
    "    def deliberate(self, percept: str):\n",
    "        \"\"\"\n",
    "        Given a percept, returns an action and changes the ECM if necessary\n",
    "        First, if the percept is new, it will be added to the ECM\n",
    "        Then, an action is selected as a function of the percept and the h-values of edges connected to that percept\n",
    "        Finally, the g-matrix is updated based on the realized percept-action pair.\n",
    "        \"\"\"\n",
    "        #Add percept to ECM if not already present\n",
    "        self.add_percept(percept)\n",
    "        #Perform Random Walk\n",
    "        # get index from dictionary entry\n",
    "        percept_index = self.percepts[percept]\n",
    "        # get h-values\n",
    "        h_values = self.hmatrix[percept_index]\n",
    "        # get probabilities from h-values through a softmax function\n",
    "        prob = transforms._softmax(self.softmax, h_values)\n",
    "        # get action\n",
    "        action = np.random.choice(range(self.num_actions), p=prob)        \n",
    "        #pdate g-matrix\n",
    "        self.gmatrix[int(percept_index),int(action)] = 1.\n",
    "        return action\n",
    "\n",
    "    def add_percept(self, percept):\n",
    "        '''\n",
    "        checks if percept is in dictionary and adds to ECM in not\n",
    "        '''\n",
    "        if percept not in self.percepts.keys(): \n",
    "            self.percepts[percept] = self.num_percepts\n",
    "            # increment number of percepts\n",
    "            self.num_percepts += 1\n",
    "            # add column to h-matrix\n",
    "            self.hmatrix = np.append(self.hmatrix, \n",
    "                                     np.ones([1,self.num_actions]),\n",
    "                                     axis=0)\n",
    "            # add column to g-matrix\n",
    "            self.gmatrix = np.append(self.gmatrix, \n",
    "                                    np.zeros([1,self.num_actions]),\n",
    "                                    axis=0)\n",
    "\n",
    "    def learn(self, reward):\n",
    "        \"\"\"\n",
    "        Given a reward, updates h-matrix. Updates g-matrix with glow.\n",
    "        \"\"\"\n",
    "        # damping h-matrix\n",
    "        self.hmatrix = self.hmatrix - self.damp*(self.hmatrix-1.)\n",
    "        # update h-matrix\n",
    "        self.hmatrix += reward*self.gmatrix\n",
    "        # update g-matrix\n",
    "        self.gmatrix = (1-self.glow)*self.gmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f161bf-f152-47b7-a791-bcbcf5f8af3d",
   "metadata": {},
   "source": [
    "### Priming Two Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca43ac0-7080-4f63-9418-0bf032ce8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Priming_ECM(Two_Layer):\n",
    "    '''\n",
    "    This sub-class of the Two-Layer ECM adds a variable for action priming.\n",
    "    This variable should be a list of floats, each element of which corresponds to an action in the ECM.\n",
    "    These \"priming values\" are summed with h-values of any edge connected to the associated action node prior to calculating walk probabilites with the softmax function\n",
    "    '''\n",
    "    def __init__(self, \n",
    "                 num_actions: int, # The number of available actions.                 \n",
    "                 glow: float = 0.1, # The glow (or eta) parameter. \n",
    "                 damp: float = 0.01, # The damping (or gamma) parameter. \n",
    "                 softmax: float = 0.5, # The softmax (or beta) parameter.\n",
    "                 action_primes: list = None, #weights on the probability that deliberation steps into each action. Defaults to 0 for each action \n",
    "                ):\n",
    "        if action_primes is None:\n",
    "            action_primes = [0.] * num_actions\n",
    "        assert len(action_primes) == num_actions\n",
    "        super().__init__(num_actions, glow, damp, softmax)\n",
    "        self.action_primes = action_primes\n",
    "        \n",
    "\n",
    "    def deliberate(self, percept):\n",
    "        '''\n",
    "        Almost identical to the deliberate function of Two-Layer parent class, but sums h-values and action primes prior to calculating walk probabilities\n",
    "        '''\n",
    "        self.add_percept(percept)\n",
    "        #Perform Random Walk\n",
    "        # get index from dictionary entry\n",
    "        percept_index = self.percepts[percept]\n",
    "        # get h-values\n",
    "        h_values = self.hmatrix[percept_index]\n",
    "        #~~~Differences from two-layer deliberate function within\n",
    "        assert len(h_values) == len (self.action_primes)\n",
    "        # get probabilities from h-values and primes through a softmax function\n",
    "        prob = transforms._softmax(self.softmax, h_values + self.action_primes)\n",
    "        #~~~~~~~\n",
    "        # get action\n",
    "        action = np.random.choice(range(self.num_actions), p=prob)        \n",
    "        #pdate g-matrix\n",
    "        self.gmatrix[int(percept_index),int(action)] = 1.\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc5f245-9469-46d2-9163-77933555e13d",
   "metadata": {},
   "source": [
    "#### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3660f-8cbd-4852-bde9-d5adb33ef485",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m T \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m      9\u001b[0m data_log \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m T\n\u001b[1;32m---> 10\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mRLGL\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#create a default red-light-green-light environment\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(T):\n\u001b[0;32m     13\u001b[0m     observation \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mget_observation()\n",
      "File \u001b[1;32mc:\\users\\alexa\\documents\\projective_simulation\\projective_simulation\\environments.py:53\u001b[0m, in \u001b[0;36mRLGL.__init__\u001b[1;34m(self, state, transition_matrix)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_labels \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mred\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgreen\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transition_matrix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m#create random uniform transition probabilities\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m     transition_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.5\u001b[39m],[\u001b[38;5;241m0.5\u001b[39m,\u001b[38;5;241m0.5\u001b[39m]])\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39mshape(transition_matrix) \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransition_matrix \u001b[38;5;241m=\u001b[39m transition_matrix\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "#| ignore\n",
    "#I don't actually wan't to ignore this - I want it to show but not run. What is the flag for that?\n",
    "from projective_simulation.environments import RLGL\n",
    "import numpy as np\n",
    "\n",
    "test_ECM = Priming_ECM(num_actions = 2, action_primes = [0., 1.5])\n",
    "#Number of steps to run simulation\n",
    "T = 100\n",
    "data_log = [None] * T\n",
    "env = RLGL() #create a default red-light-green-light environment\n",
    "\n",
    "for t in range(T):\n",
    "    observation = env.get_observation()\n",
    "    action = test_ECM.deliberate(observation)\n",
    "    reward = env.get_reward(action)\n",
    "    test_ECM.learn(reward)\n",
    "    data_log[t] = {\"env_state\": env.state, \"action\": action, \"reward\": reward}\n",
    "    env.transition(action)\n",
    "\n",
    "plt.plot(range(T), [np.mean([data_log[step][\"reward\"] for step in range(i-10,i+1) if step >= 0]) for i in range(T)]) #plot a 10 step moving average of the reward "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7261164-245b-46a4-a4ef-98013993e8b0",
   "metadata": {},
   "source": [
    "## Episodic ECM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cffba1-70f3-496a-8a18-35a9b7be9d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "class Episodic_Memory(ECM):\n",
    "    def __init__(self,\n",
    "                 num_actions: int, # the number of perceptual representations available to the agent which, when excited, have an effect on the agent's environment. Action representations are primed differently than sensory representations and do not affect surprise\n",
    "                 capacity: int = 10, # the number of memory traces avaiable to the agent. If simulation time exceeds capacity, the oldest memory trace will be overwritten each time step\n",
    "                 softmax: float = 0.7, # used to determine random walk probabilities, edge weights are normalized using a softmax function with this variable as the temperature constant\n",
    "                 focus: float = 1., #Focus scales the effect of stochastic processes underlying random walks on the ECM. (i.e. it scales the effect of Projective Simulation)\n",
    "                                    #Think of deliberation in an Episodic ECM like a very large number of particles diffusing on the ECM graph, but at each node a single particle is chosen at random and some proportion of particles are pulled along with that one. Focus defines that proportion\n",
    "                                    #If focus == 1, deliberation acts as a random walk of a single (massive) particle on the ECM. \n",
    "                                    #If focus == 0, deliberation acts as the diffusion of a large (approaching infinite) number of particles on the ECM\n",
    "                 error_tolerance: float = 0.01, #How strongly the agent discounts the similarity between two states per bit of mismatched sensory information when establishing a new belief state\n",
    "                 intrinsic_expectations: dict = {}, #If a key in this dictionary corresponds to an index of the agent's perceptual representations, the items value will be added to that percepts expectation value during the agent's predictions. Agents will seek out states that excite perceptual representations with intrinsic expectation\n",
    "                 min_expectation: float = 0.01,  # a baseline value for the priming of perceptual representations. Must be greater than 0 to prevent infinate surprise if a perceptual representation is excited that was not predicted by the agents belief state. Note this will be transormed by the logistic function, so entropy calculations are not done using precisely this number\n",
    "                 deliberation_length: int = 1,   # deliberation_length: the number of diffusive steps in the agent's deliberations\n",
    "                 t: int = 0, #used to model the temporal excitation sequence of memory traces\n",
    "                 expectation_scale: float = 3 #A logistic function is used to scale percept priming between zero and one. This default rate parameter keeps the function used approximately linear when the input is between 0.1 and 0.8\n",
    "                ):\n",
    "        '''\n",
    "        The episodic memory ECM stores percept information in a time ordered system of memory traces by establishing connections (trace_encoder) with weights (hmatrix) between excited perceptual representations and an excited memory trace.\n",
    "        The agent maintains a belief state through the activation (different from excitation) of memory traces, where the strength of activation reflects the strength of the agent's belief that its current state-in-the-world is effectively the same as a state-in-the-world represented by that memory trace\n",
    "        The agent also maintains an expectation state by priming perceptual representations as a function of (1) its belief state, (2) connections between memory traces (mmatrix, stricly time-ordered in this implementation), and (3) the h-matrix\n",
    "        The difference between the agent's expectation state and the excitation of perceptual represenations in the next time step is computed as a surprise and assigned to the last excited memory trace as a valence state\n",
    "        Perceptual representations are excited as a function of sensory-motor interactions with the world and the agent's expectation state\n",
    "        Memory Traces are excited in a time-ordered, winner-takes all fashion.\n",
    "        The agent's beliefs are updated by a process of deliberation, in which excited perceptual representations are activated and those activations then diffuse across the ECM as if they were caused by particles performing a random walk on the ECM graph\n",
    "        The probability that an activation particle walks from one node to another is a function of the edge weights in the hmatrix connected to its current node (perceptual representation or memory trace), and the belief states and valence states of all nodes to which those edges lead\n",
    "        The focus parameter of the Episodic ECM adds weight to a single departing edge from each node, adding stochastic bias to the random walk. Rewards reinforce edge weights in the ECM proportionally to the difference between the expected particle mass to traverse that edge and the observed mass\n",
    "        Rewards are determined as a function of the \"surprise advantage\", i.e. how much more or less surprised the agent was then on average over its previous (relevant) experience.\n",
    "        '''\n",
    "\n",
    "        #initialize constants\n",
    "        self.num_actions = num_actions\n",
    "        self.capacity = capacity\n",
    "        self.softmax = 0.7 \n",
    "        self.focus = focus \n",
    "        self.error_tolerance = error_tolerance\n",
    "        self.intrinsic_expectations = intrinsic_expectations\n",
    "        self.min_expectation = min_expectation\n",
    "        self.deliberation_length = deliberation_length\n",
    "        self.surprise: float = None\n",
    "        self.expectation_scale = expectation_scale\n",
    "        \n",
    "        #initialize modelling variables\n",
    "        self.t = t\n",
    "\n",
    "        #initialize ECM states\n",
    "        self.hmatrix = np.zeros((self.num_actions, self.capacity)) #weights connecting percept nodes to trace nodes\n",
    "        self.mmatrix = np.zeros((self.capacity, self.capacity)) #initialize with no connections between memory traces\n",
    "        self.trace_encoder = np.zeros((self.num_actions, self.capacity), dtype = 'bool') #boolean matrix indicating whether a percept node was encoded in a trace\n",
    "        self.action_encoder = np.ones(self.num_actions, dtype = 'bool') #boolean matrix indicated whether a percept node belongs to an action\n",
    "        self.expectations = np.zeros(self.num_actions) + self.min_expectation\n",
    "        self.beliefs = np.zeros(self.capacity) #initilize with no belief weight on any memory traces\n",
    "        self.valences = np.array([np.nan] * self.capacity) #use nan here because we will want to takes means of this array as it is filled\n",
    "        self.percept_activations = np.zeros(np.shape(self.hmatrix)[0]) #initialize with no activation of percept nodes\n",
    "        self.trace_activations = np.zeros(self.capacity) #initialize with no activation of memory traces\n",
    "        self.trace_excitations = np.zeros(self.capacity) #trace excitations represent the sensory evidence that the world is currently in a state that is effectively represented by the excited trace\n",
    "        \n",
    "\n",
    "    def deliberate(self, percept):\n",
    "        self.add_percept(percept)\n",
    "        self.surprise = self.get_surprise(percept)\n",
    "        self.excite_traces(percept)\n",
    "        self.encode_trace(percept)\n",
    "        self.activate()\n",
    "        for deliberation_step in range(self.deliberation_length):\n",
    "            self.diffuse_activation()\n",
    "        self.predict()\n",
    "        self.t = (self.t + 1) % self.capacity\n",
    "        return(self.expectations[self.action_encoder]) #return action priming\n",
    "\n",
    "    def add_percept(self, percept):\n",
    "        if len(percept) > np.shape(self.hmatrix)[0]: #if percept is longer than the first dimension of the hmatrix it means there is something new in the observation (handled by preprocessor)\n",
    "            i = np.shape(self.hmatrix)[0] #get index for new elements in ECM\n",
    "            new_elements = percept[i:len(percept)]    \n",
    "            self.hmatrix = np.append(self.hmatrix, np.zeros((len(new_elements), self.capacity)), axis = 0) #add baseline weights to connections between new percept nodes and all traces\n",
    "            self.trace_encoder = np.append(self.trace_encoder, np.zeros((len(new_elements), self.capacity), dtype = 'bool'), axis = 0) #new percept nodes have no existing connections to trace nodes\n",
    "            self.action_encoder = np.append(self.action_encoder, np.zeros(len(new_elements), dtype = 'bool')) #this ECM does not support new actions, so new percept nodes are sensory by default\n",
    "            self.expectations = np.append(self.expectations, np.zeros(len(new_elements)) + self.min_expectation)\n",
    "\n",
    "    def excite_traces(self, percept):\n",
    "        self.trace_excitations = np.zeros(self.capacity)\n",
    "        # each trace is excited proportionally to the probability that of the n connections it has to perceptual representations, r of those perceptual representations are excited . . .\n",
    "        # . . . if that trace effectively represents the current world state . . .\n",
    "        # . . . and sensory_error gives the probability that a connected sensory_representation fails to excite when the world is in the state represented by the memory trace.\n",
    "        # this probability is given by the binomial distribution\n",
    "        for t_index in range(self.capacity):\n",
    "            n = np.sum(self.trace_encoder[0:,t_index])\n",
    "            if n > 0: #no excitation if trace has no connections (functionally this shouldn't matter, but it is nicer for interpretation)\n",
    "                r = np.sum([self.trace_encoder[p_index,t_index] and percept[p_index] for p_index in range(len(percept))]) # for each percept, checks that it is both excited and connected to trace, then counts\n",
    "                self.trace_excitations[t_index] = scipy.stats.binom.pmf(r,n,p = 1-self.error_tolerance)\n",
    "    \n",
    "    def encode_trace(self, percept):\n",
    "        self.trace_encoder[0:,self.t] = [x > 0 for x in percept] #connect all excited percepts to current trace\n",
    "        self.mmatrix[self.t-1,self.t] = 1 #create forward connection from previous trace to current trace (this is a spurious but inconsequential connection for an agent's first step)\n",
    "        self.mmatrix[self.t,0:] = [0 for x in self.mmatrix[self.t,0:]] #break any forward connections from current trace (only relevant if t is greater than self.capacity)\n",
    "        self.valences[self.t-1] = self.surprise #valence is assigned to the last trace, it relfects how surprised the agent was by the outcome of the action it took in that trace\n",
    "    \n",
    "    def activate(self):\n",
    "        activation_weights = transforms._softmax(self.softmax, self.hmatrix[self.trace_encoder[0:,self.t],self.t]) #softmax function over edge weights connected to current memory trace\n",
    "        random_selection = np.random.choice(range(len(activation_weights)), p = activation_weights) #get destination of PS random walk\n",
    "        self.percept_activations = np.zeros(np.shape(self.hmatrix)[0]) #reset activations\n",
    "        #bias activation toward starting node of PS random walk\n",
    "        self.percept_activations[self.trace_encoder[0:,self.t]] = [activation_weights[i] + self.focus*(1-activation_weights[i]) if i == random_selection else activation_weights[i] * (1-self.focus) for i in range(len(activation_weights))]\n",
    "\n",
    "    def diffuse_activation(self):\n",
    "        new_trace_activations = np.zeros(self.capacity)\n",
    "        for i in range(np.shape(self.hmatrix)[0]):\n",
    "            #edge weights are the product of the the edge h-values exponent (relevance), the activation of the trace to which the edge is connected (belief prior), and the excitation of the trace to which the edge is connected (sensory evidence)\n",
    "            edge_weights = np.multiply(np.exp(self.hmatrix[i,self.trace_encoder[i,0:]]), self.trace_activations[self.trace_encoder[i,0:]], self.trace_excitations[self.trace_encoder[i,0:]])\n",
    "            if len(edge_weights) > 0: #dont diffusion activation if there are no edges (there should not be to activation to diffuse)    \n",
    "                edge_probs = transforms._softmax(self.softmax, edge_weights)\n",
    "                random_selection = np.random.choice(range(len(edge_weights)), p = edge_probs) #get destination of Projective Simulations\n",
    "                diffusion_mass = _get_diffusion_mass(self.percept_activations[i], edge_probs, random_selection, self.focus)\n",
    "                new_trace_activations[self.trace_encoder[i,0:]] = new_trace_activations[self.trace_encoder[i,0:]] + diffusion_mass\n",
    "        self.trace_activations = new_trace_activations\n",
    "\n",
    "    def predict(self):\n",
    "        self.trace_activations = np.matmul(self.trace_activations, self.mmatrix) # because each row of the matrix has either zeros or a single 1, this just propogates attention forward. non-linear functions might be necessary for more complex mmatrix structures\n",
    "        self.expectations = np.zeros(np.shape(self.hmatrix)[0]) + self.min_expectation #hmatrix rows correspond to perceptual representations\n",
    "        for percept_representation in self.intrinsic_expectations.keys():                   #if a percept representation has an intrinsic expectation, use that instead of the minimum.\n",
    "            if percept_representation in range(len(self.expectations)):\n",
    "                self.expectations[percept_rerpresentation] = self.intrinsic_expectations[percept_representation]\n",
    "        for trace in range(self.capacity):\n",
    "            action_modifiers = np.exp(self.action_encoder * (np.nanmean(self.valences) - self.valences[trace])) #used to scale priming of action representations as a function of trace valence\n",
    "            action_modifiers = [0 if np.isnan(modifier) else modifier for modifier in action_modifiers]\n",
    "            self.expectations = self.expectations + self.trace_activations[trace] * np.multiply(self.trace_encoder[0:,trace], action_modifiers)\n",
    "        self.expectations[np.invert(self.action_encoder)] = transforms._logistic(self.expectations[np.invert(self.action_encoder)], L = 2, y_shift = -1, k = self.expectation_scale) #the logistic function with L = 2 and y_shift = -1 scales positive real inputs between 0 and 1, and the expectation scale defines the rate at which the function approaches 1\n",
    "\n",
    "    def get_surprise(self, percept):\n",
    "        not_action = np.invert(self.action_encoder) #used to exclude action representations from surprise computations\n",
    "        return np.sum(np.where(percept[not_action], -np.log2(self.expectations[not_action]), -np.log2(1-self.expectations[not_action])))\n",
    "            \n",
    "def _get_diffusion_mass(activation, edge_probs, random_selection, focus):\n",
    "    #gets the activation mass that diffuses along each edge connected to a perceptual representation. Note the different effect of the focus parameter if the edge was selected by PS random walk\n",
    "    diffusions = [None] * len(edge_probs)\n",
    "    for i in range(len(edge_probs)):\n",
    "        if i == random_selection:\n",
    "            diffusions[i] = activation * (edge_probs[i] + focus*(1-edge_probs[i]))\n",
    "        else:\n",
    "            diffusions[i] = activation * edge_probs[i] * (1 - focus)\n",
    "    return(diffusions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
