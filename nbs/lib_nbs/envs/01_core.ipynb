{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b6596b-bec3-426d-a1f7-a19307ea6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp envs.core"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4043f7-bd5d-4110-85c7-e438c46a953b",
   "metadata": {},
   "source": [
    "# Environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a72016-f636-4b71-93aa-2ec6e0a43efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "from abc import ABC, abstractmethod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4133dfe8-d868-4dc5-a2df-bdb1914ab1c7",
   "metadata": {},
   "source": [
    "## Abstract Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14b271-d931-4907-a605-7f009d2956de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Abstract_Env(ABC):\n",
    "    \"\"\"\n",
    "    Abstract environment from which other environments can de derived.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, \n",
    "                 state: object):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: an object that defines the state of the environment            \n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "\n",
    "    @abstractmethod\n",
    "    def transition(self, action):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action: an action (or actions) to process\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_observation(self):\n",
    "        \"\"\"\n",
    "        should determine and return an observation for an agent or agents as a function of self.state\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf478d5-759e-4ca2-8ae0-4b0824a6791b",
   "metadata": {},
   "source": [
    "# RLGL\n",
    "\n",
    "A description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff298852-78ee-4800-b426-4e4afbab6c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class RLGL(Abstract_Env):\n",
    "    def __init__(self, state = 0, transition_matrix = None):\n",
    "        self.state = state\n",
    "        self.state_labels = {0: \"red\", 1: \"green\"}\n",
    "        if transition_matrix is None:\n",
    "            #create random uniform transition probabilities\n",
    "            transition_matrix = np.array([[0.5,0.5],[0.5,0.5]])\n",
    "        assert np.shape(transition_matrix) == (2,2)\n",
    "        self.transition_matrix = transition_matrix            \n",
    "\n",
    "    def transition(self, action):\n",
    "        '''\n",
    "        In this environment the agents action determines the reward but does not determine the state\n",
    "        '''\n",
    "        self.state = np.random.choice(range(2), p = self.transition_matrix[self.state,])\n",
    "\n",
    "    def get_observation(self):\n",
    "        return self.state_labels[self.state]\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        if action == self.state:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944c7dcc-d445-4c40-8538-c731135075e1",
   "metadata": {},
   "source": [
    "A minimal example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cc03ae",
   "metadata": {},
   "source": [
    "## Invader Game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e648aea3",
   "metadata": {},
   "source": [
    "Add a description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Invader_Game_Env(Abstract_Env):\n",
    "    def __init__(self, \n",
    "                 state = None, \n",
    "                 transition_matrix = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            state: defines the direction of the sign of the invader, either right (0) or left (1)\n",
    "            transition_matrix: a 2x2 matrix with the probabilities of transitioning from one state to another.\n",
    "        If None, a random uniform transition matrix is created.  \n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        self.state_labels = {0: \"right\", 1: \"left\"}\n",
    "        #transition_matrix is a 2x2 matrix with the probabilities of transitioning from one state\n",
    "        if transition_matrix is None:\n",
    "            #create random uniform transition probabilities\n",
    "            transition_matrix = np.array([[0.5,0.5],[0.5,0.5]])\n",
    "        assert np.shape(transition_matrix) == (2,2)\n",
    "        self.transition_matrix = transition_matrix\n",
    "\n",
    "    def transition(self, action):\n",
    "        '''\n",
    "        In this environment the agents action determines the reward but does not determine the state.\n",
    "        '''\n",
    "        self.state = np.random.choice(range(2), p = self.transition_matrix[self.state,])\n",
    "\n",
    "    def get_observation(self):\n",
    "        if self.state is None:\n",
    "            self.state = np.random.choice(range(2))\n",
    "        return self.state_labels[self.state]\n",
    "    \n",
    "    def get_reward(self, action, liar = False):\n",
    "        \"\"\" Args:\n",
    "            action: the action taken by the agent\n",
    "            liar: if True, the agent is lying and the reward is always 0\n",
    "        \"\"\"\n",
    "        if not liar:\n",
    "            if action == self.state:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            if action != self.state:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce8ea93",
   "metadata": {},
   "source": [
    "## Grid World Environment\n",
    "\n",
    "Grid world is a navigation task in a 2D environment configurated as a grid, with obstacles. The goal is for the agent to reach a target at a certain position, which entails the agent exploring enough and remembering some of its past actions. Observations consist of the agent's coordinates and actions of the set of allowed displacements in the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2557a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Grid_World_Env(Abstract_Env):\n",
    "\n",
    "    def __init__(self, \n",
    "                 dimensions = (10, 10),\n",
    "                 N_obstacles = 10,\n",
    "                 obstacle_locations = None,\n",
    "                 reward_location = None,\n",
    "                 state = None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dimensions: a tuple defining the dimensions of the grid world (rows, columns)\n",
    "            N_obstacles: number of obstacles in the grid world\n",
    "            state: defines the current position of the agent in the grid world as a tuple (row, column)\n",
    "        \"\"\"\n",
    "        self.state = state\n",
    "        self.dimensions = dimensions\n",
    "        self.N_obstacles = N_obstacles\n",
    "\n",
    "        self.action_labels = {0: \"up\",\n",
    "                              1: \"down\", \n",
    "                              2: \"left\", \n",
    "                              3: \"right\"}\n",
    "        # Place the reward at a random location in the grid world\n",
    "        if reward_location is None:\n",
    "            reward_location = (np.random.randint(0, dimensions[0]), np.random.randint(0, dimensions[1]))\n",
    "        self.reward_location = reward_location\n",
    "\n",
    "        # Generate random obstacle locations\n",
    "        if obstacle_locations is None:\n",
    "            obstacle_locations = set()\n",
    "            while len(obstacle_locations) < N_obstacles:\n",
    "                loc = (np.random.randint(0, dimensions[0]), np.random.randint(0, dimensions[1]))\n",
    "                if loc != reward_location:  # Ensure the obstacle is not placed on the reward location\n",
    "                    obstacle_locations.add(loc)\n",
    "        self.obstacle_locations = list(obstacle_locations)\n",
    "    \n",
    "    def reset_position(self, initial_state=None):\n",
    "        \"\"\"\n",
    "        Reset the agent's position to a random location in the grid world.\n",
    "        \"\"\"\n",
    "        if initial_state is None:\n",
    "            self.state = (np.random.randint(0, self.dimensions[0]), np.random.randint(0, self.dimensions[1]))\n",
    "            while self.state in self.obstacle_locations:\n",
    "                self.state = (np.random.randint(0, self.dimensions[0]), np.random.randint(0, self.dimensions[1]))\n",
    "        else:\n",
    "            self.state = initial_state\n",
    "\n",
    "    def transition(self, action, periodic = False):\n",
    "        '''\n",
    "        Move the agent in the grid world based on the action taken.\n",
    "        '''\n",
    "        action = self.action_labels[action]\n",
    "        if periodic:\n",
    "            if action == \"right\":\n",
    "                new_state = ((self.state[0])%self.dimensions[0] + 1, self.state[1])\n",
    "            elif action == \"left\":\n",
    "                new_state = ((self.state[0]-1)%self.dimensions[0], self.state[1])\n",
    "            elif action == \"down\":\n",
    "                new_state = (self.state[0], (self.state[1]-1)%self.dimensions[1])\n",
    "            elif action == \"up\":\n",
    "                new_state = (self.state[0], (self.state[1]+1)%self.dimensions[1])\n",
    "        else:\n",
    "            if action == \"right\":\n",
    "                new_state = (min(self.state[0] + 1, self.dimensions[0] - 1), self.state[1])\n",
    "            elif action == \"left\":\n",
    "                new_state = (max(self.state[0] - 1, 0), self.state[1])\n",
    "            elif action == \"down\":\n",
    "                new_state = (self.state[0], max(self.state[1] - 1, 0))\n",
    "            elif action == \"up\":\n",
    "                new_state = (self.state[0], min(self.state[1] + 1, self.dimensions[1] - 1))\n",
    "        # Change the state only if the new state is not an obstacle\n",
    "        if new_state not in self.obstacle_locations:# and new_state != self.reward_location:\n",
    "            self.state = new_state\n",
    "\n",
    "    def get_observation(self):\n",
    "        return str(self.state)\n",
    "    \n",
    "    def get_reward(self):\n",
    "        if self.state == self.reward_location:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daef9d33",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# nbdev export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7fb60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ps",
   "language": "python",
   "name": "ps"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
