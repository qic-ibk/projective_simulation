{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6d7e1311-f682-4060-8d6d-bc4f51fdcf2a",
   "metadata": {},
   "source": [
    "---\n",
    "skip_exec: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432b43b7-63c8-4360-83c9-098856e14cdf",
   "metadata": {},
   "source": [
    "# Foraging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e936db82-2862-41d4-83ea-6688b945127d",
   "metadata": {},
   "source": [
    "This notebook gathers different kinds of environments for foraging and target search in various scenarios, adapted for their use in the reinforcement learning paradigm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8661f884-5d59-469a-90a5-c65f6f72a68d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp envs.foraging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdde34f-4614-4fad-acc9-b49ef769db41",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc944947-aef5-4f8f-bc02-54b4abda36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0c5bd6-0220-4ed5-99a7-8be2b9775713",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| export\n",
    "# This library has been developed for its use in Numba. However, its use is more complicated than the vanilla numpy. To avoid\n",
    "# a steep learning curve, we don't force numba, but allow the user to freely choose to use it.\n",
    "import os\n",
    "NUMBA_ACTIVATED = True if os.getenv(\"NUMBA_ACTIVATED\", 1) == 'True' else False\n",
    "failed_numba_import = False\n",
    "\n",
    "if NUMBA_ACTIVATED:\n",
    "    try:\n",
    "        import numba\n",
    "        from numba.experimental import jitclass as _jitclass\n",
    "        from numba import types\n",
    "        from numba.experimental import jitclass\n",
    "        from numba import float64, bool_ \n",
    "        from numba import prange as _range\n",
    "        from numba import njit as _njit\n",
    "        from numba.np.extensions import cross2d as _cross\n",
    "    except:\n",
    "        import warnings\n",
    "        warnings.warn(\"numba library not installed. If you want to speed up you agents.foraging, please install it manually. You then need to fix the enviromental variable %env NUMBA_ACTIVATED = True\",\n",
    "                      UserWarning)\n",
    "        failed_numba_import = True\n",
    "\n",
    "if failed_numba_import or not NUMBA_ACTIVATED:\n",
    "    _jitclass, types, njit = None, None, None\n",
    "    from numpy import cross as _cross\n",
    "    _range = range\n",
    "\n",
    "    \n",
    "\n",
    "def maybe_jitclass(spec = None):\n",
    "    if NUMBA_ACTIVATED and _jitclass is not None:\n",
    "        print('numba jitclass ON!')\n",
    "        return _jitclass(spec)\n",
    "    def identity(cls): return cls\n",
    "    return identity\n",
    "\n",
    "def maybe_njit(*args, **kwargs):\n",
    "    \"\"\"Use as @maybe_njit() or @maybe_njit(cache=True, fastmath=True, ...).\"\"\"\n",
    "    def deco(func):\n",
    "        if NUMBA_ACTIVATED and _njit is not None:\n",
    "            return _njit(*args, **kwargs)(func)\n",
    "        return func\n",
    "    return deco    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e505d-55fb-4644-8f6e-639c3c7dca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e5b6cb-6385-4c84-be00-cbace1d00d0c",
   "metadata": {},
   "source": [
    "#|hide\n",
    "# Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd57c4a6-ebf8-4cb0-9395-a9a707f67415",
   "metadata": {},
   "source": [
    "#|hide\n",
    "## isBetween"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36567cb6-7e16-4860-bd57-c43be26fa8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# |hide\n",
    "@maybe_njit()\n",
    "def isBetween_c_Vec_numba(a, b, c, r):\n",
    "        \"\"\"\n",
    "        Checks whether point c is crossing the line formed with point a and b.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        a : tensor, shape = (1,2)\n",
    "            Previous position.\n",
    "        b : tensor, shape = (1,2)\n",
    "            Current position.\n",
    "        c : tensor, shape = (Nt,2)\n",
    "            Positions of all targets.\n",
    "        r : int/float\n",
    "            Target radius.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        mask : array of boolean values\n",
    "            True at the indices of found targets.\n",
    "\n",
    "        \"\"\"\n",
    "        if (a == b).all():\n",
    "            return np.array([False]*c.shape[0])\n",
    "\n",
    "        mask = np.array([True]*c.shape[0])\n",
    "        \n",
    "        dotproduct = (c[:, 0] - a[0]) * (b[0] - a[0]) + (c[:, 1] - a[1])*(b[1] - a[1])\n",
    "        squaredlengthba = (b[0] - a[0])*(b[0] - a[0]) + (b[1] - a[1])*(b[1] - a[1])\n",
    "        \n",
    "        #exclude the targets whose vertical projection of the vector c-a w.r.t. the vector b-a is larger than the target radius.\n",
    "        idx = np.argwhere(np.abs(_cross(b-a, c-a))/np.linalg.norm(b-a) > r) \n",
    "        for i1 in idx:\n",
    "            mask[i1] = False        \n",
    "        \n",
    "        #exclude the targets whose scalar product is negative (they are on the other side of the step direction)\n",
    "        for i2 in np.argwhere(dotproduct < 0):\n",
    "            mask[i2] = False\n",
    "\n",
    "        #exclude the targets that are beyond the step.\n",
    "        for i3 in np.argwhere(dotproduct > squaredlengthba):\n",
    "            mask[i3] = False\n",
    "            \n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02d5794-1e9c-4fa5-9086-bb21f72615dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "compiling = isBetween_c_Vec_numba(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817b36f-89b2-41f3-9b11-71038e127fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.47 μs ± 31.6 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%timeit isBetween_c_Vec_numba(np.array([0.1,1]), np.array([1,3]), np.random.rand(100,2), 0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb1ecda-8072-4072-9d0f-3eb8689a3f28",
   "metadata": {},
   "source": [
    "#|hide\n",
    "## Pareto sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1023cf70-3639-4629-9ab1-76eb94bd20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# |hide\n",
    "@maybe_njit()\n",
    "def pareto_sample(alpha, xm, size=1):\n",
    "    samples = np.zeros(size)\n",
    "    for ii in range(size):\n",
    "        u = random.random()  # Uniform random variable between 0 and 1\n",
    "        x = xm / (u ** (1 / alpha))\n",
    "        samples[ii] = x\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c099a4b2-2b60-4adb-bc60-beca05387032",
   "metadata": {},
   "source": [
    "#|hide\n",
    "\n",
    "## Random sampling from array with probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a5fee3-fac9-42fe-88a1-b12b0f4493be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "# |hide\n",
    "@maybe_njit()\n",
    "def rand_choice_nb(arr, prob):\n",
    "    \"\"\"\n",
    "    :param arr: A 1D numpy array of values to sample from.\n",
    "    :param prob: A 1D numpy array of probabilities for the given samples.\n",
    "    :return: A random sample from the given array with a given probability.\n",
    "    \"\"\"\n",
    "    return arr[np.searchsorted(np.cumsum(prob), np.random.random(), side=\"right\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0370360-425e-4698-9383-c1ff4461ddb5",
   "metadata": {},
   "source": [
    "# TargetEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3f8518-7596-4126-ae87-9d48e4dbfbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "if NUMBA_ACTIVATED:\n",
    "    spec = [(\"target_positions\", float64[:,:]) ,\n",
    "               (\"current_rewards\", float64[:]) ,\n",
    "               (\"kicked\", float64[:]) ,\n",
    "               (\"current_directions\", float64[:]) ,\n",
    "               (\"positions\", float64[:,:]),\n",
    "               (\"previous_pos\", float64[:,:]),\n",
    "               (\"lc\", float64[:,:]),\n",
    "               (\"mask\", bool_[:]),\n",
    "               (\"first_encounter\", float64[:,:])]\n",
    "else:\n",
    "    spec = None\n",
    "@maybe_jitclass(spec)\n",
    "class TargetEnv():\n",
    "    Nt : int\n",
    "    L : float\n",
    "    r : float\n",
    "    lc : np.array\n",
    "    agent_step : float\n",
    "    num_agents : int\n",
    "    destructive_targets : bool\n",
    "    target_positions : np.ndarray\n",
    "    current_rewards : np.array\n",
    "    kicked : np.array\n",
    "    current_directions : np.array\n",
    "    positions : np.array\n",
    "    previous_pos : np.array\n",
    "    mask : np.array\n",
    "    first_encounter : np.array\n",
    "    lc_distribution : str\n",
    "    \n",
    "    \n",
    "    def __init__(self, \n",
    "                 Nt = 10, # Number of targets.\n",
    "                 L = 1.3, #  Size of the (squared) world.\n",
    "                 r = 1.5, # Radius with center the target position. It defines the area in which agent detects the target.\n",
    "                 lc = [[1],[1]], # Cutoff length. Displacement away from target \n",
    "                 agent_step = 1, # Displacement of one step. The default is 1.\n",
    "                 num_agents = 1, # Number of agents that forage at the same time. The default is 1. > 1 not fully implemented\n",
    "                 destructive = False, # True if targets are destructive. The default is False.\n",
    "                 lc_distribution = 'constant' # Distribution from where to sample l_c. Can be 'power_law', 'pareto' or something else. See comments self.check_encounter for explanations\n",
    "                ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Class defining the a Foraging environment with multiple targets and two actions: continue in \n",
    "        the same direction and turn by a random angle.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.Nt = Nt\n",
    "        self.L = L\n",
    "        self.r = r\n",
    "        self.lc = np.array(lc)\n",
    "        self.agent_step = agent_step \n",
    "        self.num_agents = num_agents\n",
    "        self.destructive_targets = destructive\n",
    "        self.lc_distribution = lc_distribution\n",
    "        \n",
    "\n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        \"\"\"\n",
    "        Environment initialization.\n",
    "        \"\"\"\n",
    "        self.target_positions = np.random.rand(self.Nt, 2)*self.L\n",
    "        \n",
    "        #store who is/was rewarded\n",
    "        self.current_rewards = np.zeros(self.num_agents)\n",
    "        \n",
    "        #signal whether agent has been kicked\n",
    "        self.kicked = np.zeros(self.num_agents)\n",
    "        \n",
    "        #set positions and directions of the agents\n",
    "        self.current_directions = np.random.rand(self.num_agents)*2*np.pi\n",
    "        self.positions = np.random.rand(self.num_agents, 2)*self.L\n",
    "        self.previous_pos = self.positions.copy()       \n",
    "\n",
    "        \n",
    "\n",
    "    def update_pos(self, \n",
    "                   change_direction, # Whether the agent decided to turn or not.\n",
    "                   agent_index = 0 # Index of the given agent. The default is 0. This is only keeped for future devs\n",
    "                  ):        \n",
    "        \"\"\"\n",
    "        Updates information of the agent depending on its decision.            \n",
    "        \"\"\"\n",
    "        \n",
    "        # Save previous position to check if crossing happened\n",
    "        self.previous_pos[agent_index] = self.positions[agent_index].copy()\n",
    "        \n",
    "        if change_direction:\n",
    "            self.current_directions[agent_index] = random.uniform(0,1)*2*math.pi\n",
    "        \n",
    "        #Update position\n",
    "        self.positions[agent_index][0] = self.positions[agent_index][0] + self.agent_step*np.cos(self.current_directions[agent_index])\n",
    "        self.positions[agent_index][1] = self.positions[agent_index][1] + self.agent_step*np.sin(self.current_directions[agent_index])\n",
    "        \n",
    "       \n",
    "    def check_encounter(self,\n",
    "                       agent_index=0 # Index of the given agent. The default is 0. This is only keeped for future devs\n",
    "                       ): # True if the agent found a target, else False\n",
    "        \"\"\"\n",
    "        Checks whether the agent found a target, and updates the information accordingly.\n",
    "        \"\"\"       \n",
    "        \n",
    "        encounters = isBetween_c_Vec_numba(self.previous_pos[agent_index], self.positions[agent_index], self.target_positions, self.r)\n",
    "        \n",
    "        if sum(encounters) > 0: \n",
    "            \n",
    "            #if there is more than 1 encounter, pick the closest to the agent.\n",
    "            if sum(encounters) == 1:\n",
    "                first_encounter = np.argwhere(encounters == True).flatten()\n",
    "            else:\n",
    "                # compute the distance from the previous position to each target            \n",
    "                distance_previous_pos = np.sqrt((self.previous_pos[agent_index][0]- self.target_positions[:, 0])**2 + (self.previous_pos[agent_index][1] - self.target_positions[:, 1])**2)            \n",
    "                \n",
    "                # checking which encountered point is closer to previous position\n",
    "                min_distance_masked = np.argmin(distance_previous_pos[encounters])\n",
    "                first_encounter = np.argwhere(encounters == True)[min_distance_masked].flatten()\n",
    "            if self.destructive_targets:\n",
    "                self.target_positions[first_encounter] = np.random.rand(2)*self.L\n",
    "            else:\n",
    "                #----KICK----\n",
    "                # If there was encounter, we reset direction and change position of particle to (pos target + lc)\n",
    "                kick_direction = np.random.uniform(low = 0, high = 2*np.pi)\n",
    "                for idx_first in first_encounter: # This is super weird!\n",
    "                    if self.lc_distribution == 'power_law':\n",
    "                        # when we have the power law, the first value of lc is considered to be the exponent.\n",
    "                        # The following samples from a power law x^{-1-alpha} where alpha = self.lc.flatten()[0]                        \n",
    "                        current_lc = (1-random.uniform(0,1))**(-1/self.lc.flatten()[0])\n",
    "\n",
    "                    elif self.lc_distribution == 'pareto':\n",
    "                        # Sampling from Pareto. Here alpha = self.lc.flatten()[0] and x_minim = self.lc.flatten()[0]\n",
    "                        current_lc = pareto_sample(self.lc[0,0], self.lc[1,0])[0]\n",
    "                    else:\n",
    "                        # if lc has a single element, take that one as lc, if not sample\n",
    "                        current_lc = self.lc.flatten()[0] if len(self.lc.flatten()) == 2 else rand_choice_nb(arr = self.lc[0], prob = self.lc[1])\n",
    "                    self.positions[agent_index][0] = self.target_positions[idx_first, 0] + current_lc*np.cos(kick_direction)\n",
    "                    self.positions[agent_index][1] = self.target_positions[idx_first, 1] + current_lc*np.sin(kick_direction)\n",
    "                self.kicked[agent_index] = 1\n",
    "                #------------\n",
    "                \n",
    "            #...and we add the information that this agent got to the target\n",
    "            self.current_rewards[agent_index] = 1              \n",
    "            return 1\n",
    "        \n",
    "        else: \n",
    "            self.kicked[agent_index] = 0\n",
    "            self.current_rewards[agent_index] = 0\n",
    "            return 0   \n",
    "        \n",
    "    def check_bc(self):\n",
    "        \"\"\"\n",
    "        Updates position coordinates of agent agent_index to fulfill periodic boundary conditions.\n",
    "\n",
    "        \"\"\"\n",
    "        agent_index=0\n",
    "        self.positions[agent_index] = (self.positions[agent_index])%self.L\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e875fda0-3264-47ea-b5b2-5c36c6964b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "env = TargetEnv(Nt = 1000,\n",
    "                 L = 123,\n",
    "                 r = 50,\n",
    "                 lc = np.array([[0.1],[1]]),\n",
    "                 lc_distribution = 'pareto')\n",
    "env.check_encounter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32539df0-f46b-4ac8-b944-8faae5679b37",
   "metadata": {},
   "source": [
    "# ResetEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abf9375-7e97-43fe-9f91-0bff65d7767d",
   "metadata": {},
   "source": [
    "## 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657f70be-ac30-4f9c-8c12-b0938eb60270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@maybe_jitclass()\n",
    "class ResetEnv_1D():\n",
    "    L : float\n",
    "    D : float    \n",
    "    position : float    \n",
    "    \n",
    "    def __init__(self,\n",
    "                 L = 1.3, # Distance to cross to get reward\n",
    "                 D = 1.0, # Diffusion coefficient\n",
    "                ):  \n",
    "        '''\n",
    "        Class defining a 1D environment where the agent can either continue walking or reset to the origin.\n",
    "        Reward = 1 if the agent crosses the distance L, else = 0.\n",
    "        '''      \n",
    "   \n",
    "        self.L = L\n",
    "        self.D = D\n",
    "        self.position = 0\n",
    "        \n",
    "    def init_env(self):\n",
    "        self.position = 0\n",
    "    \n",
    "    def update_pos(self, \n",
    "                   action # 0: continue walk, 1: reset to origin\n",
    "                  ): # Reward = 1 if crossed L, else = 0\n",
    "        \n",
    "        if action == 0:\n",
    "            self.position += np.random.randn()*np.sqrt(2*self.D)        \n",
    "        else: self.position = 0\n",
    "                \n",
    "        if self.position >= self.L: \n",
    "            self.init_env()\n",
    "            return 1\n",
    "        else: return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99af22f4",
   "metadata": {},
   "source": [
    "#| hide\n",
    "\n",
    "### Search loop with fixed policy an arbitrary environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ba1108",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "\n",
    "@maybe_njit()\n",
    "def reset_search_loop(T, # Number of steps \n",
    "                      reset_policy, # Reset policy\n",
    "                      env # Environment\n",
    "                      ):\n",
    "    '''\n",
    "    Loop that runs the reset environment with a given reset policy.\n",
    "    '''\n",
    "    \n",
    "    rewards = 0\n",
    "    tau = 0 # time since last reset\n",
    "    \n",
    "    for t in range(T):\n",
    "        \n",
    "        action = 0 if np.random.rand() > reset_policy[tau] else 1\n",
    "        rew = env.update_pos(action = action)\n",
    "        \n",
    "        if rew == 1 or action == 1:\n",
    "            tau = 0\n",
    "        else:\n",
    "            tau += 1\n",
    "        \n",
    "        rewards += rew\n",
    "    \n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbefb75f",
   "metadata": {},
   "source": [
    "#| hide\n",
    "\n",
    "### Parallel search loops for Reset 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900112c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@maybe_njit(parallel = True)\n",
    "def parallel_Reset1D_sharp(T : int, # Number of steps\n",
    "                           resets : np.array, # Array of resetting times\n",
    "                            L : float, # Distance to cross to get reward\n",
    "                            D : float # Diffusion coefficient\n",
    "                            )-> np.array: # Rewards obtained for each resetting time\n",
    "    '''\n",
    "    Runs the Reset 1D loop in parallel for different sharp resetting times.\n",
    "    '''\n",
    "    rews_reset = np.zeros_like(resets)\n",
    "    \n",
    "    for idxr in _range(len(resets)):\n",
    "        \n",
    "        env = ResetEnv_1D(L, D)        \n",
    "        reset_policy = np.zeros(resets[idxr])\n",
    "        reset_policy[resets[idxr]-1] = 1        \n",
    "        \n",
    "        rews_reset[idxr] = reset_search_loop(T = T, reset_policy = reset_policy, env = env)\n",
    "    return rews_reset\n",
    "\n",
    "@maybe_njit(parallel = True)\n",
    "def parallel_Reset1D_exp(T, rates, L, D):\n",
    "    '''\n",
    "    Runs the Reset 1D loop in parallel for different exponential resetting rates.\n",
    "    '''\n",
    "    \n",
    "    rews_rate = np.zeros_like(rates)\n",
    "    for idxr in _range(len(rates)):\n",
    "        \n",
    "        env = ResetEnv_1D(L, D)        \n",
    "        reset_policy = np.ones(T)*rates[idxr]\n",
    "        \n",
    "        rews_rate[idxr] = reset_search_loop(T = T, reset_policy = reset_policy, env = env)\n",
    "    return rews_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba8309e-8b30-4205-9fb5-7ec1fa077da1",
   "metadata": {},
   "source": [
    "## 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ee9ec-6a02-452f-8017-778c9b81cf5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "if NUMBA_ACTIVATED:\n",
    "    spec = [(\"position\", float64[:]),\n",
    "               (\"target_position\", float64[:,:]),\n",
    "               (\"previous_pos\", float64[:])\n",
    "              ]\n",
    "else:\n",
    "    spec = None\n",
    "@maybe_jitclass(spec)\n",
    "class ResetEnv_2D():\n",
    "    D : float    \n",
    "    dist_target :float\n",
    "    r: float\n",
    "    position : np.array    \n",
    "    target_position : np.array\n",
    "    previous_pos : np.array\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dist_target = 0.2, # Distance from init position and target\n",
    "                 radius_target = 0.5, # Radius of the target\n",
    "                 D = 1.0, # Diffusion coefficient of the walker              \n",
    "                ): \n",
    "        '''\n",
    "        Class defining a 2D environment where the agent can either continue walking or reset to the origin.\n",
    "        Reward = 1 if the agent enters the area defined by the distance and radius of the target.\n",
    "        '''   \n",
    "        \n",
    "\n",
    "        self.D = D\n",
    "        self.dist_target = dist_target\n",
    "        self.r = radius_target        \n",
    "        \n",
    "        self.target_position = np.array([self.dist_target*np.cos(np.pi/4), self.dist_target*np.sin(np.pi/4)])[np.newaxis, :]       \n",
    "        \n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        self.position = np.array([0.0,0.0])\n",
    "        self.previous_pos = self.position.copy()     \n",
    "        \n",
    "        \n",
    "    \n",
    "    def update_pos(self, \n",
    "                   action # 0: continue walk, 1: reset to origin\n",
    "                  ): # Reward = 1 if encountered target, else = 0\n",
    "        \n",
    "        if action == 1:\n",
    "            self.init_env()\n",
    "            return 0\n",
    "        \n",
    "        elif action == 0:\n",
    "            \n",
    "            self.previous_pos = self.position.copy()            \n",
    "            self.position += np.random.randn(2)*np.sqrt(2*self.D)\n",
    "            \n",
    "            # Checking encounter\n",
    "            inside_target = np.linalg.norm(self.position-self.target_position) <= self.r\n",
    "            crossed_target = isBetween_c_Vec_numba(self.previous_pos, self.position, self.target_position, self.r)\n",
    "                        \n",
    "            if inside_target or crossed_target:\n",
    "                self.init_env()\n",
    "                return 1\n",
    "            else: \n",
    "                return 0\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1eda84",
   "metadata": {},
   "source": [
    "#| hide\n",
    "\n",
    "### Parallel search loops for Reset 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865d458f-a0eb-4683-8fc1-82d9a3b47569",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "@maybe_njit(parallel = True)\n",
    "def parallel_Reset2D_sharp(T, # Number of steps\n",
    "                           resets, # Array of resetting times\n",
    "                           dist_target, # Distance to target\n",
    "                           radius_target, # Radius of the target\n",
    "                           D # Diffusion coefficient of the walker\n",
    "                           )-> np.array: # Rewards obtained for each sharp resetting time\n",
    "    '''\n",
    "    Runs the Reset 2D loop in parallel for different sharp resetting times.\n",
    "    '''\n",
    "\n",
    "    rews_reset = np.zeros_like(resets)\n",
    "    \n",
    "    for idxr in _range(len(resets)): \n",
    "        \n",
    "        env = ResetEnv_2D(dist_target, radius_target, D)        \n",
    "        reset_policy = np.zeros(resets[idxr])\n",
    "        reset_policy[resets[idxr]-1] = 1        \n",
    "        \n",
    "        rews_reset[idxr] = reset_search_loop(T = T, reset_policy = reset_policy, env = env)\n",
    "    return rews_reset\n",
    "\n",
    "\n",
    "\n",
    "@maybe_njit(parallel = True)\n",
    "def parallel_Reset2D_exp(T, # Number of steps \n",
    "                         rates, # Reset rates\n",
    "                         dist_target, # Distance of the target\n",
    "                         radius_target, # Radius of the target\n",
    "                         D # Diffusion coefficient of the walker\n",
    "                         )-> np.array: # Rewards obtained for each exponential rate\n",
    "    '''\n",
    "    Runs the Reset 2D loop in parallel for different sharp resetting times.\n",
    "    '''\n",
    "    \n",
    "    rews_rate = np.zeros_like(rates)\n",
    "    for idxr in _range(len(rates)):\n",
    "        \n",
    "        env = ResetEnv_2D(dist_target, radius_target, D)         \n",
    "        reset_policy = np.ones(T)*rates[idxr]\n",
    "        \n",
    "        rews_rate[idxr] = reset_search_loop(T = T, reset_policy = reset_policy, env = env)\n",
    "    return rews_rate\n",
    "\n",
    "@maybe_njit(parallel = True)\n",
    "def parallel_Reset2D_policies(T, # Number of steps\n",
    "                              reset_policies, # Array of reset policies\n",
    "                              dist_target, # Distance of the target\n",
    "                              radius_target, # Radius of the target\n",
    "                              D # Diffusion coefficient of the walker\n",
    "                              )-> np.array: # Rewards obtained for each reset policy\n",
    "    '''\n",
    "    Runs the Reset 2D loop in parallel for different sharp resetting times.\n",
    "    '''\n",
    "    \n",
    "    rews_rate = np.zeros(reset_policies.shape[0])    \n",
    "    \n",
    "    for idx_policy in _range(reset_policies.shape[0]):    \n",
    "        \n",
    "        env = ResetEnv_2D(dist_target, radius_target, D)  \n",
    "                \n",
    "        rews_rate[idx_policy] = reset_search_loop(T = T, reset_policy = reset_policies[idx_policy], env = env)\n",
    "        \n",
    "    return rews_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d7e68c-86d2-4ae5-abea-8b2b4df83122",
   "metadata": {},
   "source": [
    "# Turn + Reset Environment\n",
    "> Only 2D is considered here as the 1D case is trivial (the agent would never turn, only reset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747fb886-db05-40b9-b569-b6b8a361ae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "if NUMBA_ACTIVATED:\n",
    "    spec = [(\"position\", float64[:]),\n",
    "               (\"target_position\", float64[:,:]),\n",
    "               (\"previous_pos\", float64[:])\n",
    "              ]\n",
    "else:\n",
    "    spec = None\n",
    "@maybe_jitclass(spec)\n",
    "class TurnResetEnv_2D():\n",
    "    agent_step : float    \n",
    "    dist_target :float\n",
    "    r: float\n",
    "    position : np.array    \n",
    "    target_position : np.array\n",
    "    previous_pos : np.array\n",
    "    current_direction : float\n",
    "    \n",
    "    def __init__(self,\n",
    "                 dist_target = 0.2, # Distance from init position and target\n",
    "                 radius_target = 0.5, # Radius of the target\n",
    "                 agent_step = 1.0, # Diffusion coefficient of the walker              \n",
    "                ):   \n",
    "        \n",
    "        '''\n",
    "        Class defining a Foraging environment with a single target and three possible actions:\n",
    "\n",
    "        - Continue in the same direction\n",
    "        - Turn by a random angle\n",
    "        - Reset to the origin\n",
    "\n",
    "        The agent makes steps of constant length given by agent_step. \n",
    "        '''\n",
    "        \n",
    "\n",
    "        self.agent_step = agent_step\n",
    "        self.dist_target = dist_target\n",
    "        self.r = radius_target        \n",
    "        \n",
    "        self.target_position = np.array([self.dist_target*np.cos(np.pi/4), self.dist_target*np.sin(np.pi/4)])[np.newaxis, :]       \n",
    "        \n",
    "        self.init_env()\n",
    "        \n",
    "    def init_env(self):\n",
    "        self.position = np.array([0.0,0.0])\n",
    "        self.current_direction = np.random.rand()*2*np.pi\n",
    "        self.previous_pos = self.position.copy()     \n",
    "        \n",
    "        \n",
    "    \n",
    "    def update_pos(self, \n",
    "                   change_direction, # If True, the agent changes direction by a random angle\n",
    "                   reset # If True, the agent is reset to the origin\n",
    "                   ):        \n",
    "        \"\"\"\n",
    "        Updates position of the agent depending on its decision\n",
    "        \"\"\"\n",
    "        \n",
    "        if reset:\n",
    "            self.init_env()\n",
    "            return 0\n",
    "\n",
    "        else:\n",
    "            if change_direction:\n",
    "                self.current_direction = np.random.rand()*2*np.pi\n",
    "\n",
    "            self.position[0] += self.agent_step*np.cos(self.current_direction)\n",
    "            self.position[1] += self.agent_step*np.sin(self.current_direction)\n",
    "\n",
    "            # Checking encounter\n",
    "            inside_target = np.linalg.norm(self.position-self.target_position) <= self.r\n",
    "            crossed_target = isBetween_c_Vec_numba(self.previous_pos, self.position, self.target_position, self.r)\n",
    "                        \n",
    "            if inside_target or crossed_target:\n",
    "                self.init_env()\n",
    "                return 1\n",
    "            else:                \n",
    "                self.previous_pos = self.position.copy()\n",
    "                return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651c1c49",
   "metadata": {},
   "source": [
    "#| hide\n",
    "### Search loop with fixed policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1306cd1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def search_loop_turn_reset_sharp(T, # Number of steps\n",
    "                                 reset, # Reset time\n",
    "                                turn, # Turn time\n",
    "                                 env # Environment\n",
    "                                 ): # Number of rewards obtained\n",
    "    \"\"\"\n",
    "    Runs a search loop of T steps. There is a single counter that works as follows:\n",
    "\n",
    "    - Starts at 0\n",
    "    - For each turn or continue action gets +1\n",
    "    - If reset or reach the target is set to 0\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    rewards = 0\n",
    "    counter = 0 \n",
    "\n",
    "    env.init_env()\n",
    "    \n",
    "    for t in range(T):        \n",
    "        counter += 1\n",
    "        # Reset\n",
    "        if counter == reset:\n",
    "            rew = env.update_pos(False, # change direction\n",
    "                                 True   # reset\n",
    "                                )           \n",
    "            counter = 0\n",
    "        \n",
    "        # Turn\n",
    "        elif counter == turn:\n",
    "            rew = env.update_pos(True, # change direction\n",
    "                                 False # reset\n",
    "                                )\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            rew = env.update_pos(False, # change direction\n",
    "                                 False # reset\n",
    "                                )\n",
    "        if rew == 1:\n",
    "            counter = 0\n",
    "            \n",
    "        rewards += rew\n",
    "        \n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4b2eb3",
   "metadata": {},
   "source": [
    "#| hide\n",
    "# nbdev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655ef79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev import nbdev_export ; nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44753003-8895-4082-ad10-5f364e42a4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
